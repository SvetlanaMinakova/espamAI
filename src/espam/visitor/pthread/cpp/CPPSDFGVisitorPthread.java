package espam.visitor.pthread.cpp;
import espam.datamodel.graph.cnn.neurons.generic.GenericNeuron;
import espam.datamodel.graph.cnn.operators.ComplexOperator;
import espam.datamodel.graph.cnn.operators.InternalBuffer;
import espam.datamodel.graph.cnn.operators.Operator;
import espam.datamodel.graph.csdf.*;
import espam.datamodel.graph.csdf.datasctructures.MemoryUnit;
import espam.datamodel.graph.csdf.datasctructures.Tensor;
import espam.datamodel.mapping.MProcess;
import espam.datamodel.mapping.MProcessor;
import espam.datamodel.mapping.Mapping;
import espam.datamodel.platform.processors.GPU;
import espam.utils.fileworker.FileWorker;
import espam.visitor.sesame.cpp.CPPSDFGVisitor;

import java.util.*;

public class CPPSDFGVisitorPthread extends CPPSDFGVisitor {
    ///////////////////////////////////////////////////////////////////
    ////                         public methods                     ///

     /**
     * Call CPP SDFG Visitor of the operational node (node, performs some useful job)
     * @param y  corresponding CSDFNode
     * @param dir directory for .cpp templates
     */
     @Override
     public void callVisitor(CSDFNode y, String dir){
         try {
             _printStream = FileWorker.openFile(dir, y.getName(), "cpp");
             _writeCommonCppBeginning(y.getName());
             _writeAdditionalLibraries();
             _writeCppConstructorAndDestructor(y);
             _writeMain(y);
         }
         catch (Exception e){
             System.err.println(".cpp file creation error for node" + y.getName() + " " + e.getMessage());
         }
     }

     /////////////////////////////////////////////////////////////////////
    /////////                class templates      ///////////////////////

    /**
     * generate main class template contains
     * application structure definition and
     * application control logic
     */
    public void generateMainClassTemplate(String dir, CSDFGraph sdfg){
        try {
            _printStream = FileWorker.openFile(dir, _mainClassName, "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("");
            _addAllClassesHeaders(sdfg);
            _writeMainClassCPPBeginning();
            _writeNoBaseCppConstructorAndDestructor(_mainClassName);
            _writeMainClassTimer();
            _writeMainClassMain(sdfg);
            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + _mainClassName + " " + e.getMessage());
        }
    }

      /**
     * generate application functions class, that contains functions, common
     * for every node class, such as execution and R/W primitives, print functions,
     * API to application operators library etc.
     */
    public void generateFuncClassTemplate(String dir, boolean incDnnFuncCPU, boolean incDNNFuncGPU){
        try {
            _printStream = FileWorker.openFile(dir, _funcClassName, "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("");
            _writeFuncClassCPPBeginning((incDnnFuncCPU||incDNNFuncGPU));
            _writeNoBaseCppConstructorAndDestructor(_funcClassName);
            _writeExecPrimitives(incDnnFuncCPU,incDNNFuncGPU);
            _writeFunctions();
            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + _funcClassName + " " + e.getMessage());
        }
    }

     /**
     * generate base class template contains common lines for all Layer's classes
     */
    public void generateBaseClassTemplate(String dir){
        try {
            String className = "csdfNode";
            _printStream = FileWorker.openFile(dir, className, "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("#include \"" + _baseClassName + ".h\"");
            _printStream.println(className + "::" + className + "() {}");
            _printStream.println(className + "::~" + className + "() {}");
            _printStream.println("");
            _printStream.println("//virtual main function");
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + _mainClassName + " " + e.getMessage());
        }
    }

         /**
     * generate base class template contains common lines for all Layer's classes
     */
    public void generateDataLoadClassTemplate(String dir){
        try {
            String className = _loadWeightsClassName;
            _printStream = FileWorker.openFile(dir, _loadWeightsClassName, "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("#include \"cnpy.h\"");
            _printStream.println("#include <iostream>");
            _printStream.println("#include <string>");
            _printStream.println("#include \"" + _loadWeightsClassName + ".h\"");

            _printStream.println(className + "::" + className + "() {}");
            _printStream.println(className + "::~" + className + "() {}");
            _printStream.println("");
            _generateDataLoadFunc();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + _mainClassName + " " + e.getMessage());
        }
    }


    /****************************************************************/
    /**               standard class templates                    **/
    ////////////// BLOCK-SIZE ALGORITHM  /////////////////////////////////////
    /**
     * Class to find optimal block size for GPU Convolutions
     * @param dir directory to generate the class file
     */
    public void generateAlgorithmClass(String dir, CSDFGraph csdfg){
        try {
            String name = "blocks_algorithm";
            _printStream = FileWorker.openFile(dir, name, "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("");
            _printStream.println("#include <iostream>");
            _printStream.println("#include <string>");
            _printStream.println("#include <algorithm>");
            _printStream.println("#include \"" + name + ".h\"");
            _printStream.println("using namespace std;");
            _printStream.println("");

            _writeNoBaseCppConstructorAndDestructor(name);

            Vector<CSDFNode> convNodes = csdfg.getNodesList("conv");
            int cpus = _getCPUCoresNum();
            int convGroupSize = Math.min(convNodes.size(),cpus);
            int convTailGroupSize = convNodes.size()%convGroupSize;

            //common subfuncs
            _printGetBlocksNumOfBlockAlg(convNodes.size());
            _printStream.println("");
            _printLargestFuncOfBlockAlg();
            _printStream.println("");
            _printGetMaxApproxOfBlockAlg();
            _printStream.println("");

            //inner loop
            _printInnerLoopOfAlgClass(convGroupSize,"");
            _printStream.println("");
            // inner loop for tail
            if(convTailGroupSize>0){
                _printInnerLoopOfAlgClass(convTailGroupSize,"_tail");
                _printStream.println("");
            }

            //main
            _printOuterLoopOfBlockAlg(convNodes, cpus, convGroupSize, convTailGroupSize);
           // _printStream.println("");
           // _printStream.println("");
            //API
           // _printMainFuncOfBlockAlg();

        }
        catch (Exception e){
            System.err.println("algorithm.cpp generation error: "+e.getMessage());
        }
    }

     private void _printGetBlocksNumOfBlockAlg(Integer blocksNum){
        _printStream.println(_prefix + "int blocks_algorithm::get_blocks_num(){");
        _prefixInc();
        _printStream.println(_prefix + "return " + blocksNum + ";");
        _prefixDec();
        _printStream.println(_prefix + "}");
    }

    private void _printGetMaxApproxOfBlockAlg(){
        _printStream.println(_prefix + "int blocks_algorithm::count_max_approx_number(int num_conv, int *size){");
        _prefixInc();
        _printStream.println();
        _printStream.println(_prefix + "int count[num_conv] = {0};");
        _printStream.println(_prefix + "int n[num_conv] = {0};");
        _printStream.println();
        _printStream.println(_prefix + "for(int i=0; i<num_conv; i++){");
        _prefixInc();
        _printStream.println(_prefix + "count[i] = 1;");
        _printStream.println(_prefix + "n[i] = 0;");
        _prefixDec();
        _printStream.println("}");
        _printStream.println();
        _printStream.println(_prefix + "//max approximate number per one convolution");
        _printStream.println(_prefix + "int man_per_conv = 0;");
        _printStream.println(_prefix + "//general max approximate number");
        _printStream.println(_prefix + "int man = 0;");
        _printStream.println();
        _printStream.println(_prefix + "for(int i=0; i<num_conv; i++){");
        _prefixInc();
        _printStream.println(_prefix + "while(count[i]<=size[i]){");
        _prefixInc();
        _printStream.println(_prefix + "if(size[i]%count[i] == 0)");
        _prefixInc();
        _printStream.println(_prefix + "man_per_conv++;");
        _prefixDec();
        _printStream.println(_prefix + "count[i]++;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(_prefix + "if(man<man_per_conv)");
        _prefixInc();
        _printStream.println(_prefix + "man = man_per_conv;");
        _prefixDec();
        _printStream.println(_prefix + "man_per_conv = 0;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(_prefix + "return man;");
        _prefixDec();
        _printStream.println(_prefix + "}");

    }


    //LARGEST func
    private void _printLargestFuncOfBlockAlg(){
            _printStream.println("int blocks_algorithm::largest(int arr[], int n) {");
            _prefixInc();
            _printStream.println("");
            _printStream.println(_prefix + "int i;");
            _printStream.println(_prefix + "// Initialize maximum element ");
            _printStream.println(_prefix + "int max = arr[0];");
            _printStream.println("");
            _printStream.println(_prefix + "// Traverse array elements from second and ");
            _printStream.println(_prefix + "// compare every element with current max ");
            _printStream.println(_prefix + "for (i = 1; i < n; i++){ ");
            _prefixInc();
            _printStream.println(_prefix + "if (arr[i] > max)");
            _prefixInc();
            _printStream.println(_prefix + "max = arr[i];");
            _prefixDec();
            _prefixDec();
            _printStream.println(_prefix + "}");
            _printStream.println(_prefix + "return max; ");
            _prefixDec();
            _printStream.println(_prefix + "}");
    }

    private void _printInnerLoopOfAlgClass(int convNodesSize,String funcPostfix){

        //main func
            _printStream.println("void blocks_algorithm::compute_blocks_per_group" + funcPostfix + "( int num_conv, int gpu_cores, int *size, int *blocksize, bool print_details){");
            _prefixInc();

            //LOCAL VARIABLES
            _printStream.println(_prefix + "int count[num_conv] = " + _getStaticArrInitializer("1",convNodesSize));
            _printStream.println(_prefix + "int n[num_conv] = " + _getStaticArrInitializer("0",convNodesSize));
            _printStream.println(_prefix + "const int max_approximate_number = count_max_approx_number(num_conv, &size[0]);");
            _printStream.println(_prefix + "int approximate[num_conv][max_approximate_number];");
            _printStream.println(_prefix + "int damage_array_init[num_conv];");
            _printStream.println(_prefix + "int damage = 0;");

            _printStream.println();
            //Approximate calc
            _printStream.println(_prefix + "for(int i=0; i<num_conv; i++){");
            _prefixInc();
            _printStream.println(_prefix + "while(count[i]<=size[i]){");
            _prefixInc();
            _printStream.println(_prefix + "if(size[i]%count[i] == 0){");
            _prefixInc();

            _printStream.println(_prefix + "approximate[i][n[i]++]=count[i];");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _printStream.println(_prefix + "count[i]++;");

            _prefixDec();
            _printStream.println(_prefix + "}");

            _printStream.println(_prefix + "if(print_details){");
            _prefixInc();

            _printStream.println(_prefix + "for(int j=0; j<n[i]; j++){");
            _prefixInc();
            _printStream.println(_prefix + "printf(\"%d \", approximate[i][j]);");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _printStream.println(_prefix + "printf(\"n=%d \", n[i]);");
            _printStream.println(_prefix + "printf(\"\\n\");");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _prefixDec();
            _printStream.println(_prefix + "}");

            //Blocksize and damage array
            _printStream.println();
            _printStream.println();
            _printStream.println(_prefix + "for(int i=0; i<num_conv; i++){");
            _prefixInc();
            _printStream.println(_prefix + "blocksize[i]=approximate[i][0];");
            _printStream.println(_prefix + "damage_array_init[i]=size[i]/blocksize[i];");
            _prefixDec();
            _printStream.println(_prefix + "} ");
            _printStream.println();
            _printStream.println(_prefix + "damage = largest(damage_array_init, num_conv);");
            _printStream.println();

            //MANY FOR'S (DYNAMIC PIECE)
            //open many for's
            String ind;
            for(int i = 0; i< convNodesSize; i++){
                ind = "n_" + i;
                _printStream.println(_prefix + "for(int " + ind + " = 0; " + ind + "<n[" + i + "]; "+ ind +"++){");
                _prefixInc();
            }

            //functionality inside many for's
            _printStream.println();
            _printStream.println(_prefix + "int damage_array[num_conv] = {");

            _prefixInc();
            int commaBorder = convNodesSize - 1;
            int commaId = 0;
            for(int i = 0; i< convNodesSize; i++){
                ind = "n_" + i;
                _printStream.print(_prefix + "size[" + i + "]/approximate[" + i + "][" + ind + "]");
                if (commaId<commaBorder)
                    _printStream.println(", ");
                else
                    _printStream.println(" };");
                commaId++;
            }
            _prefixDec();
            _printStream.println();
            _printStream.println(_prefix + "if((largest(damage_array, num_conv)<=damage) && (");
            _prefixInc();

            //conditions loop
            commaId = 0;
                for(int i = 0; i< convNodesSize; i++){
                ind = "n_" + i;
                _printStream.print(_prefix + "approximate[" + i + "][" + ind + "] * approximate[" + i + "][" + ind + "]");
                if (commaId<commaBorder)
                    _printStream.println(" + ");
                else {
                    _printStream.println("<=gpu_cores)) {");
                    _prefixInc();
                }
                commaId++;
            }
            _prefixDec();

            for(int i = 0; i< convNodesSize; i++) {
                 ind = "n_" + i;
                 _printStream.println(_prefix + "blocksize[" + i + "] = approximate[" + i + "][" + ind + "];");
            }

            _printStream.println(_prefix + "damage = largest(damage_array, num_conv);");

            _prefixDec();
            _printStream.println(_prefix + "}");

            //close many for's
            for(int i = 0; i< convNodesSize; i++){
                 ind = "n_" + (convNodesSize - i - 1);
                 _prefixDec();
                _printStream.println(_prefix + "}//" + ind);
            }

            //PRINT DAMAGE
            _printStream.println();
            _printStream.println(_prefix + "if(print_details){");
            _prefixInc();
            _printStream.println(_prefix + "printf(\"damage=%d\\n \", damage);");
            _prefixDec();
            _printStream.println("}");
            _prefixDec();
            _printStream.println("}//block size");

    }

    private void _printOuterLoopOfBlockAlg(Vector<CSDFNode> convNodes, int cpus, int convGroupSize, int convTailGroupSize){
            _printStream.println("void blocks_algorithm::get_blocks(int* blocksize, bool print_details) {");
            _prefixInc();
            int convNodesSize = convNodes.size();
            int groups = convNodesSize/convGroupSize;
            _printStream.println(_prefix + "// exit, if no convolutions found ");
            _printStream.println(_prefix + "int num_conv = " + convNodesSize + ";");
            _printStream.println(_prefix + "if (num_conv == 0)");
            _prefixInc();
            _printStream.println(_prefix + "return;");
            _prefixDec();
            _printStream.println("");

            _printStream.println(_prefix + "//target architecture parameters");
            _printStream.println(_prefix + "int cpus = " + cpus + ";");
            /** TODO: make a param of architecture specificatin*/
            _printStream.println(_prefix + "int gpus = 1;");
            _printStream.println(_prefix + "int gpu_cores[gpus] = {256};");
            _printStream.println();
            _printStream.println(_prefix + "//divide convs in groups");
            _printStream.println(_prefix + "int conv_per_group = std::min(num_conv,cpus);");
            _printStream.println(_prefix + "int groups = num_conv/conv_per_group;");
            _printStream.println();
            if(convTailGroupSize>0) {
                _printStream.println(_prefix + "//tail group is processed separately");
                _printStream.println(_prefix + "int conv_per_tail = num_conv%conv_per_group;");
                _printStream.println();
            }
            _printStream.println(_prefix + "//conv sizes per groups");

            int commaBorder = convNodesSize - 1;
            int commaId = 0;
            int nodeInGroupId = 0;
            int curGroupId = 0;
            Iterator i;
            i = convNodes.iterator();

            //process groups of the same size
            _printStream.println(_prefix + "int size[groups][conv_per_group] = {");
            _prefixInc();
            for(int n=0;n<convGroupSize*groups; n++ ){
                CSDFNode convNode = (CSDFNode) i.next();

                if(nodeInGroupId==0){
                    _printStream.print(_prefix + "{");
                    commaId = 0;
                }

               // else {
                    _printStream.print(_getNodeOutputWidth(convNode));
                    if (commaId < commaBorder)
                        _printStream.print(", ");
                    commaId++;
               // }

                nodeInGroupId++;

                if(nodeInGroupId>=convGroupSize){
                    _printStream.print("}");
                    curGroupId++;
                    if(curGroupId==groups)
                        _printStream.println();
                    else _printStream.println(",");
                    nodeInGroupId = 0;
                    commaId = 0;
                }

            }
            _prefixDec();
            _printStream.println(_prefix + "};");
            _printStream.println();

            //process tail group
            if(convTailGroupSize>0) {
                commaBorder = convTailGroupSize - 1;
                commaId = 0;

                _printStream.print(_prefix + "int size_tail[conv_per_tail] = {");
                while (i.hasNext()){
                    CSDFNode convNode = (CSDFNode) i.next();
                    _printStream.print(_getNodeOutputWidth(convNode));
                    if (commaId < commaBorder)
                        _printStream.print(", ");
                    commaId++;
                }

                _printStream.println("};");
                _printStream.println();
            }

            _printStream.println(_prefix + "//int blocksize[num_conv] = {0}; ");
            _printStream.println();

            _printStream.println(_prefix + "//compute block sizes by groups");
            _printStream.println(_prefix + "for(int i=0; i<groups; i++)");
            _prefixInc();
            _printStream.println(_prefix + "compute_blocks_per_group(conv_per_group, gpu_cores[0], &size[i][0], &blocksize[i*conv_per_group], print_details);");
            _prefixDec();

            if(convTailGroupSize>0){
                _printStream.println(_prefix + "//compute block sizes for tail group");
                _printStream.println(_prefix + "compute_blocks_per_group_tail(conv_per_tail, gpu_cores[0], &size_tail[0], &blocksize[groups*conv_per_group], print_details);");
            }

            _printStream.println("");
            _printStream.println(_prefix + "if (print_details) {");
            _prefixInc();
            _printStream.println(_prefix + "for(int i=0; i<num_conv; i++)");
            _prefixInc();
            _printStream.println(_prefix + "printf(\"blocksize[%d] = %d \\n\", i, blocksize[i]);");
            _prefixDec();
            _prefixDec();
            _printStream.println(_prefix + "} //print");
            _prefixDec();
            _printStream.println("} //get_blocks");
    }

    private void _printMainFuncOfBlockAlg(){
        _printStream.println("/** TODO: move to outer files");
        _printStream.println("int main(void) {");
        _prefixInc();
        _printStream.println(_prefix + "int blocks_num = get_blocks_num();");
        _printStream.println(_prefix + "int blocksizes[blocks_num] = {0};");
        _printStream.println();
        _printStream.println(_prefix + "get_blocks(&blocksizes[0], true);");
        _prefixDec();
        _printStream.println("}*/");
    }
    


    /**
     * Print static array initializer
     * @param initVal array initializer value
     * @param len array len
     */
    private String _getStaticArrInitializer(String initVal, int len){

        StringBuilder sb = new StringBuilder("{");
        for(int i=0; i<len-1; i++) {
            sb.append(initVal);
            sb.append(", ");
        }
        sb.append(initVal);
        sb.append(" };");
        return sb.toString();
    }


    /**
     * Get number of CPU cores (from mapping or default)
     * @return number of CPU cores in mapping, if mapping is
     * provided and default number of CPU cores otherwise
     */
    private Integer _getCPUCoresNum(){
        if(_mapping==null) return _maxCores;

        int cpu_num = 0;
        for (Object procObj: _mapping.getProcessorList()) {
            MProcessor proc = (MProcessor)procObj;
            if (! (proc.getResource() instanceof GPU))
                cpu_num++;
        }
        return cpu_num;
    }

    /**
     * Get node output width
     * @param node CSDF node
     * @return node output width
     */
    private Integer _getNodeOutputWidth(CSDFNode node){
        MemoryUnit output = node.getMemoryUnit("output");
        return output.getShape().getDimSize(0);
    }

    ///////////////        FIFO       ///////////////

    /**
     * FIFO class - describes FIFO communication buffer between two nodes
     * @param dir directory to generate the class file
     */
    public void generateFIFOClassTemplate(String dir){
        try {
            _printStream = FileWorker.openFile(dir, "fifo", "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("");
            _printStream.println("#include \"fifo.h\"");
            _printStream.println("#include <pthread.h>");
            _printStream.println("#include <errno.h>");
            _printStream.println("#include <stdio.h>");
            _printStream.println("#include <stdlib.h>");
            _printStream.println("#include <string.h>");
            _printStream.println("#include <iostream>");

            String firstElem="2";

            if(_generateDNNFuncNA) {
                _printStream.println("#include <mutex>");
                _printStream.println("#include \"types2.h\"");
                firstElem = "FIRST_ELEM";
            }

            _printStream.println("");
            _printStream.println("using namespace std;");
            _printStream.println("");
            _printStream.println(_prefix + "// Copy data from cpu device memory to FIFO");
            _printStream.println("void writeSWF_CPU(void* fifo, void* memobj_cpu, int len, int fifo_size) {");
             prefixInc();
            _printStream.println("");
            _printStream.println(_prefix + "while( FIFOisFull(fifo)) {pthread_yield(); };");
            _printStream.println("");
            _printStream.println(_prefix + "int w_cnt = ((int*)fifo)[0];");
            _printStream.println("");
            _printStream.println(_prefix + "// Will copy data from cpu device memory to FIFO");
            _printStream.println(_prefix + "int i;");
            _printStream.println(_prefix + "for ( i = 0; i < len; i++) {");
            _prefixInc();
            _printStream.println(_prefix+"((int*)fifo)[(w_cnt & 0x7FFFFFFF) + " + firstElem + " + i] = ((int*)memobj_cpu)[i];");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _printStream.println("");
            _printStream.println(_prefix + "w_cnt += len;\t");
            _printStream.println("");
            _printStream.println(_prefix + "if( (w_cnt & 0x7FFFFFFF) == fifo_size ) {");
            _prefixInc();
            _printStream.println(_prefix + "w_cnt &= 0x80000000;");
            _printStream.println(_prefix + "w_cnt ^= 0x80000000;");
            _prefixDec();
            _printStream.println("}");
            _printStream.println("");
            _printStream.println(_prefix + "((int*)fifo)[0] = w_cnt;");
            _prefixDec();
            _printStream.println(_prefix + "}");

            for(int i=0; i<3;i++)
                _printStream.println("");

            _printStream.println(_prefix + "// Copy data from FIFO to cpu device memory");
            _printStream.println("void readSWF_CPU(void* fifo, void* memobj_cpu, int len, int fifo_size) {");
            _prefixInc();
            _printStream.println("");
            _printStream.println(_prefix + "while( FIFOisEmpty(fifo)) { pthread_yield(); };");
            _printStream.println("");
            _printStream.println(_prefix + "int w_cnt = ((int*)fifo)[0];");
            _printStream.println(_prefix + "int r_cnt = ((int*)fifo)[1];");
            _printStream.println("");
            _printStream.println(_prefix + "// Will copy data from FIFO to cpu device memory");
            _printStream.println(_prefix + "int i;");
            _printStream.println(_prefix + "for ( i = 0; i < len; i++) {");
            _prefixInc();
            _printStream.println(_prefix + "((int*)memobj_cpu)[i] = ((int*)fifo)[(r_cnt & 0x7FFFFFFF) + " + firstElem + " + i];");
            _prefixDec();
            _printStream.println("}");
            _printStream.println("");
            _printStream.println(_prefix + "r_cnt += len;");
            _printStream.println("");
            _printStream.println(_prefix + "if( (r_cnt & 0x7FFFFFFF) == fifo_size ) {");
            _prefixInc();
            _printStream.println("");
            _printStream.println(_prefix + "r_cnt &= 0x80000000;");
            _printStream.println(_prefix + "r_cnt ^= 0x80000000;");
            _prefixDec();
            _printStream.println("}");
            _printStream.println(_prefix + "((int*)fifo)[1] = r_cnt;");
            prefixDec();
            _printStream.println("}");

            for(int i=0; i<3;i++)
                _printStream.println("");

            _printStream.println(_prefix + "// Check is FIFO is full");
            _printStream.println(_prefix + "int FIFOisFull(void *fifo){");
            prefixInc();
            _printStream.println("");
            _printStream.println(_prefix + "int r_cnt = ((int *)fifo)[1];");
            _printStream.println(_prefix + "int w_cnt = ((int *)fifo)[0];");
            _printStream.println("");
            _printStream.println(_prefix + "if ( r_cnt == (w_cnt ^ 0x80000000) ){");
            _prefixInc();
            _printStream.println(_prefix + "return (1);");
            _prefixDec();
            _printStream.println(_prefix + "} else {");
            _prefixInc();
            _printStream.println(_prefix + "return(0);");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _prefixDec();
            _printStream.println(_prefix + "}");

            for(int i=0; i<3;i++)
                _printStream.println("");

            _printStream.println(_prefix + "// Check is FIFO is empty");

            _printStream.println("int FIFOisEmpty(void *fifo){");
            _prefixInc();
            _printStream.println(_prefix + "int r_cnt = ((int *)fifo)[1];");
            _printStream.println(_prefix + "int w_cnt = ((int *)fifo)[0];");
            _printStream.println("");
            _printStream.println(_prefix + "if ( w_cnt == r_cnt ){");
            _prefixInc();
            _printStream.println(_prefix + "return (1);");
            _prefixDec();
            _printStream.println(_prefix + "} else {");
            _prefixInc();
            _printStream.println(_prefix + "return(0);");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _prefixDec();

            _printStream.println("}");

            for(int i=0; i<3;i++)
                _printStream.println("");

            _printStream.println(_prefix + "// CPU affinity mask of the thread to the CPU core ");
            _printStream.println("void setaffinity(int core){");
            _printStream.println("");
            _printStream.println("    pthread_t pid = pthread_self();");
            _printStream.println("    int core_id = core;");
            _printStream.println("");
            _printStream.println("//   cpu_set_t: This data set is a bitset where each bit represents a CPU.");
            _printStream.println("  cpu_set_t cpuset;");
            _printStream.println("//  CPU_ZERO: This macro initializes the CPU set set to be the empty set.");
            _printStream.println("  CPU_ZERO(&cpuset);");
            _printStream.println("//   CPU_SET: This macro adds cpu to the CPU set set.");
            _printStream.println("  CPU_SET(core_id, &cpuset);");
            _printStream.println("");
            _printStream.println("//   pthread_setaffinity_np: The pthread_setaffinity_np() function sets the CPU affinity mask of the thread thread to the CPU set pointed to by");
            _printStream.println("//cpuset. If the call is successful, and the thread is not currently running on one of the CPUs in cpuset, then it is migrated to one of those CPUs.");
            _printStream.println("   const int set_result = pthread_setaffinity_np(pid, sizeof(cpu_set_t), &cpuset);");
            _printStream.println("  if (set_result != 0) {");
            _printStream.println("    printf(\"pthread setaffinity failed!\\n\");");
            _printStream.println("  }");
            _printStream.println("");
            _printStream.println("");
            _printStream.println(" //  Check what is the actual affinity mask that was assigned to the thread.");
            _printStream.println(" //  pthread_getaffinity_np: The pthread_getaffinity_np() function returns the CPU affinity mask of the thread thread in the buffer pointed to by cpuset.");
            _printStream.println("  const int get_affinity = pthread_getaffinity_np(pid, sizeof(cpu_set_t), &cpuset);");
            _printStream.println("  if (get_affinity != 0) {");
            _printStream.println("    printf(\"pthread getaffinity failed!\\n\");");
            _printStream.println("  }");
            _printStream.println("");
            _printStream.println("  char *buffer;");
            _printStream.println("  // CPU_ISSET: This macro returns a nonzero value (true) if cpu is a member of the CPU set set, and zero (false) otherwise.");
            _printStream.println("  if (CPU_ISSET(core_id, &cpuset)) {");

            if(_silent)
                _printStream.print("//");
            _printStream.print("    printf(\"Successfully set thread %u to cpu core %d\\n\", pid, core_id);");
            _printStream.println("");
            _printStream.println("  } else {");

            if(_silent)
                _printStream.print("//");
            _printStream.print("    printf(\"failed!\\n\");");
            _printStream.println("");

            _printStream.println("  }");
            _printStream.println("}");
            for(int i=0; i<8;i++)
                _printStream.println("");
            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + "fifo" + " " + e.getMessage());
        }
    }


    /**
     * Generate run class - the entry point of the application
     * @param dir directory to generate the run class
     */
    public void generateRunClassTemplate(String dir){
        try {
            _printStream = FileWorker.openFile(dir, "run", "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("//============================================================================");
            _printStream.println("// Name        : erqian_proj2.cpp");
            _printStream.println("// Author      : Minakova S.  Erqian Tang");
            _printStream.println("// Version     :");
            _printStream.println("// Copyright   : Your copyright notice");
            _printStream.println("// Description : Hello World in C++, Ansi-style");
            _printStream.println("//============================================================================\n");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("#include <iostream>");
            _printStream.println("#include \"appMain.h\"");
            _printStream.println("");
            _printStream.println("using namespace std;");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("int main() {");
            _printStream.println("\t//cout << \"!!!Hello World!!!\" << endl; // prints !!!Hello World!!!");
            _printStream.println("\tappMain app = appMain();");
            _printStream.println("\tapp.main();");
            _printStream.println("\treturn 0;");
            _printStream.println("}");
            _printStream.println("");
            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + "run" + " " + e.getMessage());
        }
    }

     /******************************************/
     /**   cpp beginnings                     */

    /**
     * mention all available nodes
     * @param csdfG CSDF graph
     */
    protected void  _addAllClassesHeaders(CSDFGraph csdfG){
         Iterator i = csdfG.getNodeList().iterator();
        while (i.hasNext()) {
            CSDFNode node = (CSDFNode) i.next();
            _printStream.println("#include \""+ node.getName() + ".h\"");
        }
    }


    /**
     * Write common beginning for all generated nodes, contains:
     *  - definition of header
     *  - definition of standard libraries
     *  - definition of namespace
     * @param className name of the .cpp class
     */
    @Override
    protected void _writeCommonCppBeginning(String className){
        _printStream.println("// File automatically generated by ESPAM");
        _printStream.println("");
        _printStream.println("#include \""+ className + ".h\"");
        _printStream.println("#include <stdlib.h>");
        _printStream.println("#include <iostream>");
        _printStream.println("#include \"" + _baseClassName + ".h\"");
        _printStream.println("#include \""+_mainClassName+".h\"");
        _printStream.println("#include \""+_funcClassName+".h\"");
        /** include existing primitives definition*/
        _printStream.println("#include \"fifo.h\"");
        _printStream.println("#include <cstddef>");
        _printStream.println("#include \"types.h\"");
        //include null
        _printStream.println("#include <vector>");
        _printStream.println("#include <string>");
        //include weights loader
        _printStream.println("#include \"dataLoader.h\"");
        //use standard namespance
        _printStream.println("using namespace std;");
        _printStream.println("");
    }

    /**
     * Write application main class .cpp beginning
     */
    protected void _writeMainClassCPPBeginning(){
        _printStream.println("#include <stdlib.h>");
        _printStream.println("#include <iostream>");
        _printStream.println("#include <map>");
        _printStream.println("#include <vector>");
        _printStream.println("#include \"" + _baseClassName + ".h\"");
        _printStream.println("#include \""+_mainClassName+".h\"");
        _printStream.println("#include \""+_funcClassName+".h\"");
        /** include existing primitives definition*/
        _printStream.println("#include \"fifo.h\"");
        _printStream.println("#include <cstddef>");
        _printStream.println("#include <thread>");
        _printStream.println("#include \"dataLoader.h\"");
        if(_generateDNNFuncNA){
        _printStream.println("#include <unistd.h>");
        _printStream.println("#include \"dnnFunc.h\"");
        _printStream.println("#include <chrono>");
        }
        else {
            _printStream.println("#include \"types.h\"");
            _printStream.println("#include <sys/time.h>");
        }
        _printStream.println("#include \"blocks_algorithm.h\"");
        _printStream.println("");
        _printStream.println("using namespace std;");
        _printStream.println("");
    }

    /**
     * Write application main class .cpp beginning
     */
    protected void _writeFuncClassCPPBeginning(boolean incDnnFunc){

         /** TODO: should I define any libraries in here?? Or they will
          * TODO be copied from the graphName.so file?*/
        _printStream.println("#include <stdlib.h>");
        _printStream.println("#include <iostream>");
        _printStream.println("#include <string>");
        _printStream.println("#include <vector>");
        _printStream.println("#include <map>");
        _printStream.println("#include \"" + _funcClassName + ".h\"");
        _printStream.println("#include \"types.h\"");
        /**TODO replace by real API*/
        if(incDnnFunc)
            _printStream.println("#include \"dnnFunc.h\"");
        /** include existing primitives definition*/
        _printStream.println("#include <cstddef>");
        _printStream.println("using namespace std;");
        _printStream.println("");
    }

    /*******************************************************************/
    /********     constructors and destructors of the classes   *******/

    /**
     * Write constructor and destructor .cpp definitions
     * @param className name of the .cpp class
     */
    @Override
    protected void  _writeCppConstructorAndDestructor(String className, String baseClassName){
        _printStream.println(_prefix + className + "::" + className + "() : " + _baseClassName + "() {}");
        _printStream.println(className + "::~" + className + "() {}");
        _printStream.println("");
    }

    /**
     * Write constructor and destructor .cpp definitions
     * @param node CSDF node
     */
    protected void  _writeCppConstructorAndDestructor(CSDFNode node){
        String className = node.getName();
        _printStream.println(_prefix + className + "::" + className + "() : " + _baseClassName + "() {");
      //  _printStream.println(_prefix + "std::cout<<\" "+node.getName()+" ... \"<<endl;");
        prefixInc();
        _writeFIFOsizes(node);
        _fillInIntParams(node);
        _fillInTensorParams(node);
        _loadParameters(node);
        prefixDec();
      //  _printStream.println(_prefix + "std::cout<<\" constructed! \"<<endl;");
        _printStream.println(_prefix + "}");
        _printStream.println(className + "::~" + className + "() {");
        _prefixInc();
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println("");
    }

    /**
     * Load external parameters such as weights and biases
     * @param node CSDF node
     */
    protected void _loadParameters(CSDFNode node){
       if(node.getOperator()==null)
           return;
       if(!node.getOperator().hasTensorParams())
           return;

       TreeMap<String,Tensor> tensorParams = node.getOperator().getTensorParams();

        Tensor weights = tensorParams.get("weights");
        Tensor bias = tensorParams.get("bias");

        if(weights==null && bias==null)
            return;

        _printStream.println(_prefix + "/** parameters load */");
        _printStream.println(_prefix + "dataLoader dl = dataLoader();");

        if (weights != null) {
            if (_initWeightsDummy)
                    _initArrDummyLinear(weights.getDimensionality(), "w", "weights", 1);

                 /** TODO: refactoring*/
                 else {
                     Integer neuronStartId = 0;
                     if(node.getOperator().hasIntegerParams()) {
                        neuronStartId = node.getOperator().getIntParams().get("neuron_start_id");
                        if(neuronStartId == null)
                            neuronStartId = 0;
                     }

                _loadWeightsFromFiles(weights, node.getName(), node.getKernelsNum(), node.getFunction(), neuronStartId);
            }
        }

        if (bias!=null){
            if (_initWeightsDummy)
                    _initArrDummyLinear(bias.getDimensionality(), "b", "bias", 1);
                 else
                    _loadBiasFromFiles(node);
        }
        if(node.getFunction().contains("BN")){
            _loadLinearParamFromFiles(node, "scale");
            _loadLinearParamFromFiles(node, "variance");
            _loadLinearParamFromFiles(node, "mean");
        }
    }

     /**
     * Load weights from external files
     * @param node  node, containing bias
     */
    private void _loadBiasFromFiles(CSDFNode node){
        Tensor bias = node.getOperator().getTensorParams().get("bias");
        if(Tensor.isNullOrEmpty(bias))
            return;

        _printStream.println(_prefix + "/** bias load */");
        String biasSrcName = _getWeightsPrefixName( node.getName());

        if(node.getOperator().hasStringParams()) {
            String biasRef = node.getOperator().getStringParams().get("bias_ref");
            if(biasRef!=null)  biasSrcName = biasRef;
        }

        _printStream.println(_prefix + "std::string bias_path = \"./../../weights_npz/" + biasSrcName + "_b\";");
        _printStream.println(_prefix + "dl.data_load_from_numpy(bias_path, (-1), bias_len, neuron_start_id, 0, &bias[0]);");

        /** TODO: refactoring!*/
        _printStream.println();
        if(node.getOperator().hasStringParams()){
            String add_ref = node.getOperator().getStringParams().get("add_ref");
            if(add_ref!=null){
                _printStream.println(_prefix + "std::string add_path = \"./../../weights_npz/" +
                        node.getOperator().getStringParams().get("add_ref") + "_b\";");
                _printStream.println(_prefix + "dl.data_load_from_numpy(add_path, (-1), add_len, neuron_start_id, 0, &add[0]);");

            }
        }
    }

    /**
     * Load weights from external files
     * @param node  node, containing bias
     */
    private void _loadLinearParamFromFiles(CSDFNode node, String paramName){
        _printStream.println(_prefix + "/** " + paramName + " load */");
        String srcPrefix = _getWeightsPrefixName( node.getName());
        _printStream.println(_prefix + "std::string "+ paramName +"_path = \"./../../weights_npz/" + srcPrefix + "_" + paramName + "\";");
        _printStream.println(_prefix + "dl.data_load_from_numpy("+ paramName + "_path, (-1), "+ paramName +"_len, neuron_start_id, 0, &" + paramName + "[0]);");
    }

    /**
     * Load weights from external files
     * @param tensor espam. Tensor
     * @param nodeName name of the node, containing weights
     * @param kernels number of kernels in the node
     * @param operation operation, performed by the node
     */
    private void _loadWeightsFromFiles(Tensor tensor, String nodeName, int kernels, String operation, int neuronStartId){
        String weightsPrefixName = _getWeightsPrefixName(nodeName);
        if(operation.toLowerCase().contains("conv"))
            _loadWeightsFromFilesConv(tensor,weightsPrefixName,kernels);
        else
            _loadWeightsFromFilesDense(tensor,weightsPrefixName);
    }

    /**
     * Load weights from external files
     * @param tensor espam. Tensor

    private void _loadWeightsFromFilesDense(Tensor tensor, String weightsPrefixName, int neuronStartId){
        try {
               //int partition_size = (tensor.getDimSize(1)*_partition_size);
               //math.floor(neurs/partition_size)
               int neurs = tensor.getDimSize(0);
               int partitions = (neurs/_partition_size);
               int inp_size = 0;
               for(int i=1;i<tensor.getDimensionality();i++)
                     inp_size+=  tensor.getDimSize(i);

               int partition_size = (_partition_size * inp_size);
               int tail;
               int last_partition_size = 0;
               Integer startBlockId = 0;
               Integer neuronShiftInBlock = neuronStartId % _partition_size;
               int first_partition_size = 0;
               int shiftInBlock = neuronShiftInBlock * inp_size;

               if(neuronStartId>0) {
                   Double startBlockIdDouble = (((Math.ceil((double) neuronStartId / (double) _partition_size))) - 1);
                   startBlockId = startBlockIdDouble.intValue();
                   first_partition_size = partition_size - shiftInBlock;

                   if(first_partition_size>neurs*inp_size) {
                       first_partition_size = neurs * inp_size;
                       partitions = 1;
                   }
               }

               tail = tensor.getElementsNumber() - partitions * partition_size - first_partition_size;

               if(tail<0){
                   tail+=partition_size;
                   partitions--;
               }

               if(tail>0) {
                   partitions = partitions + 1;
                   last_partition_size = tail;
               }


               _printStream.println(_prefix + "/** weights load ");
               _printStream.println(_prefix + "std::string weights_path_prefix = \"./../../weights_npz/" + weightsPrefixName + "_w\";");
               _printStream.println(_prefix + "int partition_size = " + partition_size + ";");
               _printStream.println(_prefix + "int partitions_num = " + partitions + ";");
               _printStream.println(_prefix + "int first_partition_size = " + first_partition_size + ";");
               _printStream.println(_prefix + "int last_partition_size = " + last_partition_size + ";");


               _printStream.println(_prefix + "int start_block_id = " + startBlockId.toString() + ";");

               _printStream.println(_prefix + "/** shift of the partition beginning from the beginning of the weights array ");
               _printStream.println(_prefix + "int shift = 0; ");

               _printStream.println(_prefix + "int shift_in_block = " + neuronShiftInBlock +" * input_len;");
               _printStream.println(_prefix + "int start = 0;");

               /** FIRST PARTITION PROCESSING
               Integer midPartitionsStartId = 0;

               if(first_partition_size>0){
                  _printStream.println();
                  _printStream.println(_prefix + "/** first partition processing ");
                   midPartitionsStartId++;
                  _printStream.println(_prefix + "dl.data_load_from_numpy(weights_path_prefix, start_block_id, first_partition_size, shift_in_block, shift, &weights[0]);");
                  _printStream.println(_prefix + "shift += partition_size - shift_in_block;");
               }


            /** MID PARTITIONS PROCESSING

           if( (partitions - 1)>0) {
               _printStream.println();
               _printStream.println(_prefix + "/** partitions processing ");
               _printStream.println(_prefix + "for(int i=" + midPartitionsStartId + "; i<partitions_num - 1 ;i++) {");
               prefixInc();
               _printStream.println(_prefix + "dl.data_load_from_numpy(weights_path_prefix, i + start_block_id, partition_size, start, shift, &weights[0]);");
               _printStream.println(_prefix + "shift += partition_size;");
               // _printStream.println(_prefix + "start+= block_id * partition_size;");
               prefixDec();
               _printStream.println(_prefix + "}");
           }

               /** LAST PARTITION
               if((partitions-1)>=0) {
                   _printStream.println();
                   _printStream.println(_prefix + "/** last partition processing ");
                   _printStream.println(_prefix + "dl.data_load_from_numpy(weights_path_prefix, " + ((partitions - 1) + startBlockId ) + ", last_partition_size, start, shift, &weights[0]);");
               }

            }
            catch (Exception e){
               //_printStream.println("0 };");
               _printStream.println("//ERROR: weights load error " + e.getMessage());
            }
    }
    */

    /**
     * Load weights from external files
     * @param tensor espam. Tensor
     */
    private void _loadWeightsFromFilesDense(Tensor tensor, String weightsPrefixName){
        try {
               //int partition_size = (tensor.getDimSize(1)*_partition_size);
               //math.floor(neurs/partition_size)
               int neurs = tensor.getDimSize(0);
               int partitions = (neurs/_partition_size);
               int partition_size;
               int tail;
               int last_partition_size = 0;

               partition_size = (_partition_size * tensor.getDimSize(1));
               tail = tensor.getElementsNumber() - partitions * partition_size;


               if(tail>0) {
                   partitions = partitions + 1;
                   last_partition_size = tail;
               }

               _printStream.println(_prefix + "/** weights load */");
               _printStream.println(_prefix + "std::string weights_path_prefix = \"./../../weights_npz/" + weightsPrefixName + "_w\";");
               _printStream.println(_prefix + "int partition_size = " + partition_size + ";");
               _printStream.println(_prefix + "int partitions_num = " + partitions + ";");
               _printStream.println(_prefix + "/** shift of the partition beginning from the beginning of the weights array */");
               _printStream.println(_prefix + "int shift = 0; ");

               _printStream.println(_prefix + "/** partitions processing */");
               _printStream.println(_prefix + "for(int i=neuron_start_id; i<partitions_num-1;i++) {");
               prefixInc();
               _printStream.println(_prefix + "dl.data_load_from_numpy(weights_path_prefix, i, partition_size, neuron_start_id, shift, &weights[0]);");
               _printStream.println(_prefix + "shift += partition_size;");
               prefixDec();
               _printStream.println(_prefix+ "}");
               _printStream.println(_prefix + "/** last partition processing */");
               _printStream.println(_prefix + "dl.data_load_from_numpy(weights_path_prefix, " + (partitions-1) + ", " + last_partition_size + ", neuron_start_id, shift, &weights[0]);");

            }
            catch (Exception e){
               //_printStream.println("0 };");
               _printStream.println("//ERROR: weights load error " + e.getMessage());
            }


    }


    /**
     * Load weights from external files
     * @param tensor espam. Tensor
     */
    private void _loadWeightsFromFilesConv(Tensor tensor, String weightsPrefixName, int kernels){
        //_printStream.print(_prefix + typeDesc + " " + arrname + "[" + tensor.getElementsNumber() + "] = { ");

           try {
               int partitions = kernels;
               _printStream.println(_prefix + "/** weights load */");

               if(_mocMode){
                _printStream.println(_prefix + "for(int i=0; i<weights_len; i++)");
                prefixInc();
                _printStream.println(_prefix + "weights[i] = 0.1f; ");
                prefixDec();
               }

               else {

                   _printStream.println(_prefix + "std::string weights_path_prefix = \"./../../weights_npz/" + weightsPrefixName + "_w\";");
                   _printStream.println(_prefix + "int partition_size = " + tensor.getElementsNumber() / kernels + ";");
                   _printStream.println(_prefix + "int partitions_num = " + partitions + ";");
                   _printStream.println(_prefix + "/** shift of the partition beginning from the beginning of the weights array */");
                   _printStream.println(_prefix + "int shift = 0; ");
                   _printStream.println(_prefix + "for(int i=neuron_start_id; i<partitions_num + neuron_start_id; i++){ ");
                   prefixInc();
                   _printStream.println(_prefix + "dl.data_load_from_numpy(weights_path_prefix, i, partition_size, 0, shift, &weights[0]);");
                   _printStream.println(_prefix + "shift += partition_size;");
                   prefixDec();
                   _printStream.println(_prefix + "}");
               }
            }
            catch (Exception e){
               //_printStream.println("0 };");
               _printStream.println("//ERROR: weights load error " + e.getMessage());
            }
    }

    /**
     * Get the weights prefix from the node name
     * @param nodeName node name
     * @return weights prefix, extracted from the node name
     */
    protected String _getWeightsPrefixName(String nodeName){
    if(!nodeName.contains("_split"))
        return nodeName;

    /** remove split part*/
    int splitStart = nodeName.indexOf("_split");
        return nodeName.substring(0,splitStart);

    }

         /**
     * Write constructor and destructor .cpp definitions
     * @param className name of the .cpp class
     */
    protected void  _writeNoBaseCppConstructorAndDestructor(String className){
        _printStream.println(className + "::" + className + "() {}");
        _printStream.println(className + "::~" + className + "() {}");
        _printStream.println("");
    }


    /////////////////////////////////////////////////////////////////////
    /////////     main functions of the classes                /////////

    /** func to measure program elapsed time*/
    protected void _writeMainClassTimer(){
        _printStream.println();
        _printStream.println(_prefix + "// Declare the variables for measuring elapsed time");
        _printStream.println(_prefix + "double startTime;");
        _printStream.println(_prefix + "double endTime;");
        _printStream.println(_prefix + "double buildTime;");
        _printStream.println(_prefix + "double inferenceTime;");
        _printStream.println();
        _printStream.println(_prefix + "double appMain::getMicroSecond( void ) {");
        _prefixInc();
        _printStream.println(_prefix + "double sec;");
        _printStream.println();
        _printStream.println(_prefix + "struct timeval timev;      // time value");
        _printStream.println(_prefix + "struct timezone timez;     // time zone");
        _printStream.println();
        _printStream.println(_prefix + "if ( gettimeofday( &timev, &timez ) == -1 ) {");
        _prefixInc();
        _printStream.println(_prefix + "std::cerr << \"Could not get time by gettimeofday().\" << std::endl;");
        _printStream.println(_prefix + "exit(1);");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println();
        _printStream.println(_prefix + "// the unit of returned value is second");
        _printStream.println(_prefix + "sec = static_cast<double>(timev.tv_sec) + static_cast<double>(timev.tv_usec) * 1e-6;");
        _printStream.println(_prefix + "return sec;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println();
        _printStream.println();

    }

    /*****************************************************************************
     * Write main function of the main class (class contains the application logic)
     * @param csdfg CSDF graph, represents the application
     ****************************************************************************/

    protected void _writeMainClassMain(CSDFGraph csdfg){
        _printStream.println("// Main function");
        _prefixInc();
        _printStream.println(_prefix + "void " + _mainClassName + "::main()");
         prefixInc();
         _printStream.println(_prefix + "{");
         prefixInc();
         /** TODO: move to after construction?*/
         _startAppMainTimer();

         _printStream.println(_prefix + "// list of all available nodes");
         _printStream.println(_prefix + "int total_nodes = " + csdfg.countNodes() + ";");
         _printStream.println(" ");
         _printStream.println(_prefix + "std::map< std::string, "+ _baseClassName +
                 "* > nodes = std::map< std::string," + _baseClassName + "* >();");
         _createAllClassesInstancesAndRefs(csdfg);
         _printStream.println("");
        _computeGPUBlockSizes(csdfg);
        _printStream.println("");
        _printStream.println(_prefix + "// Prepare shared data and memory");
        _defineFIFOChannels(csdfg);
        _printStream.println(_prefix + "// Preparation work for threads on CPU");
        _prepareThreads();
        _fillInThreadInfo(csdfg);
        _printStream.println("");
        if(!_silent)
            _printStream.print(_prefix + "std::cout<<\" Application topology is constructed! \"<<std::endl;");
        _appConstructionTimer();


        _printStream.println("");

        _createAndRunThreads(csdfg);
         _printStream.println("");
         _joinThreads();
         prefixDec();
         _endAppMainTimer();
         _printStream.println(_prefix + "}//main");
         prefixDec();
        _prefixDec();
        _printStream.println("");
    }

    /** find out the application construction end time*/
    private void _appConstructionTimer(){
        _printStream.println();
       // _printStream.println(_prefix + "endTime = getMicroSecond();");
        _printStream.println(_prefix + "buildTime = getMicroSecond() - startTime;");
        if(!_silent)
        _printStream.println(_prefix + "std::cout<<\"App construction time: \"<<appConstrTime<<std::endl;");
        _printStream.println();

    }

///// Timers ////
    /** use program timer in appmain*/
    private void _startAppMainTimer(){
        _printStream.println();
        _printStream.println(_prefix + "startTime = getMicroSecond();");
        _printStream.println();
    }

    /** use program timer in appmain*/
    private void _endAppMainTimer(){
        _printStream.println();

        /**
         *


    std::cout<<"Program execution time: "<<pTime<<std::endl;
    std::cout<<"Build time: "<<buildTime<<std::endl;
    std::cout<<"Inference time: "<<inferenceTime<<std::endl;
         *
         */

        _printStream.println(_prefix + "inferenceTime = getMicroSecond() - startTime - buildTime;");
        _printStream.println(_prefix + "endTime = getMicroSecond();");
        _printStream.println(_prefix + "double pTime = endTime - startTime;");
        _printStream.println(_prefix + "std::cout<<\"Program execution time: \"<<pTime<<std::endl;");
        _printStream.println(_prefix + "std::cout<<\"Build time: \"<<buildTime<<std::endl;");
        _printStream.println(_prefix + "std::cout<<\"Inference time: \"<<inferenceTime<<std::endl;");
    }


    /**
     * Create instances of all node and references on them
     * @param csdfG csdf graph
     */
    protected void _createAllClassesInstancesAndRefs(CSDFGraph csdfG){
        Iterator i = csdfG.getNodeList().iterator();
        String nodename;
        while (i.hasNext()) {
            CSDFNode node = (CSDFNode) i.next();
            nodename = node.getName();
            _printStream.println(_prefix + nodename + " " + nodename + "_inst = " + nodename + "();");
            _printStream.println(_prefix + "nodes[\"" + nodename + "\"] = &" + nodename + "_inst;");
        }

    }

    private void _computeGPUBlockSizes(CSDFGraph csdfg){
        if(_mocMode){
             _printStream.println(_prefix + "//assign GPU block sizes");
        Vector<CSDFNode> convNodes = csdfg.getNodesList("conv");
        for(CSDFNode node:convNodes) {
            _printStream.println(_prefix + node.getName()+"_inst.int_params[\"block_size\"] = 1;");
        }

        }

            else {
            _printStream.println(_prefix + "//compute GPU block sizes");
            _printStream.println(_prefix + "blocks_algorithm ba = blocks_algorithm();");
            _printStream.println(_prefix + "int conv_blocks_num = ba.get_blocks_num();");
            _printStream.println(_prefix + "if (conv_blocks_num == 0)");
            _prefixInc();
            _printStream.println(_prefix + "conv_blocks_num = 1;");
            _prefixDec();
            _printStream.println(_prefix + "int blocksizes[conv_blocks_num] = {0};");
            _printStream.println();
            _printStream.println(_prefix + "ba.get_blocks(&blocksizes[0], false);");
            _printStream.println();
            _printStream.println(_prefix + "//assign GPU block sizes");
            Vector<CSDFNode> convNodes = csdfg.getNodesList("conv");
            int blockSizeId = 0;
            for (CSDFNode node : convNodes) {
                _printStream.println(_prefix + node.getName() + "_inst.int_params[\"block_size\"] = blocksizes[" + blockSizeId + "];");
                blockSizeId++;
            }
        }
    }

    //////////////////         communication channels             /////////////////////

      /**
     * define fifo channels sizs in ints from CSDF graph edges
     */
    protected void _defineFIFOChannels(CSDFGraph csdfg){
        _printStream.println(" ");
        _printStream.println(_prefix + "//Size of one token of FIFO in ints");
        _printStream.println(_prefix +  "int token_len=sizeof(long)/sizeof(int)+(sizeof(long)%sizeof(int)+(sizeof(int)-1))/sizeof(int);");
        _printStream.println(" ");
        _printStream.println(_prefix + "//fifo channels definition and initialization. Fifo sizes are given in tokens.");
        _printStream.println(_prefix +  "std::vector<fifo_buf> fifos = std::vector<fifo_buf>();");
        _printStream.println(" ");
        _printStream.println(_prefix + "//scale FIFO sizes for pipeline parallelism");
        _printStream.println(_prefix + "int FIFO_SCALE = " +_fifoScale.toString() + ";");
        _printStream.println("");

         Iterator i = csdfg.getEdgeList().iterator();
        while (i.hasNext()) {
            CSDFEdge edge = (CSDFEdge) i.next();
            if(!(edge.getSrc().isOverlapHandler() ||edge.getDst().isOverlapHandler())) {
                _defineFIFOChannel(edge);
            }
        }
    }
    /**
     * Define bufferized FIFO channel
     * @param edge CSDF graph node
     */
    protected void _defineFIFOChannel(CSDFEdge edge){

        CSDFPort edgeSrc = edge.getSrc();
        CSDFPort edgeDst = edge.getDst();

        String srcFullName = edgeSrc.getNode().getName() + "_" +edgeSrc.getName();
        String dstFullName = edgeDst.getNode().getName() + "_" +edgeDst.getName();

        int fifoId = edge.getId();
        String fifoName  = "fifo_" + edge.getId();
        String fifoSize = "fifo_size_"+ fifoId;
        String firstElem = "2";
        if(_generateDNNFuncNA)
            firstElem = "FIRST_ELEM";


        //datatype should be the same for src/dst ports. If it is not defined, it is set to int by default.
        String dataType = getDataType(edgeSrc);

        _printStream.println(_prefix + "// FIFO " + srcFullName  + "-->" + dstFullName );


        _printStream.println(_prefix + "void *" + fifoName +"=NULL;");
        _printStream.println(_prefix + "int "+ fifoSize +
                " = max(" +
                edgeSrc.getNode().getName() + "_inst." + edgeSrc.getName() + "_fifo_size ," +
                edgeDst.getNode().getName() + "_inst." + edgeDst.getName() + "_fifo_size) * FIFO_SCALE;");


      _printStream.println(_prefix + fifoName +" = calloc("+ fifoSize + " +" + firstElem + ", sizeof(" + dataType + "));");


      _printStream.println(_prefix + "struct fifo_buf buf_" + fifoId + " = fifo_buf ("+ fifoName +","
              + fifoSize + ", \"" + srcFullName + "\" , \"" + dstFullName + "\");");

      _printStream.println(_prefix + "fifos.push_back(buf_" + fifoId + ");");

      _printStream.println(" ");
    }

    //////////////////             threads             /////////////////////

    /** preparation work for pthreads on CPU */
    protected void _prepareThreads(){
     _printStream.println("");
     _printStream.println(_prefix + "// Allocate memory for pthread_create() arguments");
     _printStream.println(_prefix + "const int num_threads = total_nodes;");
     _printStream.println(_prefix + "struct thread_info *thread_info = (struct thread_info*)(calloc(num_threads, sizeof(struct thread_info)));");
     _printStream.println("");
     _printStream.println(_prefix + "// Main threads");
    }

      /**
     * Preapare information, necessary for each thread running
     * As each thread runs one graph node, the thread info
       * is filled for every node of the graph
     * @param csdfg CSDF graph
     */
     protected void _fillInThreadInfo(CSDFGraph csdfg){
        Iterator i = csdfg.getNodeList().iterator();
        int threadId = 0;
        while (i.hasNext()) {
            CSDFNode node = (CSDFNode) i.next();
            _fillInThreadInfo(node,threadId);
            threadId++;
        }

     }

    /**
     * Fill in thread info for one CSDF node
     * @param node CSFD node
     */
    protected void _fillInThreadInfo(CSDFNode node, int threadId){
        _printStream.println("");
        _printStream.println(_prefix + "//" + node.getName() + " thread info");
         Integer core = _assignCore(node.getName());

        _printStream.println(_prefix + "thread_info[" + threadId +"].core_id = " + core + ";");

         for(CSDFPort inport: node.getNonOverlapHandlingInPorts()){
             _fillInThreadInfo(inport,false);
         }

         for(CSDFPort outport: node.getNonOverlapHandlingOutPorts()){
             _fillInThreadInfo(outport,true);
         }
    }

    /** fill in thread info for one CSDF port
     * @param port CSDF port
     * @param isOutPort is port an output port
     */
    protected void _fillInThreadInfo(CSDFPort port, boolean isOutPort){
        int threadId = _nodes.indexOf(port.getNode().getName());

        String fifoBufSearchF = _funcClassName + "::get_buf_by_";
        if(isOutPort)
            fifoBufSearchF += "src";
        else fifoBufSearchF+= "dst";

        String fullPortName = port.getNode().getName() + "_" + port.getName();

        _printStream.println(_prefix + "fifo_buf* buf_ref_" + fullPortName + " = " + fifoBufSearchF + "( \"" + fullPortName + "\", fifos);");
        _printStream.println(_prefix + "thread_info[" + threadId + "].add_fifo_buf_ref( buf_ref_" + fullPortName + ");");
    }

    /**
     * Prepare and run fifo threads
     * @param csdfg CSDF graph
     */
     protected void _createAndRunThreads(CSDFGraph csdfg){

         Iterator i = csdfg.getNodeList().iterator();
         int threadId = 0;
        while (i.hasNext()) {
            CSDFNode node = (CSDFNode) i.next();
                String nodeName = node.getName();
                _createAndRunThread(threadId, "thread_" + nodeName, nodeName, nodeName);
                threadId++;
        }
     }

      /**
     * Create and run thread
     * @param threadId thread id
     * @param threadName thread Name
     * @param nodeClassName node class name
     * @param nodeName node name
     */
    protected void _createAndRunThread(int threadId, String threadName, String nodeClassName, String nodeName){
         _printStream.println(_prefix + "// Create and run " + threadName);
         _printStream.println(_prefix + "std::thread " + threadName +
                 "(&" + nodeClassName + "::main, &" + nodeName +"_inst" + ", &thread_info[" + threadId + "]);");

         if(!_silent)
            _printStream.print(_prefix + "cout<<\"Joined with thread " + threadName + "\"" + "<<endl;");
         _printStream.println("");
    }

    /** Join all the threads*/
    protected void _joinThreads(){
        if(_nodes == null)
            return;
        _printStream.println(_prefix + "// Join threads that should be awaited");
        for(String node: _nodes){
            _printStream.println(_prefix + "thread_" + node + ".join();");
        }

    }


     /**********************************************
     * Write main function for the operational node
     * @param y SDF Node
     **********************************************/
    @Override
    protected void _writeMain(CSDFNode y) {
        _printStream.println("void " + y.getName() + "::main(void *threadarg) {");
        _prefixInc();

        _accessCommunicationChannel(y);
        _accessExternalData(y);

        _printStream.println(_prefix + "// repetition parameters definition");
        _printStream.println(_prefix + "int q = " + y.getRepetitions() + ";");
        _printStream.println(_prefix + "int phase_len = " + y.getLength() + ";");
        _printStream.println(_prefix + "int phase; ");
         _printStream.println(" ");

        _openLoopCondition();

        _prefixInc();
        _printStream.println(_prefix + "// loop over the repetitions number");
        _printStream.println(_prefix + "for (int rep = 0; rep < q ; rep ++) {");
        _prefixInc();
        _printStream.println(_prefix + "phase = rep % phase_len;");
        _processReading(y);
       // _printStream.println("std::cout<<\"" + y.getName() + " called\"<<std::endl;");
        _processExecution(y);
       // _printStream.println("std::cout<<\"" + y.getName() + " performed\"<<std::endl;");
        _processWriting(y);
        _prefixDec();
        _printStream.println(_prefix + "}// loop over the phases");
        _prefixDec();
        if(!_silent)
            _printStream.print(_prefix + "cout<<\" " + y.getName() + " finished! \"<<endl;");
        _printStream.println("");

        _closeLoopCondition();

        _prefixDec();
        _printStream.println(_prefix + "} // main");
    }

    /**
     * Open node loop condition
     */
    protected void _openLoopCondition(){
        if(_batch==-1 || _batch == null)
            _printStream.println(_prefix + "// while (1) {");
        else
            _printStream.println(_prefix + "for (int batch = 0; batch < frames; ++batch) {");
    }

    /**
     * Close node loop condition
     */
    protected void _closeLoopCondition(){
        if(_batch==-1 || _batch == null)
            _printStream.println(_prefix + "//} while (1)");
        else
            _printStream.println(_prefix + "} //batch");
    }

    /////////////////////////////////////////////////////////////////////
    /////////  process node execution phase              ////////////////


    /**
     * process exec function with default name
     * @param node CSDF node
     */
    @Override
    protected void _processExecution(CSDFNode node){
        /** input and output nodes are processed in appMain*/
        if(_isIONode(node))
            return;
        /** concat node is processed separately too*/
        if(node.getName().toLowerCase().contains("concat")){
            //_processExecutionConcat(node);
            return;
        }
        /** now we perform all neurons execution at once*/
        _processExecution(node,_funcClassName + "::execute");
    }
        /**
     * Checks if the CSDF graph node is input or output node
     * @return true, if CSDF graph node is input or output node and false otherwise
     */
    protected boolean _isIONode(CSDFNode node){
        if(node.isSrc()||node.isSnk())
            return true;
        return false;
    }

    /**
     * Get inputs sorted in concatenation order
     * @param node CSDF node
     * @return input ports, sorted in concatenation order
     */
    private Vector<CSDFPort> _getConcatSortedInputPorts(CSDFNode node){
        Vector<CSDFPort> sortedInputPorts = new Vector<>();

        HashMap<MemoryUnit,CSDFPort> inputs = new HashMap<>();

        for(CSDFPort inport: node.getNonOverlapHandlingInPorts()){
            MemoryUnit mu = inport.getAssignedMemory();
           // System.out.println(mu.getName());
            if(mu!=null){
                inputs.put(mu,inport);
            }
        }

        for(MemoryUnit mu: node.getMemoryUnits()){
            if(inputs.containsKey(mu))
                sortedInputPorts.add(inputs.get(mu));
        }

        //for(CSDFPort p: sortedInputPorts)
        //  System.out.println(p.getName());

        return sortedInputPorts;
    }

      /**
     * Process execution primitive
     * @param node CSDF node
     * @param execPrimitiveName name of the execution primitive
     */
    protected void _processExecution(CSDFNode node,String execPrimitiveName){
      _printStream.println("");
      _printStream.println(_prefix + "//execution ");
      _processExecution(node.getOperator(),null);
    }

    /**
     * Process execution primitive
     * @param operator CSDF node operator
     * @param suboperatorID operator sequential Id (for compelx operators)
     */
    protected void _processExecution(Operator operator, Integer suboperatorID){
      if(operator==null)
          return;

      if(operator instanceof ComplexOperator){
          _processExecutionComplex((ComplexOperator) operator);
          return;
      }

    String parPrefix = "";
    if(suboperatorID!=null)
        parPrefix = "_" + suboperatorID.toString();

      String opName = operator.getName();
      _printStream.println(_prefix + "appFunc::execute(std::string(\"" + opName + "\"),"+
       "&tensor_params" + parPrefix + ", &int_params" + parPrefix + ");");

    }


     /**
     * Process execution primitive
     * @param operator Complex CSDF node operator
     */
    protected void _processExecutionComplex(ComplexOperator operator) {
      if(operator.isCompound())
      {
      String opName="";
      Vector<Operator> subOps = operator.getSubOperators();
      for (int i=0; i<subOps.size()-1;i++)
        opName+=subOps.elementAt(i).getName() + "+";
      opName+=subOps.lastElement().getName();

      _printStream.println(_prefix + "appFunc::execute(std::string(\"" + opName + "\"),"+
       "&tensor_params, &int_params);");
      }

      else {
          Integer opId = 0;
          for (Operator subOperator : operator.getSubOperators()) {
              _processExecution(subOperator, opId);
              opId++;
          }
      }
    }

    /**
     * Get reference on linear array
     * @param mu corresponding memory unit
     * @return reference on linear array
     */
    private String _getLinearArrRef(MemoryUnit mu){
      if(mu==null)
          return "nullptr";
      return "&" + mu.getName() +"[0]";
    }


    ////////////////////////////////////////////////////////////
    //////   Access to communication channels in the node  /////

    /**
     * Create FIFO communication channel fro CSDF node
     * @param node CSDF node
     */
    protected void _accessCommunicationChannel(CSDFNode node){

        _printStream.println(_prefix + "// create communication channel");
        _printStream.println(_prefix + "thread_info *thread_data;");
        _printStream.println(_prefix + "thread_data = (struct thread_info *) threadarg;");

        for(CSDFPort inport:node.getNonOverlapHandlingInPorts())
            _accessCommunicationChannel(node.getName(),inport.getName(),false);

        for(CSDFPort outport:node.getNonOverlapHandlingOutPorts())
            _accessCommunicationChannel(node.getName(),outport.getName(),true);

        _printStream.println(_prefix + "setaffinity(thread_data->core_id);");
        _defineThreadCoreIdInParams(node.getOperator());
        _printStream.println(" ");
    }

    /**
     * Define thread core id in integer parameters of the node
     * @param operator CSDF node operator
     */
    protected void _defineThreadCoreIdInParams(Operator operator){
        if (operator instanceof ComplexOperator){
            if(!((ComplexOperator) operator).isCompound()) {
                for (int i = 0; i < ((ComplexOperator) operator).getSubOperators().size(); i++) {
                    _printStream.println(_prefix + "int_params_" + i + "[\"core_id\"] = thread_data->core_id;");
                }
            }
        }

        else _printStream.println(_prefix + "int_params[\"core_id\"] = thread_data->core_id;");
    }

    /**
     * Write access to external data
     * @param node CSDF node
     */
    protected void _accessExternalData(CSDFNode node){
        if(node.isSrc()){
            _printStream.println("");
            _printStream.println(_prefix + "//TODO: load your data here");
            _printStream.println(_prefix + "std::string data_path = \"../../data/inputs/input\";");
            _printStream.println(_prefix + "dataLoader dl = dataLoader();");
            _printStream.println(_prefix + "int data_id = 0;");
            _printStream.println(_prefix + "int max_data_id = 0;");
            _printStream.println("");
        }

        if(node.isSnk()){
            _printStream.println("");
            _printStream.println(_prefix + "//TODO: load your expected data here");
            _printStream.println(_prefix + "std::string data_path = \"../../data/outputs/output\";");
            _printStream.println(_prefix + "dataLoader dl = dataLoader();");
            _printStream.println(_prefix + "int data_id = 0;");
            _printStream.println(_prefix + "int max_data_id = 0;");
            /** TODO: rem after tests!*/
            _printStream.println(_prefix + "//timer parameters");
            _printStream.println(_prefix + "std::clock_t start;");
            _printStream.println(_prefix + "double duration;");

            _printStream.println("");
        }
    }

    /**
     * Create FIFO communication channel
     */
    protected void _accessCommunicationChannel(String nodeName, String portName, boolean isOutPort){
        String getByPostfix = "dst";
        if(isOutPort)
            getByPostfix = "src";

        String bufName = nodeName + "_" + portName;
        _printStream.println(_prefix + "fifo_buf* " + bufName +
                "_buf_ptr = thread_data->get_fifo_buf_by_" + getByPostfix +"(\"" + bufName + "\");");
        _printStream.println(" ");
    }



    /**
     * Fill in CSDFNode integer parameters
     * @param node CSDFNode
     */
    protected void _fillInIntParams(CSDFNode node) {
        /** define constant parameters, if any*/
        _fillInIntParams(node.getOperator(),null);
    }

    /**
     * Fill integer parameters for CSDF node
     * @param operator CSDF node operator
     * @param suboperatorId seubOperator Id (for complex operators)
     */
    protected void _fillInIntParams(Operator operator, Integer suboperatorId){
    if(operator==null)
        return;

    if(operator instanceof ComplexOperator) {
        if(!(((ComplexOperator) operator).isCompound())) {
            _fillComplexOperatorIntParams((ComplexOperator) operator);
            return;
        }
    }

    String parPrefix = "";
    if(suboperatorId!=null)
        parPrefix = "_" + suboperatorId.toString();

    _printStream.println();
    _printStream.println("//const int parameters " + parPrefix);

    TreeMap<String,Integer> intParameters = operator.getIntParams();
    for(Map.Entry<String,Integer> intPar: intParameters.entrySet()){
        _printStream.println(_prefix + "int_params"+ parPrefix +"[\""+intPar.getKey()+"\"] = "+intPar.getKey() + parPrefix +";");
      }
    }


    /**
     * Define operator parameters for a complex operator
     * @param operator complex CSDF node operator
     */
    protected void _fillComplexOperatorIntParams(ComplexOperator operator){
        Integer opId = 0;
        for(Operator subOp: operator.getSubOperators()){
           _fillInIntParams(subOp,opId);
            opId++;
        }
    }


    /**
     * Fill in CSDFNode tensor parameters
     * @param node CSDFNode
     */
    protected void _fillInTensorParams(CSDFNode node) {
        _printStream.println("");
        _printStream.println(_prefix + "// tensor parameters");
        _setIOTensorParams(node);
        /** Operator tensor parameters*/
        _fillInTensorParams(node.getOperator(), null);
        _printStream.println("");
    }

    protected void _setIOTensorParams(CSDFNode node){
        String inpTensorParPrefix = "";
        String outpTensorParPrefix = "";

        if(node.getOperator() instanceof ComplexOperator){
            if(!((ComplexOperator) node.getOperator()).isCompound()){
            inpTensorParPrefix = "_0";
            Integer outpOpId = ((ComplexOperator) node.getOperator()).getSubOperators().size() - 1;

            outpTensorParPrefix = "_" + outpOpId;
            }
        }

        _addLinearRefToTensorParams("input", node.getMemoryUnit("input"), inpTensorParPrefix);
        if (!node.isSnk())
            _addLinearRefToTensorParams("output", node.getMemoryUnit("output"), outpTensorParPrefix);
        else
            _addLinearRefToTensorParams("output", null, outpTensorParPrefix);

        _additionalTensorParams(node);
    }


    /**
     * Fill in CSDF operator tensor parameters
     * @param operator CSDF operator tensor parameters
     */
    protected void _fillInTensorParams(Operator operator, Integer suboperatorId) {
        if(operator == null)
            return;

        if(operator instanceof ComplexOperator) {
            if(!((ComplexOperator) operator).isCompound()) {
                _fillComplexOperatorTensorParams((ComplexOperator) operator);
                return;
            }
    }

        String parPrefix = "";
        if(suboperatorId!=null)
        parPrefix = "_" + suboperatorId.toString();

        TreeMap<String,Tensor> tensorParameters = operator.getTensorParams();

        _printStream.println("");
        _printStream.println(_prefix + "// tensor parameters "+ suboperatorId);

        for(Map.Entry<String,Tensor> entry: tensorParameters.entrySet())
             _addLinearRefToTensorParam(entry.getValue(),entry.getKey(),parPrefix);
    }

        /**
     * Define operator parameters for a complex operator
     * @param operator complex CSDF node operator
     */
    protected void _fillComplexOperatorTensorParams(ComplexOperator operator){
        Integer opId = 0;
        Vector<Operator> subOperators = operator.getSubOperators();
        for(Operator subOp: subOperators){
           _fillInTensorParams(subOp,opId);
            opId++;
        }

        _printStream.println();
        _printStream.println(_prefix + "// internal buffers");
        Integer bufId = 0;
        Integer srcOpId;
        Integer dstOpId;
        for(InternalBuffer internalBuffer: operator.getInternalBuffers()){
            srcOpId = subOperators.indexOf(internalBuffer.getSrc());
            dstOpId = subOperators.indexOf(internalBuffer.getDst());

            if(srcOpId!=null && dstOpId!=null) {
                _setupInternalBuffer( bufId,srcOpId, dstOpId);
            }
            else System.err.println(operator.getName() + " internal buffer setup error!");

            bufId++;
        }
    }

    /**
     * Define internal buffer between two connections
     * @param bufId buffer Id
     * @param srcOpId source operator Id
     * @param dstOpId dst operator Id
     */
    protected void _setupInternalBuffer (Integer bufId, Integer srcOpId, Integer dstOpId){
       String name = "internal_buf" + bufId.toString();
       _printStream.println(_prefix + "tensor_params_"+ srcOpId +"[\"output\"] = &" + name + "[0];");
       _printStream.println(_prefix + "tensor_params_" + dstOpId +"[\"input\"] = &" + name + "[0];");
    }


    /**
     * Add linear reference to tensor parameters
     * @param tensor tensor
     * @param  name tensor param name
     */
    private void _addLinearRefToTensorParam(Tensor tensor, String name, String parPrefix){
        String ref = "&"+ name + parPrefix + "[0]";
        if(Tensor.isNullOrEmpty(tensor))
            ref = "nullptr";

        _printStream.println(_prefix + "tensor_params" + parPrefix + "[\""+ name +"\"] = " + ref + ";");

    }

    /**
     * Add special parameters
     * @param node CSDF node
     */
    private void _additionalTensorParams(CSDFNode node){
        if(node.getOperator().hasTensorRefs()){
          for (Map.Entry<String,String> tensorRef : node.getOperator().getTensorRefs().entrySet()){
              _addTensorRef(tensorRef);
          }
        }
    }

    /**
     * Add non-trivial tensor reference to tensor params
     * @param tensorRef
     */
    private void _addTensorRef(Map.Entry<String,String> tensorRef){
       String muval = tensorRef.getValue();
       if(muval.equals("null"))
           _printStream.println(_prefix + "tensor_params[\""+ tensorRef.getKey() +"\"] = nullptr;");
       else
       _printStream.println(_prefix + "tensor_params[\""+ tensorRef.getKey() +"\"] = &"+ muval + "[0]"+ ";");
    }

     /**
     * Add linear reference to tensor parameters
     * @param mu Memory unit
     */
    private void _addLinearRefToTensorParams(String muname, MemoryUnit mu, String tensorParamsPrefix){
        String muref = "&"+ muname + "[0]";

        if(mu==null)
            muref = "nullptr";

        _printStream.println(_prefix + "tensor_params" + tensorParamsPrefix + "[\""+ muname +"\"] = " + muref + ";");

    }

    /**
     * Init array with dummy values (data mocs for debug purposes)
     * @param dims array dimensions
     * @param itName iterator name
     * @param arrName array name
     * @param dummyVal dummy value
     */
    private void _initArrDummyLinear(int dims, String itName, String arrName, Integer dummyVal){

        _printStream.println("");
        _printStream.println("// fill " + arrName + " with dummy values ");
        String arrDesc = arrName + "[" + itName + "]";

        String dimsTotal = arrName +"_dim_0";
        for(int i=1; i<dims; i++)
            dimsTotal+="* " + arrName + "_dim_"+ i;

        _printStream.println(_prefix + "for (int "+ itName + "= 0;" + itName +"<"+ dimsTotal + ";" + itName +"++)");
            prefixInc();

         _printStream.println(_prefix + arrDesc + " = " + dummyVal.toString() + "; ");

        prefixDec();
    }


     /**
     * Write FIFO sizes for CSDF node
     * @param node CSDF node
      * TODO check why min FIFO sizes (without repetitions) are not always applicable
     */
    protected void _writeFIFOsizes(CSDFNode node){
        _printStream.println(_prefix + "//assign FIFO sizes");
        for(CSDFPort inport: node.getNonOverlapHandlingInPorts())
            _writeFIFOsize(inport,node.getRepetitions());
        for(CSDFPort outport: node.getNonOverlapHandlingOutPorts())
            _writeFIFOsize(outport,node.getRepetitions());
        _printStream.println("");
    }


     /**
      * TODO fix min FIFO sizes
     * Write FIFO sizes for CSDF node
     * @param port CSDF port
     */
     protected void _writeFIFOsize(CSDFPort port, int repetitions){
      MemoryUnit mu = port.getAssignedMemory();
      if(mu==null) {
          _printStream.println(_prefix + port.getName() + "_fifo_size = 0;");
          return;
      }
      if(mu.getDimensionality()<1) {
          _printStream.println(_prefix + port.getName() + "_fifo_size = 0;");
          return;
      }
      _printStream.println(_prefix + port.getName() + "_fifo_size = "
           //   + (mu.getShape().getElementsNumber() + 10) + ";");
              + mu.getShape().getElementsNumber() * repetitions + ";"); //max size
           //     + mu.getShape().getElementsNumber()+";"); //min size
    }

     /**
     * Write FIFO sizes for CSDF node
     * @param port CSDF port
     */
     protected void _writeFIFOsize(CSDFPort port){
      MemoryUnit mu = port.getAssignedMemory();
      if(mu==null) {
          _printStream.println(_prefix + port.getName() + "_fifo_size = 0;");
          return;
      }
      if(mu.getDimensionality()<1) {
          _printStream.println(_prefix + port.getName() + "_fifo_size = 0;");
          return;
      }
      _printStream.println(_prefix + port.getName() + "_fifo_size = "
              + mu.getShape().getElementsNumber() + ";");
    }





   /////////////////////////////////////////////////////////////////////
   ///////// R/W operations /////////////////////////////////

    /**
     * process node input ports
     * @param node SDF Node
     *
     */
    @Override
    protected void _processReading(CSDFNode node){
       _printStream.println("");
       _printStream.println(_prefix + "//reading");

       /** reading into concat node */
       if(node.getFunction().toLowerCase().equals("concat")){
           _processReadingConcat(node, "output");
           return;
       }

       /** reading into built-in concat node */
       if(node.getOperator().isConcat()){
           _processReadingConcat(node, "input");
           return;
       }

       if(node.isSrc()){
           _processReadingSrc();
           return;
       }

       /** Todo rem after testing*/
       if(node.isSnk()){
           _printStream.println(_prefix + "//start timer");
           _printStream.println(_prefix + "start = std::clock();");
       }

        for (CSDFPort inport: node.getOverlapHandlingInPorts()){
          //  _definePhasesLimitation(inport, false);
            printReadTemplate(inport);
        }

        for (CSDFPort inport: node.getNonOverlapHandlingInPorts()){
            _definePhasesLimitation(inport, false);
            printReadTemplate(inport);
        }
    }

    /**
     * TODO now it is assumed that concat is always performed over the dim_0 (depth) dimension
     * Process concatencation node reading
     * @param node CSDF concatenation node
     */
    protected void _processReadingConcat(CSDFNode node, String commonInpName){
        Vector<CSDFPort> inputs = _getConcatSortedInputPorts(node);
        _printStream.println("");
        _printStream.println(_prefix + "int ptr_start = 0;");

        for(CSDFPort p: inputs)
          _processReadingConcatPort(p,commonInpName);
    }

    /**
     * Process source (data) node reading
     */
    protected void _processReadingSrc(){
        _printStream.println("");
        if(_mocMode){
            _printStream.println(_prefix + "for(int i=0; i<output_len; i++)");
            _prefixInc();
             _printStream.println(_prefix + "output[i] = (float)i;");
            _prefixDec();

        }
        else
        _printStream.println(_prefix + "dl.data_load_from_numpy(data_path, data_id, output_len, 0, 0, &output[0]);");

        _printStream.println(_prefix + "data_id++;");
        _printStream.println(_prefix + "if (data_id > max_data_id)");
        _prefixInc();
        _printStream.println(_prefix + "data_id = 0;");
        _prefixDec();
    }


    /**
     * print reading template
     * @param port SDF input port
     */
     @Override
      public void printReadTemplate(CSDFPort port) {
        String arrayName = port.getAssignedMemoryName();
         if(port.isOverlapHandler()){
              _printInternalArrayShift(port,arrayName);
              return;
         }

         if(port.getStartTokens()==null)
             printOperationTemplate(port, "read" + _externalRWPostfix, arrayName);
         else
             printOperationShiftedTemplate(port,"read" + _externalRWPostfix,arrayName,
                     port.getName() + "_shift");
    }

    /**
    * process node output ports
    * @param node SDF Node
    * */
    @Override
    protected void _processWriting(CSDFNode node){
       _printStream.println("");
       _printStream.println(_prefix + "//writing");
       if(node.isSnk()) {
           _processSnkNodeWriting();
           return;
       }

        for (CSDFPort outport: node.getOutPorts()){
            if(!outport.isOverlapHandler())
                _definePhasesLimitation(outport, false);
            printWriteTemplate(outport);
        }
    }

    /**
     * Process writing for sink node
     */
    protected void _processSnkNodeWriting(){
        _printStream.println(_prefix + "//print application output data");
        if(_mocMode){}
               //_printStream.println(_prefix + "std::cout<<\"application output: \"<<rep<<std::endl;");
        else {
            _printStream.println(_prefix + "std::cout<<\"application output: \";");
            _printStream.println(_prefix + "for(int i=0;i<input_len;i++)");
            prefixInc();
            _printStream.println(_prefix + "  cout<<input[i]<<' ';");
            prefixDec();
            _printStream.println(_prefix + "std::cout<<std::endl;");
        }
    }

    /**
     * Print write template
     * @param port CSDF port performs writing
     */
    public void printWriteTemplate(CSDFPort port) {
        String arrayName = port.getAssignedMemoryName();
        if(!port.isOverlapHandler())
            printOperationTemplate(port, "write" + _externalRWPostfix, arrayName);
    }

     /**
     * print reading/writing template for port,
     * taking into account only end border limitations
     * R/W primitive parameters:
     * 1. fifo: reference on destination array (name of CSDF node input port array) or FIFO
     * 2. memobj_cpu : reference on source array (default value, stored in header)
     * 3. len - number of tokens to be transferred
     * 4. fifo_size - total fifo size (const value, stored in header)
      *
      * write fifo --> local mem
      * read: local mem --> fifo
      * */
     @Override
    public void printOperationTemplate(CSDFPort port,String operation, String arrayName){
       if(port==null || arrayName==null)
           return;

       String portName = port.getName();
       String fifoRef = port.getNode().getName() + "_" + portName + "_buf_ptr";
        _printStream.println(" ");
        _printStream.println(_prefix + "// " + operation + " to " + arrayName);
         /** check, if there are any tokens to r/w*/
        _printStream.println(_prefix + "if ( " + portName + "_tokens > 0 )" );
        prefixInc();
        _printStream.println(_prefix + operation + "(" + fifoRef +"->fifo, " +
              _getLinearArrRef(port.getAssignedMemory()) + ", "+
                portName + "_tokens, " + fifoRef +"->fifo_size);");
        _prefixDec();
    }

    /**
     * Specific reading template for Concatenation node CSDF port
     * @param port concatenation node CSDF port
     */
    protected void _processReadingConcatPort(CSDFPort port, String commonInpName){
        if(port==null)
           return;
        MemoryUnit mu = port.getAssignedMemory();
        String portName = port.getName();
        String fifoRef = port.getNode().getName() + "_" + portName + "_buf_ptr";
        _printStream.println(" ");
        _printStream.println(_prefix + "// read from " + portName);
        _printStream.println(_prefix + "int " + portName + "_tokens = " + mu.getName() + "_len;" );
         /** check, if there are any tokens to r/w*/
        _printStream.println(_prefix + "if ( " + portName + "_tokens > 0 )" );
        prefixInc();
        _printStream.println(_prefix + "read" + _externalRWPostfix + "(" + fifoRef +"->fifo, " +
              "&" + commonInpName + "[ptr_start], " + portName + "_tokens, " + fifoRef +"->fifo_size);");
        _prefixDec();
        _printStream.println(_prefix + "ptr_start += "+ portName + "_tokens;");

        //printOperationTemplate(port, "read" + _externalRWPostfix, arrayName);
        //readSWF_CPU(Pooling66_output_concat_0_IP0_buf_ptr->fifo, &output[ptr_start], IP0_tokens, Pooling66_output_concat_0_IP0_buf_ptr->fifo_size);

    }

     /**
     * print reading/writing template for port,
     * taking into account only end border limitations
     */
     @Override
    public void printOperationShiftedTemplate(CSDFPort port,String operation, String arrayName, String shiftDesc){
       if(port==null || arrayName==null)
           return;

       String portName = port.getName();
       String fifoRef = port.getNode().getName() + "_" + portName + "_buf_ptr";
       String portDataType = getDataType(port);
       port.getMemoryDim();

        _printStream.println(" ");
        _printStream.println(_prefix + "// " + operation + " to " + arrayName);
        /** check, if there are any tokens to r/w*/
        _printStream.println(_prefix + "if ( " + portName + "_tokens > 0 )" );
        prefixInc();
        _printStream.println(_prefix + operation + "(" + fifoRef +"->fifo, "+
                _getLinearArrRef(port.getAssignedMemory()) + " + " + portName + "_shift*sizeof("+
                portDataType + "), " + portName + "_tokens, " + fifoRef +"->fifo_size);");
        prefixDec();
    }

   /**
     * Get data type of CSDF port. If there is a memory unit, assigned to
     * a memory port, the memory unit data type is used. Otherwise,
     * int (integer) data type is used
     * @param port CSDF port
     * @return data type of CSDF port
     */
    private String getDataType(CSDFPort port){
        String dataType;
        MemoryUnit mu = port.getAssignedMemory();
        if(mu==null)
            dataType = "int";
        else
            dataType = mu.getTypeDesc();
        return dataType;
    }

    /**
     * print internal data shift for overlapping port memory,
     * taking into account only end border limitations
     *
     * C++ shift functions:
     * static void shift_2D(int h, int w, int *x, int stride);
     * appMain::shift_2D(h,w,&arr_to_shift_3D[0][0],stride);
     * static void shift_3D(int d, int h, int w, int *x, int stride);
     * appMain::shift_3D(d,h,w,&arr_to_shift_3D[0][0][0],stride);
     *
     */

    public void _printInternalArrayShift(CSDFPort port,String arrayName){
       if(port==null || arrayName==null)
           return;
        int dataDimensionality = port.getMemoryDim();
        if(dataDimensionality<2 || dataDimensionality>3)
            return;

        _printStream.println(" ");
        _printStream.println(_prefix + "// internal shift of " + arrayName);
        //shift 2D
        if(dataDimensionality==2)
                   _printStream.print(_prefix + _funcClassName + "::shift_2D(" + arrayName + "_dim_0," +
                arrayName + "_dim_1," + "&" + arrayName +"[0][0], stride);");
        //shift 3D
        else
            _printStream.print(_prefix + _funcClassName + "::shift_3D("+ arrayName + "_dim_2," +
                arrayName + "_dim_1," + arrayName + "_dim_0," + "&" + arrayName +"[0], stride);");

        _printStream.println("");
    }


    /************************************************************/
    /*******        appFunc class functions              *******/

    /** Execution primitives
     * @param incDnnFuncCPU if internal DNN operators lib is used for CPU
     * @param incDnnFuncGPU if internal DNN operators lib is used for GPU
     */
    protected void _writeExecPrimitives(boolean incDnnFuncCPU, boolean incDnnFuncGPU){
        _printStream.println(_prefix + "// Execution function primitive");
        prefixInc();
        _writeExecPrimitiveSimplest(incDnnFuncCPU,incDnnFuncGPU);
        if(_generateDNNFuncNA) {
            _writeExecPrimitiveNA();
             prefixDec();
             return;
        }
            _writeExecPrimitiveDNCPU();
            prefixDec();
            return;

    }

    /** Standard appFunc functions*/
    protected void _writeFunctions(){
        _printStream.println("");
        _writeTransposeFunction();
        _writeCommunicationMocs();
        prefixInc();
        _writeCPULine();
        _writeShiftFunctions();
        _writePrintFunctions();
        _writeGetBufFuncs();
        prefixDec();
    }

    /**
     * Communication mocs replace the execution with dummy function,
     * that transmit data from node input to node output
     * and allow to check if the communication between the nodes works
     */
    protected void _writeCommunicationMocs(){
     _printStream.println("");
     _printStream.println(_prefix + "/**");
     _printStream.println(_prefix + "* execution moc (for communication checkout) ");
     _printStream.println(_prefix + "* set output to (first_element_of_input + 1)");
     _printStream.println(_prefix + "* input -input data ptr");
     _printStream.println(_prefix + "* output - output data ptr");
     _printStream.println(_prefix + "* outp_len - output data length");
     _printStream.println(_prefix + "*/");
     _writeCommunicationMoc();
    }

    /** Print communication moc function*/
    protected void _writeCommunicationMoc(){
     prefixInc();
     _printStream.println(_prefix + "void appFunc::communication_moc("+ _IODatatype + "* input, "+ _IODatatype + "* output, int inp_len, int outp_len){");
     _printStream.println(_prefix+"int to_fill = std::min(inp_len, outp_len);");
     _printStream.println(_prefix + "for(int i=0;i<to_fill;i++)");
     prefixInc();
     _printStream.println(_prefix + "output[i] = input[i]+1;");
     prefixDec();
     prefixDec();
     _printStream.println(_prefix + "}");
    }

    /**
     * Print functions of getting buffer from vector of buffers
     */
    protected void _writeGetBufFuncs(){
        _writeGetBufFunc("src");
        _writeGetBufFunc("dst");
    }

     /**
     * Print functions of getting buffer from vector of buffers
     * @param bufPrefix source or destination buffer
     */
    protected void _writeGetBufFunc(String bufPrefix){
     _printStream.println("");
     _printStream.println(_prefix+"// get fifo buffer by " + bufPrefix);
     _printStream.println(_prefix + "fifo_buf* "+ _funcClassName +"::get_buf_by_" + bufPrefix +
     " (std::string name, std::vector<fifo_buf>& fifos){");
     prefixInc();
     _printStream.println(_prefix + "for (auto & fifos_elem: fifos)  {");
     prefixInc();
     _printStream.println(_prefix + "if (name.compare(fifos_elem." + bufPrefix + ") == 0)");
     prefixInc();
     _printStream.println(_prefix + "return &fifos_elem;");
     prefixDec();
     _printStream.println(_prefix + "}");
     _printStream.println(_prefix + "return nullptr;");
     prefixDec();
     prefixDec();
     _printStream.println(_prefix + "}");

    }


    /**
     * Write line copy function
     */
    protected void _writeCPULine(){
         _printStream.println(_prefix + "/**");
         prefixInc();
          _printStream.println(_prefix + "* copies 2D-data line from src to dst.");
          _printStream.println(_prefix + "* data_h - src data height");
          _printStream.println(_prefix + "* data_w - src data width");
          _printStream.println(_prefix + "* src - pointer to first data source array element");
          _printStream.println(_prefix + "* dst - pointer to first copy destination array element");
         prefixDec();
         _printStream.println(_prefix + "*/");

         _printStream.println(_prefix + " void " + _funcClassName + "::cpy_2D_data_line(const int &data_w, "+
                 _IODatatype + " *src,"+ _IODatatype + " *dst,"+" const int &line_id)");
         prefixInc();
         _printStream.println(_prefix + "{");
         prefixInc();
         _printStream.println(_prefix + "int line_start = line_id * data_w;");
         _printStream.println(_prefix + "for (int i = 0; i < data_w ; i++)");
         prefixInc();
         _printStream.println(_prefix + "dst[i] = src[line_start];");
         prefixDec();
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
    }


      /**
     * Write shift function (for shifting overlapping data in I/O arrays)"
     * TODO refactor shift function : make one for general arrays - tensors
     */
    protected void _writeTransposeFunction(){
        _printStream.println(_prefix + "/**");
        _printStream.println(_prefix + "Transpose matrix");
        _printStream.println(_prefix + "@param input : matrix to transpose");
        _printStream.println(_prefix + "@param inp_h : input matrix height");
        _printStream.println(_prefix + "@param inp_w : input matrix width");
        _printStream.println(_prefix + "*/");

         _printStream.println(_prefix + " void "+_funcClassName + "::transpose(" + _IODatatype + " *input, int inp_h, int inp_w) {");
         prefixInc();
        _printStream.println(_prefix + _IODatatype + " tmp[inp_w][inp_h] = {0};");
        _printStream.println(_prefix + "");
        _printStream.println(_prefix + "for(int j=0; j<inp_h; j++){");
        _prefixInc();
        _printStream.println(_prefix + "for(int i=0; i<inp_w; i++){");
        _prefixInc();
        _printStream.println(_prefix + "tmp[i][j] = *(input+i+j*inp_w);");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(_prefix + "");

        _printStream.println(_prefix + "for(int j=0; j<inp_h; j++){");
        _prefixInc();
        _printStream.println(_prefix + "for(int i=0; i<inp_w; i++){");
        _prefixInc();
        _printStream.println(_prefix + "*(input+j+i*inp_h) = tmp[i][j];");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(_prefix + "");
         prefixDec();
         _printStream.println(_prefix + "}");
         _printStream.println(_prefix + "");
    }


    /**
     * Write shift function (for shifting overlapping data in I/O arrays)"
     * TODO refactor shift function : make one for general arrays - tensors
     */
    protected void _writeShiftFunctions(){
        _printStream.println(_prefix + "/**");
        _printStream.println(_prefix + "Data shift functions (for shifting overlapping data in I/O arrays)");
        _printStream.println(_prefix + "@param array : I/O overlapping array");
        _printStream.println(_prefix + "@param dim   : I/O overlapping array dimensionality");
        _printStream.println(_prefix + "*/");

        _writeShift2D();
        _writeShift3D();
    }

    /**
     * Write 2D shift
     */
    protected void _writeShift2D(){
        _printStream.println(_prefix + "/**");
         prefixInc();
          _printStream.println(_prefix + "* Moves 2D data on n lines to top.");
          _printStream.println(_prefix + "* Required for overlapping data.");
          _printStream.println(_prefix + "* h - array height");
          _printStream.println(_prefix + "* w - array width");
          _printStream.println(_prefix + "* x - pointer to first array element");
       //   _printStream.println(_prefix + "* TODO: remove cout after testing");
         prefixDec();
         _printStream.println(_prefix + "*/");

         _printStream.println(_prefix + " void " + _funcClassName + "::shift_2D (const int &h, const int &w, "+
                 _IODatatype + " *x, const int &stride)");
         prefixInc();
         _printStream.println(_prefix + "{");
         prefixInc();
      //   _printStream.println(_prefix + "cout<<\"2D data shift\"<<endl;");
         _printStream.println(_prefix + "for(int line_ind = stride; line_ind < w ; line_ind ++){");
         prefixInc();
         _printStream.println(_prefix + "for(int i=0; i<w; i++)");
         prefixInc();
         _printStream.println(_prefix + "x[i + (line_ind - stride)* w] = x[i + line_ind * w];");
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
    }

    /**
     * Write 3D shift
     */
    protected void _writeShift3D(){
         _printStream.println(_prefix + "/**");
         prefixInc();
          _printStream.println(_prefix + "* Moves 3D data on n lines to top.");
          _printStream.println(_prefix + "* Required for overlapping data.");
          _printStream.println(_prefix + "* d - array depth");
          _printStream.println(_prefix + "* h - array height");
          _printStream.println(_prefix + "* w - array width");
          _printStream.println(_prefix + "* x - pointer to first array element");
      //    _printStream.println(_prefix + "* TODO: remove cout after testing");
         prefixDec();
         _printStream.println(_prefix + "*/");

         _printStream.println(_prefix + " void " + _funcClassName + "::shift_3D (const int &d, const int &h, const int &w, "+
                 _IODatatype + " *x, const int &stride)");
         prefixInc();
         _printStream.println(_prefix + "{");
         prefixInc();
       //  _printStream.println(_prefix + "cout<<\"3D data shift\"<<endl;");

         _printStream.println(_prefix + "int start_elem_id = 0;");
         _printStream.println(_prefix + "for(int depth=0; depth < d; depth++ ){");
         prefixInc();
         _printStream.println(_prefix + "for(int line_ind = stride; line_ind < w ; line_ind ++){");
         prefixInc();
         _printStream.println(_prefix + "for(int i=0; i<w; i++)");
         prefixInc();
         _printStream.println(_prefix + "x[i + (line_ind - stride)* w + start_elem_id] = x[i + line_ind * w + start_elem_id];");
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
         _printStream.println(_prefix + "start_elem_id +=w*h;");
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
    }

    /** Write printout functions*/
    protected void _writePrintFunctions(){
        _writeShowVal();
        _writeprint2D();
        _writeprint3D();
    }

    protected void _writeShowVal(){
        _printStream.println(_prefix + "// function to show first num values of array");
        _printStream.println(_prefix + " void " + _funcClassName + "::show_val("+ _IODatatype + " *x, int xlen, int num){");
        prefixInc();
        _printStream.println(_prefix + "for (int i = 0; i < std::min(xlen,num); i++)");
        prefixInc();
        _printStream.println(_prefix + "std::cout << x[i] << ' ';");
        prefixDec();
        _printStream.println(_prefix + "std::cout << std::endl;");
        prefixDec();
        _printStream.println(_prefix + "}");

    }

    protected void _writeprint2D(){
        _printStream.println(_prefix + "//2D array print function, type: " + _IODatatype);
        _printStream.println(_prefix + " void " + _funcClassName + "::print_2D(const int &h, const int &w, "+ _IODatatype + " *x)");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println(_prefix + "for (int i = 0; i < h; i++){");
        prefixInc();
        _printStream.println(_prefix + "for (int j = 0; j < w ; j++)");
        prefixInc();
        _printStream.println(_prefix + "std::cout << x[i * w + j] << ' ';");
        prefixDec();
        _printStream.println(_prefix + "std::cout<<endl;");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }

    protected void _writeprint3D(){
        _printStream.println(_prefix + "//3D array print function, type: " + _IODatatype);
        _printStream.println(_prefix + " void " + _funcClassName + "::print_3D(const int &d, const int &h, const int &w, "+ _IODatatype + " *x)");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println(_prefix + "int start_elem_id = 0;");
        _printStream.println(_prefix + "for(int depth=0; depth < d; depth++ ){");
        prefixInc();
        _printStream.println(_prefix + "std::cout<<\"depth\"<<depth<<endl;");
        _printStream.println(_prefix + "for (int i = 0; i < h; i++){");
        prefixInc();
        _printStream.println(_prefix + "for (int j = 0; j < w ; j++)");
        prefixInc();
        _printStream.println(_prefix + "std::cout << x[i * w + j + start_elem_id] << ' ';");
        prefixDec();
        _printStream.println(_prefix + "std::cout<<endl;");
        prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(_prefix + "start_elem_id +=w*h;");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }

    /**
     * Write execution function primitive simplest MoC
     * (only function name is a parameter)
     */
    protected void _writeExecPrimitiveSimplest(boolean incDnnFuncCPU, boolean incDnnFuncGPU){
        _printStream.println("");
        _printStream.println(_prefix + "/** simplest exec MoC ");
        _printStream.println(_prefix + "(only function name is a parameter)*/ ");
        _printStream.println(_prefix + " void " + _funcClassName +
                    "::execute (std::string function)");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println(_prefix + "// cout<<function<<endl;");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }


    /************************************************************/
    /*******    DNN-library dependent API functions      *******/

    /**
     * Write execution function primitive MoC
     * (with a number of parameters, that can be used by DNN operators)
     * TODO extend or replace by DNN library
     */
    protected void _writeExecPrimitiveDNCPU(){
        _printStream.println("");
        _printStream.println(_prefix + "/** TODO: place an API to your CNN operators library here*/");
        _printStream.println(_prefix + " void " + _funcClassName +
                    "::execute (std::string function" +
                ",std::map<std::string," + _IODatatype + "*>* tensor_params_ptr"+
                ", std::map<std::string,int>* int_params_ptr )");

        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();

        _printStream.println(_prefix + "bool op_done = false;");

        //SOFTMAX
        _printStream.println(_prefix + "if (function.find(\"SOFTMAX\") != std::string::npos) {");
        prefixInc();
        _printStream.println(_prefix + "dnnFunc::softmax(int_params_ptr,tensor_params_ptr);");
        _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //THN, SIGM, ReLU (Any nonlinear)
        _printStream.println(_prefix + "if ((function.find(\"ReLU\") != std::string::npos)" +
                " || (function.find(\"THN\") != std::string::npos) || (function.find(\"SIGM\") != std::string::npos)" +
                " || (function.find(\"LeakyReLu\") != std::string::npos) ){");
        prefixInc();
        _printStream.println(_prefix + "dnnFunc::activation(function, int_params_ptr, tensor_params_ptr);");
        _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //CONV
        _printStream.println(_prefix + "if (function.find(\"CONV\") != std::string::npos) {");
        _prefixInc();
        _printStream.println(_prefix + "dnnFunc::conv(int_params_ptr,tensor_params_ptr);");
        _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //MAXPOOL
        _printStream.println(_prefix + "if (function.find(\"MAXPOOL\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "dnnFunc::maxpool(int_params_ptr,tensor_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //AVGPOOL
        _printStream.println(_prefix + "if (function.find(\"AVGPOOL\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "dnnFunc::avgpool(int_params_ptr,tensor_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //DENSEBLOCK
        _printStream.println(_prefix + "if (function.find(\"DENSEBLOCK\") != std::string::npos || function.find(\"MATMUL\") != std::string::npos ||function.find(\"GEMM\") != std::string::npos) {");
        prefixInc();
        _printStream.println(_prefix + "dnnFunc::gemm(int_params_ptr,tensor_params_ptr); ");
        _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");
        //prefixDec();

        //LRN
        _printStream.println(_prefix + "if (function.find(\"LRN\") != std::string::npos) {");
        prefixInc();
        _printStream.println(_prefix + " dnnFunc::lrn(int_params_ptr,tensor_params_ptr); ");
        _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //BN
        _printStream.println(_prefix + "if (function.find(\"BN\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "dnnFunc::batch_normalization(int_params_ptr,tensor_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");
        //prefixDec();

         //SUBConst
        _printStream.println(_prefix + "if (function.find(\"SUBConst\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "dnnFunc::sub_const(int_params_ptr,tensor_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //DIVConst
        _printStream.println(_prefix + "if (function.find(\"DIVConst\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "dnnFunc::div_const(int_params_ptr,tensor_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //MULconst
        _printStream.println(_prefix + "if (function.find(\"MULconst\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "dnnFunc::mul_const(int_params_ptr,tensor_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        _printStream.println("");
        _printStream.println("");
        _printStream.println(_prefix + "if(!op_done){");
        _prefixInc();
       /// _printStream.println(_prefix + "communication_moc(input, output,input_len, output_len);");
        //_printStream.print(_prefix);
        // if(_silent)
          // _printStream.print("//");
        _printStream.println(_prefix + "std::cout<<function<<\" operation not found!\"<<std::endl;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();


    }


     /**
     * Write execution function primitive MoC
     * (with a number of parameters, that can be used by DNN operators)
     * TODO extend or replace by DNN library
     */
    protected void _writeExecPrimitiveNA(){
        _printStream.println("");
        _printStream.println(_prefix + "/** TODO: place an API to your CNN operators library here*/");
        _printStream.println(_prefix + " void " + _funcClassName +
                    "::execute (std::string function," +
                _IODatatype +"* input, " + _paramDataType + "* weights, "
                + _IODatatype + "* output, "+ _paramDataType + "* bias, std::map<std::string,int>* int_params_ptr )");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println("");
        _printStream.println(_prefix + "bool op_done = false;");
        _printStream.println(_prefix + "int input_len = int_params_ptr->at(\"input_len\");");
        _printStream.println(_prefix + "int output_len = int_params_ptr->at(\"output_len\");");
        _printStream.println(_prefix + "int core_id = int_params_ptr->at(\"core_id\");" );
        _printStream.println("");

        //SOFTMAX
        _printStream.println(_prefix + "if (function.find(\"SOFTMAX\") != std::string::npos) {");
        prefixInc();
        _printStream.println(_prefix + "softmax(app_output, output_len, app_output);");
        _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //THN
         _printStream.println(_prefix + "if (function.find(\"THN\") != std::string::npos) {; }");

        //SIGM
        _printStream.println(_prefix + "if (function.find(\"SIGM\") != std::string::npos) {;} ");
        prefixInc();

        //ReLU
        _printStream.println(_prefix + "if (function.find(\"ReLU\") != std::string::npos) {");
        prefixInc();
        _printStream.println(_prefix + " relu(input, output, int_params_ptr);");
        _printStream.println(_prefix + " op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //CONV
        _printStream.println(_prefix + "if (function.find(\"CONV\") != std::string::npos) {");
        _prefixInc();
            _printStream.println(_prefix + "execute_conv(input, output, weights, bias, int_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //MAXPOOL
        _printStream.println(_prefix + "if (function.find(\"MAXPOOL\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "maxpool(input, output, int_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //AVGPOOL
        _printStream.println(_prefix + "if (function.find(\"AVGPOOL\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "//dnnFunc::avgpool_layer(input, output, int_params_ptr);");
            _printStream.println(_prefix + "//op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //LRN
        _printStream.println(_prefix + "if (function.find(\"AVGPOOL\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "//execut local response normalization operator ;");
            _printStream.println(_prefix + "//op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //DENSEBLOCK
        _printStream.println(_prefix + "if (function.find(\"DENSEBLOCK\") != std::string::npos || function.find(\"MATMUL\") != std::string::npos ||function.find(\"GEMM\") != std::string::npos) {");
        prefixInc();
        _printStream.println(_prefix + "execute_dense_block(function, input, output, weights, bias, int_params_ptr); ");
        _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println("");
        _printStream.println(_prefix + "/** TODO: the communication_moc is used for communications checkout.");
        _printStream.println(_prefix + "*   TODO: remove it when the real CNN operators library is used */");
        _printStream.println("");
        _printStream.println(_prefix + "if(!op_done){");
        _prefixInc();
        _printStream.println(_prefix + "communication_moc(input, output,input_len, output_len);");
        _printStream.print(_prefix);
         if(_silent)
           _printStream.print("//");
        _printStream.println("std::cout<<function<<\" op moc used\"<<std::endl;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }



    /**
     * Write execution function primitive MoC
     * (with a number of parameters, that can be used by DNN operators)
     * TODO extend or replace by DNN library
     */
    protected void _writeExecPrimitive(boolean incDnnFuncCPU, boolean incDnnFuncGPU){
        _printStream.println("");
        _printStream.println(_prefix + "/** TODO: place an API to your CNN operators library here*/");
        _printStream.println(_prefix + " void " + _funcClassName +
                    "::execute (std::string function," +
                _IODatatype +"* input, " + _paramDataType + "* weights, "
                + _IODatatype + "* output, "+ _paramDataType + "* bias, std::map<std::string,int>* int_params_ptr )");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println("");
        _printStream.println(_prefix + "bool op_done = false;");
        _printStream.println(_prefix + "int input_len = int_params_ptr->at(\"input_len\");");
        _printStream.println(_prefix + "int output_len = int_params_ptr->at(\"output_len\");");
        _printStream.println(_prefix + "int core_id = int_params_ptr->at(\"core_id\");" );
        _printStream.println("");

        //SOFTMAX
        _printStream.println(_prefix + "if (function.find(\"SOFTMAX\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::softmax(input,input_len, output);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //THN
         _printStream.println(_prefix + "if (function.find(\"THN\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_thn(input, output, input_len);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //SIGM
        _printStream.println(_prefix + "if (function.find(\"SIGM\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_sigm(input, output, input_len);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //ReLU

        _printStream.println(_prefix + "if (function.find(\"ReLU\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_relu(input, output ,input_len);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //CONV
        _printStream.println(_prefix + "if (function.find(\"CONV\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_conv(input, weights, output, bias, int_params_ptr,core_id);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //POOL
        _printStream.println(_prefix + "if (function.find(\"POOL\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_pool(function,input, output, bias, int_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        _printStream.println(_prefix + "if (function.find(\"DENSEBLOCK\") != std::string::npos || function.find(\"MATMUL\") != std::string::npos ||function.find(\"GEMM\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU){
            _printStream.println(_prefix + "dnnFunc::execute_dense_block(function, input, weights, output, bias, int_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.print(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println("");

        _printStream.println(_prefix + "/** TODO: the communication_moc is used for communications checkout.");
        _printStream.println(_prefix + "*   TODO: remove it when the real CNN operators library is used */");
        _printStream.println("");
        _printStream.println(_prefix + "if(!op_done){");
        _prefixInc();
        _printStream.println(_prefix + "communication_moc(input, output,int_params_ptr->at(\"input_len\"), int_params_ptr->at(\"output_len\"));");
        _printStream.print(_prefix);
         if(_silent)
           _printStream.print("//");
        _printStream.println("std::cout<<function<<\" op moc used\"<<std::endl;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }

     /***************************************************************/
     /**                            Mapping                        */

    /**
     * Find core id for the CSDF node
     * @param nodeName name of the CSDF node
     * @return core id for the CSDF node
     */
    protected Integer _assignCore(String nodeName){
        Integer coreId = null;

        if(_mapping!=null)
            coreId = _findCoreInMapping(nodeName);
        if(coreId!=null)
            return coreId;
        else
            return _assignFreeCore();
    }

    /**
     * TODO: now it is assumed that every core is launched by CPU
     * Find core id in the provided mapping
     * @return core id, found in the provided mapping
     */
    private Integer _findCoreInMapping(String nodeName){
        Vector processorList = _mapping.getProcessorList();
        Vector cpuList = new Vector();
        for(Object proc: processorList){
            if(!(((MProcessor)proc).getResource() instanceof GPU))
                cpuList.add(proc);
        }

        Integer coreId = 0;

        for(Object mp: _mapping.getProcessorList()) {
            Vector processes = ((MProcessor)mp).getProcessList();
            Iterator i;

            i = processes.iterator();
            while (i.hasNext()) {
                MProcess process = (MProcess) i.next();
                if (process.getName().equals(nodeName)) {
                    coreId = cpuList.indexOf(mp);
            if(coreId!=null)
			    return coreId;
                }
            }
        }

        System.err.println("Mapping core not found for CSDF node "+nodeName+" . Default core is assigned");
        return 1;
    }


    /**
     * get next free core id (Moc in case the mapping is not provided)
     * @return next free core id
     */
    private Integer _assignFreeCore(){
        if(_curCore>=(_maxCores-1))
            _curCore=0;
        else
            _curCore++;
        return _curCore;
    }



    /***********************************************************/
    /**   getters and setters             *********************/

    /**
     * Set csdfg schedule
     * CSDFG nodes schedule : get from the repetition vector of CSDFG??
     * @param schedule CSDFG nodes schedule : get from the repetition vector of CSDFG??
     */
    public void setSchedule(Vector<String> schedule){
        _nodes = schedule;
    }

    /**
     * Set flag, if the debug couts should be printed
     * @param silent silent flag
     */
    public void setSilent(boolean silent) {
        CPPSDFGVisitorPthread._silent = silent;
    }

    /**
     * Set maximum cores number
     * @param maxCores maximum cores number
     * TODO should be replaced by mapping specification
     */
    public void setMaxCores(int maxCores) {
        CPPSDFGVisitorPthread._maxCores = maxCores;
    }


    /**
     * Set mapping for he CSDF graph
     * @param mapping mapping for the CSDF graph
     */
    public void setMapping(Mapping mapping) {
        this._mapping = mapping;
    }

    /** use neuraghe functions*/
    public void setGenerateFuncNA(boolean generateDNNFuncNA) {
        _generateDNNFuncNA = generateDNNFuncNA;
    }

    /**Set number of inputs to be processed, until the application stops.
     * Default batch = 1;
     * @param batch number of inputs to be processed, until the application stops.
     */
    public void setBatch(Integer batch){
        _batch = batch;
    }

    /**
     * Set max number of input samples to be stored between two nodes
     * Default value = 10
     * @param fifoScale scale factor of fifo buffers
     */
    public void setFifoScale(Integer fifoScale){
        _fifoScale = fifoScale;
    }

    ///////////////////////////////////////////////////////////////////
    ///           internal DNN library templates                   ///

    /** Generate function that loads weights from external numpy files*/
    private void _generateDataLoadFunc(){
       _printStream.println(_prefix + "void dataLoader::data_load_from_numpy(std::string srcpathprefix, int partition_id, int partition_size, int start, int shift, " + _IODatatype + "* dst) {");
       prefixInc();
       _printStream.println(_prefix + "/** form the source file path*/");
       _generateDataPartitionPath();
       _printStream.println("");
       _printStream.println(_prefix + "/** load  source file into a new array */");
       _printStream.println(_prefix + "cnpy::NpyArray arr = cnpy::npy_load(srcfile);");
       _printStream.println(_prefix + _IODatatype +"* loaded_data = arr.data<" + _IODatatype +">();");
       _printStream.println("");
       _printStream.println(_prefix + "/** copy loaded array to destination array */");
       _printStream.println(_prefix + "for(int i = start;i<partition_size + start;i++)");
       prefixInc();
       _printStream.println(_prefix + "dst[i+shift-start] = *(loaded_data + i);");
       prefixDec();
       prefixDec();
       _printStream.print(_prefix);
       if(_silent)
           _printStream.print("//");
       _printStream.print("std::cout<<srcfile<<\" loaded \"<<std::endl;");
       _printStream.println(" ");
       _printStream.println(_prefix + "}");
    }

    /** Generate data partiotion path for weights load function*/
    private void _generateDataPartitionPath(){
       _printStream.println(_prefix + "std::string srcfile;");
       _printStream.println(_prefix + "srcfile.append(srcpathprefix);");
       _printStream.println(_prefix + "if (partition_id>=0) { ");
       _prefixInc();
       _printStream.println(_prefix + "std::stringstream ss;");
       _printStream.println(_prefix + "ss << partition_id;");
       _printStream.println(_prefix + "std::string stri = ss.str();");
       _printStream.println(_prefix + "srcfile.append(stri);");
       _prefixDec();
       _printStream.println(_prefix + "} ");
       _printStream.println(_prefix + "srcfile.append(\".npy\");");
    }

    public void setMocMode(boolean mocMode){
        _mocMode = mocMode;
    }


    ///////////////////////////////////////////////////////////////////
    ///                private variables                           ///

    /** primitive postfix*/
    private static String _externalRWPostfix = "SWF_CPU";

    /** application main class name*/
    private static String _mainClassName = "appMain";

    /** CSDF graph node base class*/
    private static String _baseClassName = "csdfNode";

    /** CSDF graph node functions class*/
    private static String _funcClassName = "appFunc";

    /** CSDF graph node base class*/
    private static String _loadWeightsClassName = "dataLoader";

    /** DNN input/output type */
    public String _IODatatype = "int";

    /** DNN parameters type*/
    public String _paramDataType = "int";

    /** CSDFG nodes schedule : get from the repetition vector of CSDFG??*/
    Vector<String> _nodes;

    /** current mapping */
    Mapping _mapping = null;


    /** Mapping moc, in case mapping is not provided*/
    /** Number of cores */
    private static int _maxCores = 6;

    /** Current core Id */
    private static int _curCore = 0;

    /**
     * TODO should be replaced by an external parameter
     * partition size
     */
    private static int _partition_size = 100;

    /** If the debug couts should be printed*/
    private static boolean _silent = true;

    /** if weights should be initialized with dummy values*/
    private static boolean _initWeightsDummy = false;

    /** If the NA library is used*/
    private static boolean _generateDNNFuncNA = false;

    /** How many input samples should be processed, until the application stops.
     * If batch ==-1, application runs infinitely
     */
    private Integer _batch = 1;

    /** How many input samples can be stored between two nodes
     * Default value = 10
     */
    private Integer _fifoScale = 10;

    private static boolean _mocMode;
}
