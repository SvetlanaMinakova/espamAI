package espam.visitor.pthread.cpp;

import espam.datamodel.graph.csdf.*;
import espam.datamodel.graph.csdf.datasctructures.MemoryUnit;
import espam.datamodel.graph.csdf.datasctructures.Tensor;
import espam.datamodel.mapping.MProcess;
import espam.datamodel.mapping.MProcessor;
import espam.datamodel.mapping.Mapping;
import espam.main.Config;
import espam.parser.xml.mapping.XmlMappingParser;
import espam.utils.fileworker.FileWorker;
import espam.visitor.pthread.weights.WeightsLoader;
import espam.visitor.sesame.cpp.CPPSDFGVisitor;

import java.io.File;
import java.util.Iterator;
import java.util.Vector;

public class CPPSDFGVisitorPthread extends CPPSDFGVisitor {
    ///////////////////////////////////////////////////////////////////
    ////                         public methods                     ///

     /**
     * Call CPP SDFG Visitor of the operational node (node, performs some useful job)
     * @param y  corresponding CSDFNode
     * @param dir directory for .cpp templates
     */
     @Override
     public void callVisitor(CSDFNode y, String dir){
         try {
             _printStream = FileWorker.openFile(dir, y.getName(), "cpp");
             _writeCommonCppBeginning(y.getName());
             _writeAdditionalLibraries();
             _writeCppConstructorAndDestructor(y);
             _writeMain(y);
         }
         catch (Exception e){
             System.err.println(".cpp file creation error for node" + y.getName() + " " + e.getMessage());
         }
     }

     /////////////////////////////////////////////////////////////////////
    /////////                class templates      ///////////////////////

    /**
     * generate main class template contains
     * application structure definition and
     * application control logic
     */
    public void generateMainClassTemplate(String dir, CSDFGraph sdfg){
        try {
            _printStream = FileWorker.openFile(dir, _mainClassName, "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("");
            _addAllClassesHeaders(sdfg);
            _writeMainClassCPPBeginning();
            _writeNoBaseCppConstructorAndDestructor(_mainClassName);
            _writeMainClassMain(sdfg);
            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + _mainClassName + " " + e.getMessage());
        }
    }

      /**
     * generate application functions class, that contains functions, common
     * for every node class, such as execution and R/W primitives, print functions,
     * API to application operators library etc.
     */
    public void generateFuncClassTemplate(String dir, boolean incDnnFuncCPU, boolean incDNNFuncGPU){
        try {
            _printStream = FileWorker.openFile(dir, _funcClassName, "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("");
            _writeFuncClassCPPBeginning((incDnnFuncCPU||incDNNFuncGPU));
            _writeNoBaseCppConstructorAndDestructor(_funcClassName);
            _writeExecPrimitives(incDnnFuncCPU,incDNNFuncGPU);
            _writeFunctions();
            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + _funcClassName + " " + e.getMessage());
        }
    }

     /**
     * generate base class template contains common lines for all Layer's classes
     */
    public void generateBaseClassTemplate(String dir){
        try {
            String className = "csdfNode";
            _printStream = FileWorker.openFile(dir, className, "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("#include \"" + _baseClassName + ".h\"");
            _printStream.println(className + "::" + className + "() {}");
            _printStream.println(className + "::~" + className + "() {}");
            _printStream.println("");
            _printStream.println("//virtual main function");
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + _mainClassName + " " + e.getMessage());
        }
    }

         /**
     * generate base class template contains common lines for all Layer's classes
     */
    public void generateDataLoadClassTemplate(String dir){
        try {
            String className = _loadWeightsClassName;
            _printStream = FileWorker.openFile(dir, _loadWeightsClassName, "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("#include \"cnpy.h\"");
            _printStream.println("#include <iostream>");
            _printStream.println("#include <string>");
            _printStream.println("#include \"" + _loadWeightsClassName + ".h\"");

            _printStream.println(className + "::" + className + "() {}");
            _printStream.println(className + "::~" + className + "() {}");
            _printStream.println("");
            _generateDataLoadFunc();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + _mainClassName + " " + e.getMessage());
        }
    }



    ///////////////////////////////////////////////////////////////////
    ///                standard class templates                    ///

    /**
     * FIFO class - describes FIFO communication buffer between two nodes
     * @param dir directory to generate the class file
     */
    public void generateFIFOClassTemplate(String dir){
        try {
            _printStream = FileWorker.openFile(dir, "fifo", "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("");
            _printStream.println("#include \"fifo.h\"");
            _printStream.println("#include <pthread.h>");
            _printStream.println("#include <errno.h>");
            _printStream.println("#include <stdio.h>");
            _printStream.println("#include <stdlib.h>");
            _printStream.println("#include <string.h>");
            _printStream.println("#include <iostream>");

            String firstElem="2";

            if(_generateDNNFuncNA) {
                _printStream.println("#include <mutex>");
                _printStream.println("#include \"types2.h\"");
                firstElem = "FIRST_ELEM";
            }

            _printStream.println("");
            _printStream.println("using namespace std;");
            _printStream.println("");
            _printStream.println(_prefix + "// Copy data from cpu device memory to FIFO");
            _printStream.println("void writeSWF_CPU(void* fifo, void* memobj_cpu, int len, int fifo_size) {");
             prefixInc();
            _printStream.println("");
            _printStream.println(_prefix + "while( FIFOisFull(fifo)) {pthread_yield(); };");
            _printStream.println("");
            _printStream.println(_prefix + "int w_cnt = ((int*)fifo)[0];");
            _printStream.println("");
            _printStream.println(_prefix + "// Will copy data from cpu device memory to FIFO");
            _printStream.println(_prefix + "int i;");
            _printStream.println(_prefix + "for ( i = 0; i < len; i++) {");
            _prefixInc();
            _printStream.println(_prefix+"((int*)fifo)[(w_cnt & 0x7FFFFFFF) + " + firstElem + " + i] = ((int*)memobj_cpu)[i];");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _printStream.println("");
            _printStream.println(_prefix + "w_cnt += len;\t");
            _printStream.println("");
            _printStream.println(_prefix + "if( (w_cnt & 0x7FFFFFFF) == fifo_size ) {");
            _prefixInc();
            _printStream.println(_prefix + "w_cnt &= 0x80000000;");
            _printStream.println(_prefix + "w_cnt ^= 0x80000000;");
            _prefixDec();
            _printStream.println("}");
            _printStream.println("");
            _printStream.println(_prefix + "((int*)fifo)[0] = w_cnt;");
            _prefixDec();
            _printStream.println(_prefix + "}");

            for(int i=0; i<3;i++)
                _printStream.println("");

            _printStream.println(_prefix + "// Copy data from FIFO to cpu device memory");
            _printStream.println("void readSWF_CPU(void* fifo, void* memobj_cpu, int len, int fifo_size) {");
            _prefixInc();
            _printStream.println("");
            _printStream.println(_prefix + "while( FIFOisEmpty(fifo)) { pthread_yield(); };");
            _printStream.println("");
            _printStream.println(_prefix + "int w_cnt = ((int*)fifo)[0];");
            _printStream.println(_prefix + "int r_cnt = ((int*)fifo)[1];");
            _printStream.println("");
            _printStream.println(_prefix + "// Will copy data from FIFO to cpu device memory");
            _printStream.println(_prefix + "int i;");
            _printStream.println(_prefix + "for ( i = 0; i < len; i++) {");
            _prefixInc();
            _printStream.println(_prefix + "((int*)memobj_cpu)[i] = ((int*)fifo)[(r_cnt & 0x7FFFFFFF) + " + firstElem + " + i];");
            _prefixDec();
            _printStream.println("}");
            _printStream.println("");
            _printStream.println(_prefix + "r_cnt += len;");
            _printStream.println("");
            _printStream.println(_prefix + "if( (r_cnt & 0x7FFFFFFF) == fifo_size ) {");
            _prefixInc();
            _printStream.println("");
            _printStream.println(_prefix + "r_cnt &= 0x80000000;");
            _printStream.println(_prefix + "r_cnt ^= 0x80000000;");
            _prefixDec();
            _printStream.println("}");
            _printStream.println(_prefix + "((int*)fifo)[1] = r_cnt;");
            prefixDec();
            _printStream.println("}");

            for(int i=0; i<3;i++)
                _printStream.println("");

            _printStream.println(_prefix + "// Check is FIFO is full");
            _printStream.println(_prefix + "int FIFOisFull(void *fifo){");
            prefixInc();
            _printStream.println("");
            _printStream.println(_prefix + "int r_cnt = ((int *)fifo)[1];");
            _printStream.println(_prefix + "int w_cnt = ((int *)fifo)[0];");
            _printStream.println("");
            _printStream.println(_prefix + "if ( r_cnt == (w_cnt ^ 0x80000000) ){");
            _prefixInc();
            _printStream.println(_prefix + "return (1);");
            _prefixDec();
            _printStream.println(_prefix + "} else {");
            _prefixInc();
            _printStream.println(_prefix + "return(0);");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _prefixDec();
            _printStream.println(_prefix + "}");

            for(int i=0; i<3;i++)
                _printStream.println("");

            _printStream.println(_prefix + "// Check is FIFO is empty");

            _printStream.println("int FIFOisEmpty(void *fifo){");
            _prefixInc();
            _printStream.println(_prefix + "int r_cnt = ((int *)fifo)[1];");
            _printStream.println(_prefix + "int w_cnt = ((int *)fifo)[0];");
            _printStream.println("");
            _printStream.println(_prefix + "if ( w_cnt == r_cnt ){");
            _prefixInc();
            _printStream.println(_prefix + "return (1);");
            _prefixDec();
            _printStream.println(_prefix + "} else {");
            _prefixInc();
            _printStream.println(_prefix + "return(0);");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _prefixDec();

            _printStream.println("}");

            for(int i=0; i<3;i++)
                _printStream.println("");

            _printStream.println(_prefix + "// CPU affinity mask of the thread to the CPU core ");
            _printStream.println("void setaffinity(int core){");
            _printStream.println("");
            _printStream.println("    pthread_t pid = pthread_self();");
            _printStream.println("    int core_id = core;");
            _printStream.println("");
            _printStream.println("//   cpu_set_t: This data set is a bitset where each bit represents a CPU.");
            _printStream.println("  cpu_set_t cpuset;");
            _printStream.println("//  CPU_ZERO: This macro initializes the CPU set set to be the empty set.");
            _printStream.println("  CPU_ZERO(&cpuset);");
            _printStream.println("//   CPU_SET: This macro adds cpu to the CPU set set.");
            _printStream.println("  CPU_SET(core_id, &cpuset);");
            _printStream.println("");
            _printStream.println("//   pthread_setaffinity_np: The pthread_setaffinity_np() function sets the CPU affinity mask of the thread thread to the CPU set pointed to by");
            _printStream.println("//cpuset. If the call is successful, and the thread is not currently running on one of the CPUs in cpuset, then it is migrated to one of those CPUs.");
            _printStream.println("   const int set_result = pthread_setaffinity_np(pid, sizeof(cpu_set_t), &cpuset);");
            _printStream.println("  if (set_result != 0) {");
            _printStream.println("    printf(\"pthread setaffinity failed!\\n\");");
            _printStream.println("  }");
            _printStream.println("");
            _printStream.println("");
            _printStream.println(" //  Check what is the actual affinity mask that was assigned to the thread.");
            _printStream.println(" //  pthread_getaffinity_np: The pthread_getaffinity_np() function returns the CPU affinity mask of the thread thread in the buffer pointed to by cpuset.");
            _printStream.println("  const int get_affinity = pthread_getaffinity_np(pid, sizeof(cpu_set_t), &cpuset);");
            _printStream.println("  if (get_affinity != 0) {");
            _printStream.println("    printf(\"pthread getaffinity failed!\\n\");");
            _printStream.println("  }");
            _printStream.println("");
            _printStream.println("  char *buffer;");
            _printStream.println("  // CPU_ISSET: This macro returns a nonzero value (true) if cpu is a member of the CPU set set, and zero (false) otherwise.");
            _printStream.println("  if (CPU_ISSET(core_id, &cpuset)) {");

            if(_silent)
                _printStream.print("//");
            _printStream.print("    printf(\"Successfully set thread %u to cpu core %d\\n\", pid, core_id);");
            _printStream.println("");
            _printStream.println("  } else {");

            if(_silent)
                _printStream.print("//");
            _printStream.print("    printf(\"failed!\\n\");");
            _printStream.println("");

            _printStream.println("  }");
            _printStream.println("}");
            for(int i=0; i<8;i++)
                _printStream.println("");
            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + "fifo" + " " + e.getMessage());
        }
    }


    /**
     * Generate run class - the entry point of the application
     * @param dir directory to generate the run class
     */
    public void generateRunClassTemplate(String dir){
        try {
            _printStream = FileWorker.openFile(dir, "run", "cpp");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("//============================================================================");
            _printStream.println("// Name        : erqian_proj2.cpp");
            _printStream.println("// Author      : Minakova S.  Erqian Tang");
            _printStream.println("// Version     :");
            _printStream.println("// Copyright   : Your copyright notice");
            _printStream.println("// Description : Hello World in C++, Ansi-style");
            _printStream.println("//============================================================================\n");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("#include <iostream>");
            _printStream.println("#include \"appMain.h\"");
            _printStream.println("");
            _printStream.println("using namespace std;");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("int main() {");
            _printStream.println("\t//cout << \"!!!Hello World!!!\" << endl; // prints !!!Hello World!!!");
            _printStream.println("\tappMain app = appMain();");
            _printStream.println("\tapp.main();");
            _printStream.println("\treturn 0;");
            _printStream.println("}");
            _printStream.println("");
            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + "run" + " " + e.getMessage());
        }
    }

     /////////////////////////////////////////////////////////////////////
    /////////   cpp beginnings                     //////////////////////

    /**
     * TODO refactoring on idea
     * mention all available nodes
     */
    protected void  _addAllClassesHeaders(CSDFGraph csdfG){
         Iterator i = csdfG.getNodeList().iterator();
        while (i.hasNext()) {
            CSDFNode node = (CSDFNode) i.next();
            _printStream.println("#include \""+ node.getName() + ".h\"");
        }
    }


    /**
     * Write common beginning for all generated nodes, contains:
     *  - definition of header
     *  - definition of standard libraries
     *  - definition of namespace
     * @param className name of the .cpp class
     */
    @Override
    protected void _writeCommonCppBeginning(String className){
        _printStream.println("// File automatically generated by ESPAM");
        _printStream.println("");
        _printStream.println("#include \""+ className + ".h\"");
        _printStream.println("#include <stdlib.h>");
        _printStream.println("#include <iostream>");
        _printStream.println("#include \"" + _baseClassName + ".h\"");
        _printStream.println("#include \""+_mainClassName+".h\"");
        _printStream.println("#include \""+_funcClassName+".h\"");
        /** include existing primitives definition*/
        _printStream.println("#include \"fifo.h\"");
        _printStream.println("#include <cstddef>");
        _printStream.println("#include \"types.h\"");
        //include null
        _printStream.println("#include <vector>");
        _printStream.println("#include <string>");
        //include weights loader
        _printStream.println("#include \"dataLoader.h\"");
        //use standard namespance
        _printStream.println("using namespace std;");
        _printStream.println("");
    }

    /**
     * Write application main class .cpp beginning
     */
    protected void _writeMainClassCPPBeginning(){

         /** TODO: should I define any libraries in here?? Or they will
          * TODO be copied from the graphName.so file?*/
        _printStream.println("#include <stdlib.h>");
        _printStream.println("#include <iostream>");
        _printStream.println("#include <map>");
        _printStream.println("#include <vector>");
        _printStream.println("#include \"" + _baseClassName + ".h\"");
        _printStream.println("#include \""+_mainClassName+".h\"");
        _printStream.println("#include \""+_funcClassName+".h\"");
        /** include existing primitives definition*/
        _printStream.println("#include \"fifo.h\"");
        _printStream.println("#include <cstddef>");
        _printStream.println("#include <thread>");
        _printStream.println("#include \"dataLoader.h\"");
        if(_generateDNNFuncNA){
        _printStream.println("#include <unistd.h>");
        _printStream.println("#include \"dnnFunc.h\"");
        _printStream.println("#include <chrono>");
        }
        else
            _printStream.println("#include \"types.h\"");
        _printStream.println("");
        _printStream.println("using namespace std;");
        _printStream.println("");
    }

    /**
     * Write application main class .cpp beginning
     */
    protected void _writeFuncClassCPPBeginning(boolean incDnnFunc){

         /** TODO: should I define any libraries in here?? Or they will
          * TODO be copied from the graphName.so file?*/
        _printStream.println("#include <stdlib.h>");
        _printStream.println("#include <iostream>");
        _printStream.println("#include <string>");
        _printStream.println("#include <vector>");
        _printStream.println("#include <map>");
        _printStream.println("#include \"" + _funcClassName + ".h\"");
        _printStream.println("#include \"types.h\"");
        /**TODO replace by real API*/
        if(incDnnFunc)
            _printStream.println("#include \"dnnFunc.h\"");
        /** include existing primitives definition*/
        _printStream.println("#include <cstddef>");
        _printStream.println("using namespace std;");
        _printStream.println("");
    }

    /////////////////////////////////////////////////////////////////////
    /////////     constructors and destructors of the classes   /////////

    /**
     * Write constructor and destructor .cpp definitions
     * @param className name of the .cpp class
     */
    @Override
    protected void  _writeCppConstructorAndDestructor(String className, String baseClassName){
        _printStream.println(_prefix + className + "::" + className + "() : " + _baseClassName + "() {}");
        _printStream.println(className + "::~" + className + "() {}");
        _printStream.println("");
    }

    /**
     * Write constructor and destructor .cpp definitions
     * @param node CSDF node
     */
    protected void  _writeCppConstructorAndDestructor(CSDFNode node){
        String className = node.getName();
        _printStream.println(_prefix + className + "::" + className + "() : " + _baseClassName + "() {");
        prefixInc();
        _writeFIFOsizes(node);
        _fillInIntParams(node);
        _loadParameters(node);
        prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(className + "::~" + className + "() {}");
        _printStream.println("");
    }

    /**
     * Load external parameters such as weights and biases
     * @param node CSDF node
     */
    protected void _loadParameters(CSDFNode node){
        MemoryUnit weights = node.getMemoryUnit("weights");
        MemoryUnit bias = node.getMemoryUnit("bias");

        if(weights==null && bias==null)
            return;

        _printStream.println(_prefix + "/** parameters load */");
        _printStream.println(_prefix + "dataLoader dl = dataLoader();");


        if (weights != null) {
            if (_initWeightsDummy)
                    _initArrDummyLinear(weights.getDimensionality(), "w", "weights", 1);
                 else
                    _loadWeightsFromFiles(weights.getShape(),node.getName(),node.getKernelsNum(),node.getOperation());
        }

        if (bias!=null){
            if (_initWeightsDummy)
                    _initArrDummyLinear(bias.getDimensionality(), "b", "bias", 1);
                 else
                    _loadBiasFromFiles(node);
        }
    }

     /**
     * Load weights from external files
     * @param node  node, containing bias
     */
    private void _loadBiasFromFiles(CSDFNode node){
        _printStream.println(_prefix + "/** bias load */");
        String biasSrcName = node.getName();
        MemoryUnit biasRef = node.getMemoryUnit("bias_ref");
        if(biasRef!=null) {
            biasSrcName = biasRef.getUnitParamDesc().replace("\"", "");
        }
        _printStream.println(_prefix + "std::string bias_path = \"./../../weights_npz/" + biasSrcName + "_b\";");
        _printStream.println(_prefix + "dl.data_load_from_numpy(bias_path, (-1), bias_len, neuron_start_id, 0, &bias[0]);");
    }

    /**
     * Load weights from external files
     * @param tensor espam. Tensor
     * @param nodeName name of the node, containing weights
     * @param kernels number of kernels in the node
     * @param operation operation, performed by the node
     */
    private void _loadWeightsFromFiles(Tensor tensor, String nodeName, int kernels, String operation){

        if(operation.toLowerCase().contains("conv"))
            _loadWeightsFromFilesConv(tensor,nodeName,kernels);
        else
            _loadWeightsFromFilesDense(tensor,nodeName);
    }

    /**
     * Load weights from external files
     * @param tensor espam. Tensor
     */
    private void _loadWeightsFromFilesDense(Tensor tensor, String nodeName){
        try {
               //int partition_size = (tensor.getDimSize(1)*_partition_size);
               //math.floor(neurs/partition_size)
               int neurs = tensor.getDimSize(0);
               int partitions = (neurs/_partition_size);
               int partition_size;
               int tail;
               int last_partition_size = 0;

               partition_size = (_partition_size * tensor.getDimSize(1));
               tail = tensor.getElementsNumber() - partitions * partition_size;


               if(tail>0) {
                   partitions = partitions + 1;
                   last_partition_size = tail;
               }

               _printStream.println(_prefix + "/** weights load */");
               _printStream.println(_prefix + "std::string weights_path_prefix = \"./../../weights_npz/" + nodeName + "_w\";");
               _printStream.println(_prefix + "int partition_size = " + partition_size + ";");
               _printStream.println(_prefix + "int partitions_num = " + partitions + ";");
               _printStream.println(_prefix + "/** shift of the partition beginning from the beginning of the weights array */");
               _printStream.println(_prefix + "int shift = 0; ");

               _printStream.println(_prefix + "/** partitions processing */");
               _printStream.println(_prefix + "for(int i=neuron_start_id; i<partitions_num-1;i++) {");
               prefixInc();
               _printStream.println(_prefix + "dl.data_load_from_numpy(weights_path_prefix, i, partition_size, neuron_start_id, shift, &weights[0]);");
               _printStream.println(_prefix + "shift += partition_size;");
               prefixDec();
               _printStream.println(_prefix+ "}");
               _printStream.println(_prefix + "/** last partition processing */");
               _printStream.println(_prefix + "dl.data_load_from_numpy(weights_path_prefix, " + (partitions-1) + ", " + last_partition_size + ", neuron_start_id, shift, &weights[0]);");

            }
            catch (Exception e){
               //_printStream.println("0 };");
               _printStream.println("//ERROR: weights load error " + e.getMessage());
            }


    }

    /**
     * Load weights from external files
     * @param tensor espam. Tensor
     */
    private void _loadWeightsFromFilesConv(Tensor tensor, String nodeName, int kernels){
        //_printStream.print(_prefix + typeDesc + " " + arrname + "[" + tensor.getElementsNumber() + "] = { ");

           try {
               int partitions = kernels;
               _printStream.println(_prefix + "/** weights load */");
               _printStream.println(_prefix + "std::string weights_path_prefix = \"./../../weights_npz/" + nodeName + "_w\";");
               _printStream.println(_prefix + "int partition_size = " + tensor.getElementsNumber()/kernels + ";");
               _printStream.println(_prefix + "int partitions_num = " + partitions + ";");
               _printStream.println(_prefix + "/** shift of the partition beginning from the beginning of the weights array */");
               _printStream.println(_prefix + "int shift = 0; ");
               _printStream.println(_prefix + "for(int i=neuron_start_id; i<partitions_num;i++){ ");
               prefixInc();
               _printStream.println(_prefix + "dl.data_load_from_numpy(weights_path_prefix, i, partition_size, neuron_start_id, shift, &weights[0]);");
               _printStream.println(_prefix + "shift += partition_size;");
               prefixDec();
               _printStream.println(_prefix + "}");
            }
            catch (Exception e){
               //_printStream.println("0 };");
               _printStream.println("//ERROR: weights load error " + e.getMessage());
            }
    }

         /**
     * Write constructor and destructor .cpp definitions
     * @param className name of the .cpp class
     */
    protected void  _writeNoBaseCppConstructorAndDestructor(String className){
        _printStream.println(className + "::" + className + "() {}");
        _printStream.println(className + "::~" + className + "() {}");
        _printStream.println("");
    }


    /////////////////////////////////////////////////////////////////////
    /////////     main functions of the classes                /////////

    /*****************************************************************************
     * Write main function of the main class (class contains the application logic)
     * @param csdfg CSDF graph, represents the application
     ****************************************************************************/

    protected void _writeMainClassMain(CSDFGraph csdfg){
        _printStream.println("// Main function");
        _prefixInc();
        _printStream.println(_prefix + "void " + _mainClassName + "::main()");
         prefixInc();
        //open main function
         _printStream.println(_prefix + "{");
         prefixInc();
         _printStream.println(_prefix + "// list of all available nodes");
         _printStream.println(_prefix + "int total_nodes = " + csdfg.countNodes() + ";");
         _printStream.println(" ");
         _printStream.println(_prefix + "std::map< std::string, "+ _baseClassName +
                 "* > nodes = std::map< std::string," + _baseClassName + "* >();");
         _createAllClassesInstancesAndRefs(csdfg);
         _printStream.println("");
         _defineIONodes(csdfg);
       //  _writeSchedule();
         _printStream.println("");
         _printStream.println(_prefix + "// Prepare shared data and memory");
         _defineFIFOChannels(csdfg);
         _printStream.println(_prefix + "// Preparation work for threads on CPU");
         _prepareThreads();
         _fillInThreadInfo(csdfg);
         _printStream.println("");
         if(_silent)
             _printStream.print("// ");
         _printStream.print(_prefix + "std::cout<<\" Application topology is constructed! \"<<std::endl;");
         _printStream.println("");
         //_defineDataMoc(csdfg);
         _defineDataLoader(csdfg.getName());
         /** Todo - to be changed*/
         int appRepetitions = _getAppRepetitionsFromExamplesNum(csdfg.getName());
         _openAppRepetitions(appRepetitions);
         _simulateDataLoad(csdfg);
         _createAndRunThreads(csdfg);
         _printStream.println("");
         _joinThreads();
        /* _printStream.println("");
         _printStream.println(_prefix + "// Call dnn nodes according to the schedule ");
         _callNodesInScheduleOrder();*/
         _printStream.println("");
         if(_silent)
             _printStream.print("// ");
         _printStream.print(_prefix + "cout<<\"Generated program run \"<<inp_img<<\" is finished!\"<<endl;");
         _printStream.println("");
         /** TODO delete after debug is finished*/
         _writeNodeOutputs(csdfg);
         _simulateOutputDataDisplay();
        _closeAppRepetitions();
         prefixDec();
         //close main function
         _printStream.println(_prefix + "}");
         prefixDec();
        _prefixDec();
        _printStream.println("");
    }

    /**
     * compute number of application repetitions, based on
     * numper of input data examples in default data folder
     * @return number of application repetitions, based on
     * numper of input data examples in default data folder
     */
    private int _getAppRepetitionsFromExamplesNum(String graphName){
        try {
            String defaultDataPath = _getDefaultDataPath(graphName);
            Vector<String> inputPaths = FileWorker.getAllFilePaths(defaultDataPath, "npy");
            return inputPaths.size();
        }
        catch (Exception e){
            System.out.println("application repetitions computation error: " + e.getMessage());
            return 0;
        }

    }


    /**
     * TODO refactoring on idea
     * Create instances of all node and references on them
     * @param csdfG csdf graph
     */
    protected void _createAllClassesInstancesAndRefs(CSDFGraph csdfG){
        Iterator i = csdfG.getNodeList().iterator();
        String nodename;
        while (i.hasNext()) {
            CSDFNode node = (CSDFNode) i.next();
            nodename = node.getName();
            _printStream.println(_prefix + nodename + " " + nodename + "_inst = " + nodename + "();");
            _printStream.println(_prefix + "nodes[\"" + nodename + "\"] = &" + nodename + "_inst;");
        }

    }


    //////////////////             I/O data             /////////////////////
      /**
     * Define CSDF graph input and output nodes
     * @param csdfg CSDF graph
     */
    protected void _defineIONodes(CSDFGraph csdfg){

        _printStream.println(_prefix + "/** Application input and output nodes*/");
        _defineInputNode(csdfg);
        _defineOutputNode(csdfg);
    }

    /**
     * Define application input (src) node and input data length
     * @param csdfg CSDF graph
     */
    protected void _defineInputNode(CSDFGraph csdfg){
        CSDFNode inputNode = csdfg.getSrcNode();
        if(_processDataNodeNullOrEmpty(inputNode))
            return;
        MemoryUnit mu = inputNode.getMemoryUnit("output");
        String inputDataDesc = _getLinearArrRef(mu).replace("&","");
        _printStream.println(_prefix + mu.getTypeDesc()+"* app_input = &" + inputNode.getName() + "_inst."+ inputDataDesc +";");
        _printStream.println("");
        /** input data dimensions*/
        _printStream.println(_prefix + "int data_len = " + mu.getShape().getElementsNumber() + ";");
    }

       /**
     * Define application input (src) node and input data length
     * @param csdfg CSDF graph
     */
    protected void _defineOutputNode(CSDFGraph csdfg){
        CSDFNode outputNode = csdfg.getSnkNode();
        if(_processOutputNodeNullOrEmpty(outputNode))
            return;
        MemoryUnit mu = outputNode.getMemoryUnit("input");

        _printStream.print(_prefix + mu.getTypeDesc()+"* app_output = &" + outputNode.getName() + "_inst.input");
        for(int i=0;i<mu.getDimensionality();i++)
            _printStream.print("[0]");
        _printStream.print(";");
        _printStream.println("");
        /** input data dimensions*/
        _printStream.println(_prefix + "int output_len = " + mu.getShape().getElementsNumber() + ";");
    }
    
     /**
     * Define moc-up data of the same size as application input
     * @param csdfg CSDF graph
     */
    protected void _defineDataMoc(CSDFGraph csdfg){
        CSDFNode inputNode = csdfg.getSrcNode();
        _printStream.println(_prefix + "//Input data moc");
        _printStream.println(_prefix + "//TODO: remove when data loader is provided");
        if(_processDataNodeNullOrEmpty(inputNode))
            return;
        MemoryUnit mu = inputNode.getMemoryUnit("output");
        _printStream.println(_prefix + mu.getTypeDesc() + " data_moc[data_len]={0};");
        _printStream.println(_prefix + "for (int dat=0; dat < data_len; dat++)");
        prefixInc();
        _printStream.print(_prefix + "data_moc[dat] = 1;");
        prefixDec();
        _printStream.println("");
    }

         /**
     * Define moc-up data of the same size as application input
     */
    protected void _defineDataLoader(String graphName){
        _printStream.println(_prefix + "//Input data loader example");
        _printStream.println(_prefix + "//TODO: replace by your own data loader is provided");
        _printStream.println(_prefix + "dataLoader dl = dataLoader();");
        _printStream.println(_prefix + "std::string data_path = "+"\"../../data/inputs/input\";");
        _printStream.println("");
    }

    /**
     * Get data path, provided for CSDF graph by default
     * @return data path, provided for CSDF graph by default
     */
    protected String _getDefaultDataPath(String graphName){
        return Config.getInstance().getOutputDir() + File.separator + graphName + File.separator + "data" + File.separator + "inputs";
    }


    /**
     * Simulate setting input of input (data) node to moc value (one)
     */
    protected void _simulateDataLoadMoc(CSDFGraph csdfg){
        CSDFNode inputNode = csdfg.getSrcNode();
        _printStream.println(_prefix + "//Simulate input data load");
        _printStream.println(_prefix + "//TODO: load your data here. The data should have length, defined in data_len variable");
        if(_processDataNodeNullOrEmpty(inputNode))
            return;

        _printStream.println(_prefix + "for (int dat=0; dat<data_len; dat++)");
        _prefixInc();
        _printStream.println(_prefix + "app_input[dat]=data_moc[dat];");
        _prefixDec();
        _printStream.println("");
        _printStream.println(_prefix + "//Simulate setting input of input (data) node");
        _printStream.println("");

    }

    /**
     * Simulate setting input of input (data) node
     * @param csdfg CSDF graph
     */
    protected void _simulateDataLoad(CSDFGraph csdfg){
        CSDFNode inputNode = csdfg.getSrcNode();
        _printStream.println(_prefix + "//Simulate input data load");
        _printStream.println(_prefix + "//TODO: load your data here. The data should have length, defined in data_len variable");
        if(_processDataNodeNullOrEmpty(inputNode))
            return;
        _printStream.println(_prefix + "dl.data_load_from_numpy(data_path, inp_img, data_len, 0, 0, app_input);");
    }

    /**
     * Process the case, when data node is null or contains no data (empty)
     * @return true, if data node is null or contains no data (empty) and false otherwise
     */
    private boolean _processDataNodeNullOrEmpty(CSDFNode dataNode){
        if(dataNode ==null){
            _printStream.println(_prefix + "//No input node found!");
            return true;
        }
        MemoryUnit mu = dataNode.getMemoryUnit("output");

        if(mu==null){
            _printStream.println(_prefix + "//Input node has no data!");
            return true;
        }

        return false;
    }

        /**
     * Process the case, when data node is null or contains no data (empty)
     * @return true, if data node is null or contains no data (empty) and false otherwise
     */
    private boolean _processOutputNodeNullOrEmpty(CSDFNode dataNode){
        if(dataNode ==null){
            _printStream.println(_prefix + "//No output node found!");
            return true;
        }
        MemoryUnit mu = dataNode.getMemoryUnit("input");

        if(mu==null){
            _printStream.println(_prefix + "//Output node has no data!");
            return true;
        }

        return false;
    }

    protected void _writeNodeOutputs(CSDFGraph csdfg){
        _printStream.println(_prefix + "/**");
        _printStream.println(_prefix + "* In this part the beginning of nodes outputs is printed, ");
        _printStream.println(_prefix + "* which might be useful for debug purposes");
        _printStream.println(_prefix + "* TODO: delete printout of nodes outputs, if not in debug mode");
        _printStream.println(_prefix + "*/");

        if(_silent)
            _printStream.println(_prefix + "/**");
        _printStream.println(_prefix + "// how many values should be printed");
        _printStream.println(_prefix + "int show_values = 10;");
        prefixInc();

         Iterator i = csdfg.getNodeList().iterator();
        while (i.hasNext()) {
            CSDFNode node = (CSDFNode) i.next();
                _writeNodeOutput(node);
        }

        prefixDec();

        if(_silent)
            _printStream.println(_prefix + "*/");
    }

    protected void _writeNodeOutput(CSDFNode node){
        /** do not process output node*/
        if(node.getOperation().toLowerCase().equals("write"))
            return;
        /** do not process nodes with empty output*/
        MemoryUnit mu = node.getMemoryUnit("output");
        if(mu==null)
            return;

        _printStream.println(_prefix + "std::cout<<\"" + node.getName() +" output \"<<std::endl;");

        _printStream.print(_prefix + "appFunc::show_val(");
        _printStream.print("&" + node.getName() +"_inst.output[0]");

        _printStream.print(", (" + node.getName() +"_inst.output_dim_0 ");

        for (int i=1;i<mu.getDimensionality();i++)
            _printStream.print(" * "+ node.getName() +"_inst.output_dim_" + i);

        _printStream.print("), show_values);");
        _printStream.println("");

    }
       /**TODO REPLACE BY DATA DISPLAY AFTER TEST*/
   protected void _simulateOutputDataDisplay(){
       _printStream.println(_prefix + "//print application output data");
       if(!_silent)
           _printStream.println(_prefix + "/**");
       _printStream.println(_prefix + "std::cout<<\"app output: \";");
       _printStream.println(_prefix + "for(int i=0;i<output_len;i++)");
       _prefixInc();
       _printStream.println(_prefix + " printf(\"%f \", app_output[i]);");
       _prefixDec();
       _printStream.println(_prefix + "printf(\"\\n\");");
       if(!_silent)
           _printStream.println(_prefix + "*/");
   }


    //////////////////         schedule              /////////////////////

    /** TODO: schedule is outdated (replaced by threads self-scheduling by data)
     * TODO: Remove after generator implementation is finished.
     //define schedule - LB: layers in traverse order
	   //NB : neurons of one layer can run in parallel?
     */
    protected void _writeSchedule(){
        _printStream.println(_prefix + "/**");
        prefixInc();
        _printStream.println(_prefix + "Define schedule ");
        _printStream.println(_prefix + "Dummy schedule for LB-mode: layers in traverse order");
        _printStream.println(_prefix + "Dummy schedule for NB-mode: neurons of one layer can run in parallel");
        prefixDec();
        _printStream.println(_prefix + "*/");
        _printStream.println(_prefix + "vector<std::string> schedule = vector<std::string>();");
        _printStream.println("");
        //
        if(_isScheduleNullOrEmpty())
            _processNullSchedule();

        else {
            for (String nodeCall : _schedule)
                _printStream.println(_prefix + "schedule.push_back(\"" + nodeCall + "\");");
        }

    }

    /**
     * Checks if schedule is null or empty
     * @return true, if schedule is null or empty and false otherwise
     */
    private boolean _isScheduleNullOrEmpty(){
        if(_schedule==null)
            return true;
        if(_schedule.size()==0)
            return true;
        return false;
    }

    /**
     * Process null schedule in generated .cpp - code
     */
    private void _processNullSchedule(){
        _printStream.println(_prefix + "//WARNING: Please, write CSDF graph schedule here using schedule.push_back(\"node_name\");");
        _printStream.println(_prefix + "cout<<\"WARNING: CSDF graph schedule is undefined.\"<<endl<<\"Please, write schedule manually in appMain.main()\";");
    }

    /**
     * Call dnn nodes according to schedule
     */
    protected void _callNodesInScheduleOrder(){
        _printStream.println(_prefix + "for(int i=0; i<schedule.size();i++){");
        prefixInc();
        _printStream.println(_prefix + "nodes[schedule.at(i)]->main();");
        prefixDec();
        _printStream.println(_prefix + "}");
    }

    //////////////////         communication channels             /////////////////////

      /**
     * define fifo channels sizs in ints from CSDF graph edges
     */
    protected void _defineFIFOChannels(CSDFGraph csdfg){
        _printStream.println(" ");
        _printStream.println(_prefix + "//Size of one token of FIFO in ints");
        _printStream.println(_prefix +  "int token_len=sizeof(long)/sizeof(int)+(sizeof(long)%sizeof(int)+(sizeof(int)-1))/sizeof(int);");
        _printStream.println(" ");
        _printStream.println(_prefix + "//fifo channels definition and initialization. Fifo sizes are given in tokens.");
        _printStream.println(_prefix +  "std::vector<fifo_buf> fifos = std::vector<fifo_buf>();");
        _printStream.println(" ");

         Iterator i = csdfg.getEdgeList().iterator();
        while (i.hasNext()) {
            CSDFEdge edge = (CSDFEdge) i.next();
            if(!(edge.getSrc().isOverlapHandler() ||edge.getDst().isOverlapHandler())) {
                _defineFIFOChannel(edge);
            }
        }
    }

    /**
     * Define bufferized FIFO channel
     * @param edge CSDF graph node
     */
    protected void _defineFIFOChannel(CSDFEdge edge){

        CSDFPort edgeSrc = edge.getSrc();
        CSDFPort edgeDst = edge.getDst();

        String srcFullName = edgeSrc.getNode().getName() + "_" +edgeSrc.getName();
        String dstFullName = edgeDst.getNode().getName() + "_" +edgeDst.getName();

        int fifoId = edge.getId();
        String fifoName  = "fifo_" + edge.getId();
        String fifoSize = "fifo_size_"+ fifoId;
        String firstElem = "2";
        if(_generateDNNFuncNA)
            firstElem = "FIRST_ELEM";


        //datatype should be the same for src/dst ports. If it is not defined, it is set to int by default.
        String dataType = getDataType(edgeSrc);

        _printStream.println(_prefix + "// FIFO " + srcFullName  + "-->" + dstFullName );


        _printStream.println(_prefix + "void *" + fifoName +"=NULL;");
        _printStream.println(_prefix + "int "+ fifoSize +
                " = max(" +
                edgeSrc.getNode().getName() + "_inst." + edgeSrc.getName() + "_fifo_size ," +
                edgeDst.getNode().getName() + "_inst." + edgeDst.getName() + "_fifo_size);");


      _printStream.println(_prefix + fifoName +" = calloc("+ fifoSize + " +" + firstElem + ", sizeof(" + dataType + "));");


      _printStream.println(_prefix + "struct fifo_buf buf_" + fifoId + " = fifo_buf ("+ fifoName +","
              + fifoSize + ", \"" + srcFullName + "\" , \"" + dstFullName + "\");");

      _printStream.println(_prefix + "fifos.push_back(buf_" + fifoId + ");");

      _printStream.println(" ");
    }

    //////////////////             threads             /////////////////////

    /** preparation work for pthreads on CPU */
    protected void _prepareThreads(){
     _printStream.println("");
     _printStream.println(_prefix + "// Allocate memory for pthread_create() arguments");
     _printStream.println(_prefix + "const int num_threads = total_nodes;");
     _printStream.println(_prefix + "struct thread_info *thread_info = (struct thread_info*)(calloc(num_threads, sizeof(struct thread_info)));");
     _printStream.println("");
     _printStream.println(_prefix + "// Main threads");
    }

      /**
     * Preapare information, necessary for each thread running
     * As each thread runs one graph node, the thread info
       * is filled for every node of the graph
     * @param csdfg CSDF graph
     */
     protected void _fillInThreadInfo(CSDFGraph csdfg){
        Iterator i = csdfg.getNodeList().iterator();
        int threadId = 0;
        while (i.hasNext()) {
            CSDFNode node = (CSDFNode) i.next();
            _fillInThreadInfo(node,threadId);
            threadId++;
        }

     }

    /**
     * Fill in thread info for one CSDF node
     * @param node CSFD node
     */
    protected void _fillInThreadInfo(CSDFNode node, int threadId){
        _printStream.println("");
        _printStream.println(_prefix + "//" + node.getName() + " thread info");
        String core = _assignCore(node.getName());

        _printStream.println(_prefix + "thread_info[" + threadId +"].core_id = " + core + ";");

         for(CSDFPort inport: node.getNonOverlapHandlingInPorts()){
             _fillInThreadInfo(inport,false);
         }

         for(CSDFPort outport: node.getNonOverlapHandlingOutPorts()){
             _fillInThreadInfo(outport,true);
         }
    }

    /** fill in thread info for one CSDF port
     * @param port CSDF port
     * @param isOutPort is port an output port
     */
    protected void _fillInThreadInfo(CSDFPort port, boolean isOutPort){
        int threadId = _schedule.indexOf(port.getNode().getName());

        String fifoBufSearchF = _funcClassName + "::get_buf_by_";
        if(isOutPort)
            fifoBufSearchF += "src";
        else fifoBufSearchF+= "dst";

        String fullPortName = port.getNode().getName() + "_" + port.getName();

        _printStream.println(_prefix + "fifo_buf* buf_ref_" + fullPortName + " = " + fifoBufSearchF + "( \"" + fullPortName + "\", fifos);");
        _printStream.println(_prefix + "thread_info[" + threadId + "].add_fifo_buf_ref( buf_ref_" + fullPortName + ");");
    }

    /**
     * Prepare and run fifo threads
     * @param csdfg CSDF graph
     */
     protected void _createAndRunThreads(CSDFGraph csdfg){

         Iterator i = csdfg.getNodeList().iterator();
         int threadId = 0;
        while (i.hasNext()) {
            CSDFNode node = (CSDFNode) i.next();
                String nodeName = node.getName();
                _createAndRunThread(threadId, "thread_" + nodeName, nodeName, nodeName);
                threadId++;
        }
     }

      /**
     * Create and run thread
     * @param threadId thread id
     * @param threadName thread Name
     * @param nodeClassName node class name
     * @param nodeName node name
     */
    protected void _createAndRunThread(int threadId, String threadName, String nodeClassName, String nodeName){
         _printStream.println(_prefix + "// Create and run " + threadName);
         _printStream.println(_prefix + "std::thread " + threadName +
                 "(&" + nodeClassName + "::main, &" + nodeName +"_inst" + ", &thread_info[" + threadId + "]);");

         if(_silent)
             _printStream.print("// ");
         _printStream.print(_prefix + "cout<<\"Joined with thread " + threadName + "\"" + "<<endl;");
         _printStream.println("");
         _printStream.println(_prefix + "//" + threadName + ".join();");
         _printStream.println("");
    }

    /**
     * Join all threads so application waits for all the
     * threads to be finished
     */
    protected void _joinThreads(){
        if(_schedule==null)
            return;
        _printStream.println(_prefix + "// Join threads that should be awaited");
        for(String node: _schedule){
            _printStream.println(_prefix + "thread_" + node + ".join();");
        }

    }


    //////////////////             application loop             /////////////////////
    /**
     * Open moc-loop, running the application in for-loop
     * @param applicationRepetitionsNum number of application repetitions in a for loop
     */
    protected void _openAppRepetitions(int applicationRepetitionsNum){
        _printStream.println("");
        _printStream.println(_prefix + "/**");
        _printStream.println(_prefix + "* This loop simulates the application running over the test data samples, ");
        _printStream.println(_prefix + "* until the \"ESC\" key is pressed");
        _printStream.println(_prefix + "* TODO: Replace this condition with your application running condition");
        _printStream.println(_prefix + "*/");
        _printStream.println(_prefix+"// while (true) {");
        _printStream.println(_prefix + "for(int inp_img =0; inp_img < " + applicationRepetitionsNum + "; inp_img++) {");
        prefixInc();
    }


    /** Close moc-loop, running the application in for-loop*/
    protected void _closeAppRepetitions(){
        prefixDec();
        _printStream.println(_prefix + "} // for all images");
        _printStream.println(_prefix + "//} // infinite while");
    }

     /**********************************************
     * Write main function for the operational node
     * @param y SDF Node
     **********************************************/
    @Override
    protected void _writeMain(CSDFNode y) {
        _printStream.println("void " + y.getName() + "::main(void *threadarg) {");
        _prefixInc();
        _accessCommunicationChannel(y);
        _printStream.println(_prefix + "// repetition parameters definition");
        _printStream.println(_prefix + "int q = " + y.getRepetitions() + ";");
        _printStream.println(_prefix + "int phase_len = " + y.getLength() + ";");
        _printStream.println(_prefix + "int phase; ");
         _printStream.println(" ");
        _printStream.println(_prefix + "// while (1) {");
        _prefixInc();
        _printStream.println(_prefix + "// loop over the repetitions number");
        _printStream.println(_prefix + "for (int rep = 0; rep < q ; rep ++) {");
        _prefixInc();
        _printStream.println(_prefix + "phase = rep % phase_len;");
        _processReading(y);
        _processExecution(y);
        _processWriting(y);
        _prefixDec();
        _printStream.println(_prefix + "}// loop over the phases");
        _prefixDec();
        if(_silent)
             _printStream.print("// ");
        _printStream.print(_prefix + "cout<<\" " + y.getName() + " finished! \"<<endl;");
        _printStream.println("");
        _printStream.println(_prefix + "//} while (1)");
        _prefixDec();
        _printStream.println(_prefix + "} // main");
    }

     /////////////////////////////////////////////////////////////////////
    /////////  process node execution phase              ////////////////


    /**
     * process exec function with default name
     * @param node CSDF node
     */
    @Override
    protected void _processExecution(CSDFNode node){
        /** input and output nodes are processed in appMain*/
        if(_isIONode(node))
            return;
        /** concat is processed separately too*/
        if(node.getName().toLowerCase().contains("concat")){
            _processExecutionConcat(node);
            return;
        }
        /** now we perform all neurons execution at once*/
        _processExecutionOneKernel(node,_funcClassName + "::execute");
    }
        /**
     * Checks if the CSDF graph node is input or output node
     * @return true, if CSDF graph node is input or output node and false otherwise
     */
    protected boolean _isIONode(CSDFNode node){
        if(node.getOperation().toLowerCase().equals("read")||node.getOperation().toLowerCase().equals("write"))
            return true;
        return false;
    }

    /**
     * TODO now it is assumed that concat is always performed over the dim_0 (depth) dimension
     * Process concatencation node execution
     * @param node CSDF concatenation node
     */
    protected void _processExecutionConcat(CSDFNode node){
        Vector<MemoryUnit> inputs = new Vector<>();

        for(CSDFPort inport: node.getNonOverlapHandlingInPorts()){
            MemoryUnit mu = inport.getAssignedMemory();
            if(mu!=null){
                inputs.add(mu);
            }
        }
        _printStream.println("");
        _printStream.println(_prefix + "int ptr_shift = 0;");

        for(MemoryUnit mu: inputs){
         _printStream.println(_prefix + "//add input " + mu.getName());
         _prefixInc();
         _printStream.println(_prefix + "for(int i=0; i<" + mu.getName() + "_len;i++)");

         _prefixInc();
         _printStream.println(_prefix + "output_ptr[i+ptr_shift] = " + mu.getName() + "_ptr[i];");
         _prefixDec();
         _printStream.println(_prefix + "ptr_shift += "+ mu.getName() + "_len;");
         _printStream.println("");
         _prefixDec();
        }

    }

      /**
     * Process execution, when kernels number == 1
     * @param node CSDF node
     * @param execPrimitiveName name of the execution primitive
     */
    protected void _processExecutionOneKernel(CSDFNode node,String execPrimitiveName){
      int opRepetitionsNum = node.getOperationRepetitionsNumber();
      if(opRepetitionsNum==0)
          return;
      String operation = node.getOperation();
      if(operation==null)
          return;

      _printStream.println("");
      _printStream.println(_prefix + "//execution");

      /** input*/
      CSDFPort inp = node.getNonOverlapHandlingInPorts().firstElement();
      MemoryUnit inp_mu = inp.getAssignedMemory();
      String inp_desc = _getLinearArrRef(inp_mu);

      /**weights*/
      MemoryUnit weights_mu = node.getMemoryUnit("weights");
      //if(weights_mu!=null)
      //  System.out.println(node.getName() + " weights_shape: "+weights_mu.getShape());
      String w_desc = _getLinearArrRef(weights_mu);

      /**output*/
      CSDFPort outp = node.getNonOverlapHandlingOutPorts().firstElement();
      MemoryUnit outp_mu = outp.getAssignedMemory();
      String outp_desc = _getLinearArrRef(outp_mu);

      /**bias*/
      String bias_desc = _getLinearArrRef(node.getMemoryUnit("bias"));

      /**pads*/
      MemoryUnit pads = node.getMemoryUnit("pads[4]");
      String pads_desc = _getPadsArrRef(pads);


      //exec primitive call, format : exec(input, weights, output, vector<int*>* int_params_ptr);
      _printStream.print(_prefix + execPrimitiveName +"(std::string(\"" + operation + "\"),");
      _printStream.print(inp_desc + ", ");
      _printStream.print(w_desc + ", ");
      _printStream.print(outp_desc + ", ");
      _printStream.print(bias_desc + ", ");
      _printStream.print(pads_desc + ", ");
      _printStream.println("&int_params);");
    }

    /**
     * Get reference on linear array
     * @param mu corresponding memory unit
     * @return reference on linear array
     */
    private String _getLinearArrRef(MemoryUnit mu){
      if(mu==null)
          return "nullptr";
      return "&" + mu.getName() +"[0]";
    }

     /**
     * Get reference on pads array
     * @param mu corresponding memory unit
     * @return reference on pads array
     */
    private String _getPadsArrRef(MemoryUnit mu){
      if(mu==null)
          return "nullptr";
      return "&pads[0]";
    }


    ////////////////////////////////////////////////////////////
    //////   Access to communication channels in the node  /////

    /**
     * TODO more fifos for general model
     * Create FIFO communication channel
     */
    protected void _accessCommunicationChannel(CSDFNode node){

        _printStream.println(_prefix + "// create communication channel");
        _printStream.println(_prefix + "thread_info *thread_data;");
        _printStream.println(_prefix + "thread_data = (struct thread_info *) threadarg;");

        for(CSDFPort inport:node.getNonOverlapHandlingInPorts())
            _accessCommunicationChannel(node.getName(),inport.getName(),false);

        for(CSDFPort outport:node.getNonOverlapHandlingOutPorts())
            _accessCommunicationChannel(node.getName(),outport.getName(),true);

        _printStream.println(_prefix + "setaffinity(thread_data->core_id);");
        _printStream.println(_prefix + "int_params[\"core_id\"] = thread_data->core_id;");
        _printStream.println(" ");
    }

    /**
     * Create FIFO communication channel
     */
    protected void _accessCommunicationChannel(String nodeName, String portName, boolean isOutPort){
        String getByPostfix = "dst";
        if(isOutPort)
            getByPostfix = "src";

        String bufName = nodeName + "_" + portName;
        _printStream.println(_prefix + "fifo_buf* " + bufName +
                "_buf_ptr = thread_data->get_fifo_buf_by_" + getByPostfix +"(\"" + bufName + "\");");
        _printStream.println(" ");
    }



    /**
     * TODO refactoring
     * Fill in CSDFNode integer parameters
     * @param node CSDFNode
     */
    protected void _fillInIntParams(CSDFNode node) {
        /** define constant parameters, if any*/
        Vector<MemoryUnit> constParams = node.getUnitParams();
        if (constParams.size() > 0) {
            _printStream.println("//const int parameters");
            _printStream.println(_prefix + "int_params[\"neurons\"] = neurons;");

            for (MemoryUnit mu : constParams) {
                if (mu.getTypeDesc().equals("int"))
                    _printStream.println(_prefix + "int_params[\"" + mu.getName() + "\"] = " + mu.getName() + ";");
            }
        }

        for (CSDFPort inport : node.getNonOverlapHandlingInPorts()) {
            MemoryUnit mu = inport.getAssignedMemory();
            if (mu != null) {
                _printStream.println(_prefix + "int_params[\"" + mu.getName() +
                        "_dims\"] = " + mu.getName() + "_dims;");
                for (int i = 0; i < mu.getDimensionality(); i++) {
                    _printStream.println(_prefix + "int_params[\"" + mu.getName() +
                            "_dim_" + i + "\"] = " + mu.getName() + "_dim_" + i + ";");
                }
                _printStream.println(_prefix + "int_params[\"" + mu.getName() +
                        "_len\"] = " + mu.getName() + "_len ;");
            }
        }

        /** define only distinct out ports*/
        Vector<String> defined = new Vector<>();
        for (CSDFPort outport : node.getNonOverlapHandlingOutPorts()) {
            MemoryUnit mu = outport.getAssignedMemory();
            if (mu != null) {
                if (!defined.contains(mu.getName())) {
                    _printStream.println(_prefix + "int_params[\"" + mu.getName() +
                            "_dims\"] = " + mu.getName() + "_dims;");
                    for (int i = 0; i < mu.getDimensionality(); i++) {
                        _printStream.println(_prefix + "int_params[\"" + mu.getName() +
                                "_dim_" + i + "\"] = " + mu.getName() + "_dim_" + i + ";");
                        defined.add(mu.getName());
                    }
                    _printStream.println(_prefix + "int_params[\"" + mu.getName() +
                            "_len\"] = " + mu.getName() + "_len ;");
                }
            }
        }
        defined.clear();
    }

    /**
     * Init array with dummy values (data mocs for debug purposes)
     * @param dims array dimensions
     * @param itName iterator name
     * @param arrName array name
     * @param dummyVal dummy value
     */
    private void _initArrDummyLinear(int dims, String itName, String arrName, Integer dummyVal){

        _printStream.println("");
        _printStream.println("// fill " + arrName + " with dummy values ");
        String arrDesc = arrName + "[" + itName + "]";

        String dimsTotal = arrName +"_dim_0";
        for(int i=1; i<dims; i++)
            dimsTotal+="* " + arrName + "_dim_"+ i;

        _printStream.println(_prefix + "for (int "+ itName + "= 0;" + itName +"<"+ dimsTotal + ";" + itName +"++)");
            prefixInc();

         _printStream.println(_prefix + arrDesc + " = " + dummyVal.toString() + "; ");

        prefixDec();
    }


     /**
     * Write FIFO sizes for CSDF node
     * @param node CSDF node
      * TODO check why min FIFO sizes (without repetitions) are not always applicable
     */
    protected void _writeFIFOsizes(CSDFNode node){
        _printStream.println(_prefix + "//assign FIFO sizes");
        for(CSDFPort inport: node.getNonOverlapHandlingInPorts())
            _writeFIFOsize(inport,node.getRepetitions());
        for(CSDFPort outport: node.getNonOverlapHandlingOutPorts())
            _writeFIFOsize(outport,node.getRepetitions());
        _printStream.println("");
    }


     /**
      * TODO fix min FIFO sizes
     * Write FIFO sizes for CSDF node
     * @param port CSDF port
     */
     protected void _writeFIFOsize(CSDFPort port, int repetitions){
      MemoryUnit mu = port.getAssignedMemory();
      if(mu==null) {
          _printStream.println(_prefix + port.getName() + "_fifo_size = 0;");
          return;
      }
      if(mu.getDimensionality()<1) {
          _printStream.println(_prefix + port.getName() + "_fifo_size = 0;");
          return;
      }
      _printStream.println(_prefix + port.getName() + "_fifo_size = "
           //   + (mu.getShape().getElementsNumber() + 10) + ";");
              + mu.getShape().getElementsNumber() * repetitions + ";"); //max size
           //     + mu.getShape().getElementsNumber()+";"); //min size
    }

     /**
     * Write FIFO sizes for CSDF node
     * @param port CSDF port
     */
     protected void _writeFIFOsize(CSDFPort port){
      MemoryUnit mu = port.getAssignedMemory();
      if(mu==null) {
          _printStream.println(_prefix + port.getName() + "_fifo_size = 0;");
          return;
      }
      if(mu.getDimensionality()<1) {
          _printStream.println(_prefix + port.getName() + "_fifo_size = 0;");
          return;
      }
      _printStream.println(_prefix + port.getName() + "_fifo_size = "
              + mu.getShape().getElementsNumber() + ";");
    }





   /////////////////////////////////////////////////////////////////////
   ///////// R/W operations /////////////////////////////////

    /**
     * process node input ports
     * @param node SDF Node
     */
    @Override
    protected void _processReading(CSDFNode node){
       _printStream.println("");
       _printStream.println(_prefix + "//reading");
        for (CSDFPort inport: node.getOverlapHandlingInPorts()){
          //  _definePhasesLimitation(inport, false);
            printReadTemplate(inport);
        }

        for (CSDFPort inport: node.getNonOverlapHandlingInPorts()){
            _definePhasesLimitation(inport, false);
            printReadTemplate(inport);
        }
    }

     /**
     * print reading template
     * @param port SDF input port
     */
     @Override
      public void printReadTemplate(CSDFPort port) {
        String arrayName = port.getAssignedMemoryName();
         if(port.isOverlapHandler()){
              _printInternalArrayShift(port,arrayName);
              return;
         }

         if(port.getStartTokens()==null)
             printOperationTemplate(port, "read" + _externalRWPostfix, arrayName);
         else
             printOperationShiftedTemplate(port,"read" + _externalRWPostfix,arrayName,
                     port.getName() + "_shift");
    }

    /**
    * process node output ports
    * @param node SDF Node
    * */
    @Override
    protected void _processWriting(CSDFNode node){
       _printStream.println("");
       _printStream.println(_prefix + "//writing");
        for (CSDFPort outport: node.getOutPorts()){
            if(!outport.isOverlapHandler())
                _definePhasesLimitation(outport, false);
            printWriteTemplate(outport);
        }
    }

    /**
     * Print write template
     * @param port CSDF port performs writing
     */
    public void printWriteTemplate(CSDFPort port) {
        String arrayName = port.getAssignedMemoryName();
        if(!port.isOverlapHandler())
            printOperationTemplate(port, "write" + _externalRWPostfix, arrayName);
    }

     /**
     * print reading/writing template for port,
     * taking into account only end border limitations
     * R/W primitive parameters:
     * 1. fifo: reference on destination array (name of CSDF node input port array) or FIFO
     * 2. memobj_cpu : reference on source array (default value, stored in header)
     * 3. len - number of tokens to be transferred
     * 4. fifo_size - total fifo size (const value, stored in header)
      *
      * write fifo --> local mem
      * read: local mem --> fifo
      * */
     @Override
    public void printOperationTemplate(CSDFPort port,String operation, String arrayName){
       if(port==null || arrayName==null)
           return;

       String portName = port.getName();
       String fifoRef = port.getNode().getName() + "_" + portName + "_buf_ptr";
        _printStream.println(" ");
        _printStream.println(_prefix + "// " + operation + " to " + arrayName);
                /** check, if there are any tokens to r/w*/
        _printStream.println(_prefix + "if ( " + portName + "_tokens > 0 )" );
        prefixInc();
        _printStream.println(_prefix + operation + "(" + fifoRef +"->fifo, " +
              _getLinearArrRef(port.getAssignedMemory()) + ", "+
                portName + "_tokens, " + fifoRef +"->fifo_size);");
        _prefixDec();
    }

     /**
     * print reading/writing template for port,
     * taking into account only end border limitations
     */
     @Override
    public void printOperationShiftedTemplate(CSDFPort port,String operation, String arrayName, String shiftDesc){
       if(port==null || arrayName==null)
           return;

       String portName = port.getName();
       String fifoRef = port.getNode().getName() + "_" + portName + "_buf_ptr";
       String portDataType = getDataType(port);
       port.getMemoryDim();

        _printStream.println(" ");
        _printStream.println(_prefix + "// " + operation + " to " + arrayName);
        /** check, if there are any tokens to r/w*/
        _printStream.println(_prefix + "if ( " + portName + "_tokens > 0 )" );
        prefixInc();
        _printStream.println(_prefix + operation + "(" + fifoRef +"->fifo, "+
                _getLinearArrRef(port.getAssignedMemory()) + " + " + portName + "_shift*sizeof("+
                portDataType + "), " + portName + "_tokens, " + fifoRef +"->fifo_size);");
        prefixDec();
    }

   /**
     * Get data type of CSDF port. If there is a memory unit, assigned to
     * a memory port, the memory unit data type is used. Otherwise,
     * int (integer) data type is used
     * @param port CSDF port
     * @return data type of CSDF port
     */
    private String getDataType(CSDFPort port){
        String dataType;
        MemoryUnit mu = port.getAssignedMemory();
        if(mu==null)
            dataType = "int";
        else
            dataType = mu.getTypeDesc();
        return dataType;
    }

    /**
     * print internal data shift for overlapping port memory,
     * taking into account only end border limitations
     *
     * C++ shift functions:
     * static void shift_2D(int h, int w, int *x, int stride);
     * appMain::shift_2D(h,w,&arr_to_shift_3D[0][0],stride);
     * static void shift_3D(int d, int h, int w, int *x, int stride);
     * appMain::shift_3D(d,h,w,&arr_to_shift_3D[0][0][0],stride);
     *
     */

    public void _printInternalArrayShift(CSDFPort port,String arrayName){
       if(port==null || arrayName==null)
           return;
        int dataDimensionality = port.getMemoryDim();
        if(dataDimensionality<2 || dataDimensionality>3)
            return;

        _printStream.println(" ");
        _printStream.println(_prefix + "// internal shift of " + arrayName);
        //shift 2D
        if(dataDimensionality==2)
                   _printStream.print(_prefix + _funcClassName + "::shift_2D(" + arrayName + "_dim_0," +
                arrayName + "_dim_1," + "&" + arrayName +"[0][0], stride);");
        //shift 3D
        else
            _printStream.print(_prefix + _funcClassName + "::shift_3D("+ arrayName + "_dim_0," +
                arrayName + "_dim_1," + arrayName + "_dim_2," + "&" + arrayName +"[0][0][0], stride);");

        _printStream.println("");
    }


    /************************************************************/
    /*******        appFunc class functions              *******/

    /** Execution primitives
     * @param incDnnFuncCPU if internal DNN operators lib is used for CPU
     * @param incDnnFuncGPU if internal DNN operators lib is used for GPU
     */
    protected void _writeExecPrimitives(boolean incDnnFuncCPU, boolean incDnnFuncGPU){
        _printStream.println(_prefix + "// Execution function primitive");
        prefixInc();
        _writeExecPrimitive();
        if(_generateDNNFuncNA)
            _writeExecPrimitiveNA();
        else
            _writeExecPrimitive(incDnnFuncCPU,incDnnFuncGPU);
        prefixDec();
    }

    /** Standard appFunc functions*/
    protected void _writeFunctions(){
        _printStream.println("");
        _writeTransposeFunction();
        _writeCommunicationMocs();
        prefixInc();
        _writeCPULine();
        _writeShiftFunctions();
        _writePrintFunctions();
        _writeGetBufFuncs();
        prefixDec();
    }

    /**
     * Communication mocs replace the execution with dummy function,
     * that transmit data from node input to node output
     * and allow to check if the communication between the nodes works
     */
    protected void _writeCommunicationMocs(){
     _printStream.println("");
     _printStream.println(_prefix + "/**");
     _printStream.println(_prefix + "* execution moc (for communication checkout) ");
     _printStream.println(_prefix + "* set output to (first_element_of_input + 1)");
     _printStream.println(_prefix + "* input -input data ptr");
     _printStream.println(_prefix + "* output - output data ptr");
     _printStream.println(_prefix + "* outp_len - output data length");
     _printStream.println(_prefix + "*/");
     _writeCommunicationMoc();
    }

    /** Print communication moc function*/
    protected void _writeCommunicationMoc(){
     prefixInc();
     _printStream.println(_prefix + "void appFunc::communication_moc("+ _IODatatype + "* input, "+ _IODatatype + "* output, int inp_len, int outp_len){");
     _printStream.println(_prefix+"int to_fill = std::min(inp_len, outp_len);");
     _printStream.println(_prefix + "for(int i=0;i<to_fill;i++)");
     prefixInc();
     _printStream.println(_prefix + "output[i] = input[i]+1;");
     prefixDec();
     prefixDec();
     _printStream.println(_prefix + "}");
    }

    /**
     * Print functions of getting buffer from vector of buffers
     */
    protected void _writeGetBufFuncs(){
        _writeGetBufFunc("src");
        _writeGetBufFunc("dst");
    }

     /**
     * Print functions of getting buffer from vector of buffers
     * @param bufPrefix source or destination buffer
     */
    protected void _writeGetBufFunc(String bufPrefix){
     _printStream.println("");
     _printStream.println(_prefix+"// get fifo buffer by " + bufPrefix);
     _printStream.println(_prefix + "fifo_buf* "+ _funcClassName +"::get_buf_by_" + bufPrefix +
     " (std::string name, std::vector<fifo_buf>& fifos){");
     prefixInc();
     _printStream.println(_prefix + "for (auto & fifos_elem: fifos)  {");
     prefixInc();
     _printStream.println(_prefix + "if (name.compare(fifos_elem." + bufPrefix + ") == 0)");
     prefixInc();
     _printStream.println(_prefix + "return &fifos_elem;");
     prefixDec();
     _printStream.println(_prefix + "}");
     _printStream.println(_prefix + "return nullptr;");
     prefixDec();
     prefixDec();
     _printStream.println(_prefix + "}");

    }


    /**
     * Write line copy function
     */
    protected void _writeCPULine(){
         _printStream.println(_prefix + "/**");
         prefixInc();
          _printStream.println(_prefix + "* copies 2D-data line from src to dst.");
          _printStream.println(_prefix + "* data_h - src data height");
          _printStream.println(_prefix + "* data_w - src data width");
          _printStream.println(_prefix + "* src - pointer to first data source array element");
          _printStream.println(_prefix + "* dst - pointer to first copy destination array element");
         prefixDec();
         _printStream.println(_prefix + "*/");

         _printStream.println(_prefix + " void " + _funcClassName + "::cpy_2D_data_line(const int &data_w, "+
                 _IODatatype + " *src,"+ _IODatatype + " *dst,"+" const int &line_id)");
         prefixInc();
         _printStream.println(_prefix + "{");
         prefixInc();
         _printStream.println(_prefix + "int line_start = line_id * data_w;");
         _printStream.println(_prefix + "for (int i = 0; i < data_w ; i++)");
         prefixInc();
         _printStream.println(_prefix + "dst[i] = src[line_start];");
         prefixDec();
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
    }


      /**
     * Write shift function (for shifting overlapping data in I/O arrays)"
     * TODO refactor shift function : make one for general arrays - tensors
     */
    protected void _writeTransposeFunction(){
        _printStream.println(_prefix + "/**");
        _printStream.println(_prefix + "Transpose matrix");
        _printStream.println(_prefix + "@param input : matrix to transpose");
        _printStream.println(_prefix + "@param inp_h : input matrix height");
        _printStream.println(_prefix + "@param inp_w : input matrix width");
        _printStream.println(_prefix + "*/");

         _printStream.println(_prefix + " void "+_funcClassName + "::transpose(" + _IODatatype + " *input, int inp_h, int inp_w) {");
         prefixInc();
        _printStream.println(_prefix + _IODatatype + " tmp[inp_w][inp_h] = {0};");
        _printStream.println(_prefix + "");
        _printStream.println(_prefix + "for(int j=0; j<inp_h; j++){");
        _prefixInc();
        _printStream.println(_prefix + "for(int i=0; i<inp_w; i++){");
        _prefixInc();
        _printStream.println(_prefix + "tmp[i][j] = *(input+i+j*inp_w);");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(_prefix + "");

        _printStream.println(_prefix + "for(int j=0; j<inp_h; j++){");
        _prefixInc();
        _printStream.println(_prefix + "for(int i=0; i<inp_w; i++){");
        _prefixInc();
        _printStream.println(_prefix + "*(input+j+i*inp_h) = tmp[i][j];");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(_prefix + "");
         prefixDec();
         _printStream.println(_prefix + "}");
         _printStream.println(_prefix + "");
    }


    /**
     * Write shift function (for shifting overlapping data in I/O arrays)"
     * TODO refactor shift function : make one for general arrays - tensors
     */
    protected void _writeShiftFunctions(){
        _printStream.println(_prefix + "/**");
        _printStream.println(_prefix + "Data shift functions (for shifting overlapping data in I/O arrays)");
        _printStream.println(_prefix + "@param array : I/O overlapping array");
        _printStream.println(_prefix + "@param dim   : I/O overlapping array dimensionality");
        _printStream.println(_prefix + "*/");

        _writeShift2D();
        _writeShift3D();
    }

    /**
     * Write 2D shift
     */
    protected void _writeShift2D(){
        _printStream.println(_prefix + "/**");
         prefixInc();
          _printStream.println(_prefix + "* Moves 2D data on n lines to top.");
          _printStream.println(_prefix + "* Required for overlapping data.");
          _printStream.println(_prefix + "* h - array height");
          _printStream.println(_prefix + "* w - array width");
          _printStream.println(_prefix + "* x - pointer to first array element");
       //   _printStream.println(_prefix + "* TODO: remove cout after testing");
         prefixDec();
         _printStream.println(_prefix + "*/");

         _printStream.println(_prefix + " void " + _funcClassName + "::shift_2D (const int &h, const int &w, "+
                 _IODatatype + " *x, const int &stride)");
         prefixInc();
         _printStream.println(_prefix + "{");
         prefixInc();
      //   _printStream.println(_prefix + "cout<<\"2D data shift\"<<endl;");
         _printStream.println(_prefix + "for(int line_ind = stride; line_ind < w ; line_ind ++){");
         prefixInc();
         _printStream.println(_prefix + "for(int i=0; i<w; i++)");
         prefixInc();
         _printStream.println(_prefix + "x[i + (line_ind - stride)* w] = x[i + line_ind * w];");
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
    }

    /**
     * Write 3D shift
     */
    protected void _writeShift3D(){
         _printStream.println(_prefix + "/**");
         prefixInc();
          _printStream.println(_prefix + "* Moves 3D data on n lines to top.");
          _printStream.println(_prefix + "* Required for overlapping data.");
          _printStream.println(_prefix + "* d - array depth");
          _printStream.println(_prefix + "* h - array height");
          _printStream.println(_prefix + "* w - array width");
          _printStream.println(_prefix + "* x - pointer to first array element");
      //    _printStream.println(_prefix + "* TODO: remove cout after testing");
         prefixDec();
         _printStream.println(_prefix + "*/");

         _printStream.println(_prefix + " void " + _funcClassName + "::shift_3D (const int &d, const int &h, const int &w, "+
                 _IODatatype + " *x, const int &stride)");
         prefixInc();
         _printStream.println(_prefix + "{");
         prefixInc();
       //  _printStream.println(_prefix + "cout<<\"3D data shift\"<<endl;");

         _printStream.println(_prefix + "int start_elem_id = 0;");
         _printStream.println(_prefix + "for(int depth=0; depth < d; depth++ ){");
         prefixInc();
         _printStream.println(_prefix + "for(int line_ind = stride; line_ind < w ; line_ind ++){");
         prefixInc();
         _printStream.println(_prefix + "for(int i=0; i<w; i++)");
         prefixInc();
         _printStream.println(_prefix + "x[i + (line_ind - stride)* w + start_elem_id] = x[i + line_ind * w + start_elem_id];");
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
         _printStream.println(_prefix + "start_elem_id +=w*h;");
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
         _printStream.println(_prefix + "}");
         prefixDec();
    }

    /** Write printout functions*/
    protected void _writePrintFunctions(){
        _writeShowVal();
        _writeprint2D();
        _writeprint3D();
    }

    protected void _writeShowVal(){
        _printStream.println(_prefix + "// function to show first num values of array");
        _printStream.println(_prefix + " void " + _funcClassName + "::show_val("+ _IODatatype + " *x, int xlen, int num){");
        prefixInc();
        _printStream.println(_prefix + "for (int i = 0; i < std::min(xlen,num); i++)");
        prefixInc();
        _printStream.println(_prefix + "std::cout << x[i] << ' ';");
        prefixDec();
        _printStream.println(_prefix + "std::cout << std::endl;");
        prefixDec();
        _printStream.println(_prefix + "}");

    }

    protected void _writeprint2D(){
        _printStream.println(_prefix + "//2D array print function, type: " + _IODatatype);
        _printStream.println(_prefix + " void " + _funcClassName + "::print_2D(const int &h, const int &w, "+ _IODatatype + " *x)");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println(_prefix + "for (int i = 0; i < h; i++){");
        prefixInc();
        _printStream.println(_prefix + "for (int j = 0; j < w ; j++)");
        prefixInc();
        _printStream.println(_prefix + "std::cout << x[i * w + j] << ' ';");
        prefixDec();
        _printStream.println(_prefix + "std::cout<<endl;");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }

    protected void _writeprint3D(){
        _printStream.println(_prefix + "//3D array print function, type: " + _IODatatype);
        _printStream.println(_prefix + " void " + _funcClassName + "::print_3D(const int &d, const int &h, const int &w, "+ _IODatatype + " *x)");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println(_prefix + "int start_elem_id = 0;");
        _printStream.println(_prefix + "for(int depth=0; depth < d; depth++ ){");
        prefixInc();
        _printStream.println(_prefix + "std::cout<<\"depth\"<<depth<<endl;");
        _printStream.println(_prefix + "for (int i = 0; i < h; i++){");
        prefixInc();
        _printStream.println(_prefix + "for (int j = 0; j < w ; j++)");
        prefixInc();
        _printStream.println(_prefix + "std::cout << x[i * w + j + start_elem_id] << ' ';");
        prefixDec();
        _printStream.println(_prefix + "std::cout<<endl;");
        prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(_prefix + "start_elem_id +=w*h;");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }

    /**
     * Write execution function primitive simplest MoC
     * (only function name is a parameter)
     */
    protected void _writeExecPrimitive(){
        _printStream.println("");
        _printStream.println(_prefix + "/**Write execution function primitive simplest MoC ");
        _printStream.println(_prefix + "(only function name is a parameter)*/ ");
        _printStream.println(_prefix + " void " + _funcClassName +
                    "::execute (std::string function)");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println(_prefix + "// cout<<function<<endl;");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }


     /**
     * Write execution function primitive MoC
     * (with a number of parameters, that can be used by DNN operators)
     * TODO extend or replace by DNN library
     */
    protected void _writeExecPrimitiveNA(){
        _printStream.println("");
        _printStream.println(_prefix + "/** TODO: place an API to your CNN operators library here*/");
        _printStream.println(_prefix + " void " + _funcClassName +
                    "::execute (std::string function," +
                _IODatatype +"* input, " + _paramDataType + "* weights, "
                + _IODatatype + "* output, "+ _paramDataType + "* bias, const int* pads, std::map<std::string,int>* int_params_ptr )");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println("");
        _printStream.println(_prefix + "bool op_done = false;");
        _printStream.println(_prefix + "int input_len = int_params_ptr->at(\"input_len\");");
        _printStream.println(_prefix + "int output_len = int_params_ptr->at(\"output_len\");");
        _printStream.println(_prefix + "int core_id = int_params_ptr->at(\"core_id\");" );
        _printStream.println("");

        //SOFTMAX
        _printStream.println(_prefix + "if (function.find(\"SOFTMAX\") != std::string::npos) {");
        prefixInc();
        _printStream.println(_prefix + "softmax(app_output, output_len, app_output);");
        _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //THN
         _printStream.println(_prefix + "if (function.find(\"THN\") != std::string::npos) {; }");

        //SIGM
        _printStream.println(_prefix + "if (function.find(\"SIGM\") != std::string::npos) {;} ");
        prefixInc();

        //ReLU
        _printStream.println(_prefix + "if (function.find(\"ReLU\") != std::string::npos) {");
        prefixInc();
        _printStream.println(_prefix + " relu(input, output, int_params_ptr);");
        _printStream.println(_prefix + " op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //CONV
        _printStream.println(_prefix + "if (function.find(\"CONV\") != std::string::npos) {");
        _prefixInc();
            _printStream.println(_prefix + "execute_conv(input, output, weights, bias, int_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //MAXPOOL
        _printStream.println(_prefix + "if (function.find(\"MAXPOOL\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "maxpool(input, output, int_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //AVGPOOL
        _printStream.println(_prefix + "if (function.find(\"AVGPOOL\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "//dnnFunc::avgpool_layer(input, output, int_params_ptr);");
            _printStream.println(_prefix + "//op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //LRN
        _printStream.println(_prefix + "if (function.find(\"AVGPOOL\") != std::string::npos) {");
        prefixInc();
            _printStream.println(_prefix + "//execut local response normalization operator ;");
            _printStream.println(_prefix + "//op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");

        //DENSEBLOCK
        _printStream.println(_prefix + "if (function.find(\"DENSEBLOCK\") != std::string::npos || function.find(\"MATMUL\") != std::string::npos ||function.find(\"GEMM\") != std::string::npos) {");
        prefixInc();
        _printStream.println(_prefix + "execute_dense_block(function, input, output, weights, bias, int_params_ptr); ");
        _printStream.println(_prefix + "op_done=true;");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println("");
        _printStream.println(_prefix + "/** TODO: the communication_moc is used for communications checkout.");
        _printStream.println(_prefix + "*   TODO: remove it when the real CNN operators library is used */");
        _printStream.println("");
        _printStream.println(_prefix + "if(!op_done){");
        _prefixInc();
        _printStream.println(_prefix + "communication_moc(input, output,input_len, output_len);");
        _printStream.print(_prefix);
         if(_silent)
           _printStream.print("//");
        _printStream.println("std::cout<<function<<\" op moc used\"<<std::endl;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }



    /**
     * Write execution function primitive MoC
     * (with a number of parameters, that can be used by DNN operators)
     * TODO extend or replace by DNN library
     */
    protected void _writeExecPrimitive(boolean incDnnFuncCPU, boolean incDnnFuncGPU){
        _printStream.println("");
        _printStream.println(_prefix + "/** TODO: place an API to your CNN operators library here*/");
        _printStream.println(_prefix + " void " + _funcClassName +
                    "::execute (std::string function," +
                _IODatatype +"* input, " + _paramDataType + "* weights, "
                + _IODatatype + "* output, "+ _paramDataType + "* bias, const int* pads, std::map<std::string,int>* int_params_ptr )");
        prefixInc();
        _printStream.println(_prefix + "{");
        prefixInc();
        _printStream.println("");
        _printStream.println(_prefix + "bool op_done = false;");
        _printStream.println(_prefix + "int input_len = int_params_ptr->at(\"input_len\");");
        _printStream.println(_prefix + "int output_len = int_params_ptr->at(\"output_len\");");
        _printStream.println(_prefix + "int core_id = int_params_ptr->at(\"core_id\");" );
        _printStream.println("");

        //SOFTMAX
        _printStream.println(_prefix + "if (function.find(\"SOFTMAX\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::softmax(input,input_len, output);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //THN
         _printStream.println(_prefix + "if (function.find(\"THN\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_thn(input, output, input_len);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //SIGM
        _printStream.println(_prefix + "if (function.find(\"SIGM\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_sigm(input, output, input_len);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //ReLU

        _printStream.println(_prefix + "if (function.find(\"ReLU\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_relu(input, output ,input_len);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //CONV
        _printStream.println(_prefix + "if (function.find(\"CONV\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_conv(input, weights, output, bias, pads, int_params_ptr,core_id);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        //POOL
        _printStream.println(_prefix + "if (function.find(\"POOL\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU) {
            _printStream.println(_prefix + "dnnFunc::execute_pool(function,input, output, bias, pads, int_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.println(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");

        _printStream.println(_prefix + "if (function.find(\"DENSEBLOCK\") != std::string::npos || function.find(\"MATMUL\") != std::string::npos ||function.find(\"GEMM\") != std::string::npos) {");
        prefixInc();
        if(incDnnFuncCPU || incDnnFuncGPU){
            _printStream.println(_prefix + "dnnFunc::execute_dense_block(function, input,weights, output, bias, int_params_ptr);");
            _printStream.println(_prefix + "op_done=true;");
        }
        else
            _printStream.print(_prefix + " ; ");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println("");
        _printStream.println(_prefix + "/** TODO: the communication_moc is used for communications checkout.");
        _printStream.println(_prefix + "*   TODO: remove it when the real CNN operators library is used */");
        _printStream.println("");
        _printStream.println(_prefix + "if(!op_done){");
        _prefixInc();
        _printStream.println(_prefix + "communication_moc(input, output,int_params_ptr->at(\"input_len\"), int_params_ptr->at(\"output_len\"));");
        _printStream.print(_prefix);
         if(_silent)
           _printStream.print("//");
        _printStream.println("std::cout<<function<<\" op moc used\"<<std::endl;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
        _printStream.println(_prefix + "}");
        prefixDec();
    }

    /***********************************************************/
    /**   getters and setters             *********************/

    /**
     * Set csdfg schedule
     * CSDFG nodes schedule : get from the repetition vector of CSDFG??
     * @param schedule CSDFG nodes schedule : get from the repetition vector of CSDFG??
     */
    public void setSchedule(Vector<String> schedule){
        _schedule = schedule;
    }

    /**
     * Set flag, if the debug couts should be printed
     * @param silent silent flag
     */
    public void setSilent(boolean silent) {
        CPPSDFGVisitorPthread._silent = silent;
    }

    ////////////////////////////////////////////////////////////////////
    ////                            Mapping                         ///

    /**
     * Find core id for the CSDF node
     * @param nodeName name of the CSDF node
     * @return core id for the CSDF node
     */
    protected String _assignCore(String nodeName){
        String coreId = null;

        if(_mapping!=null)
            coreId = _findCoreInMapping(nodeName);
        if(coreId!=null)
            return coreId;
        else
            return _assignFreeCore().toString();
    }

    /**
     * Find core id in the provided mapping
     * @return core id, found in the provided mapping
     */
    private String _findCoreInMapping(String nodeName){
        Vector processorList = _mapping.getProcessorList();
        int processorIndex = 0;

        for(Object mp: _mapping.getProcessorList()) {
            Vector processes = ((MProcessor)mp).getProcessList();
            Iterator i;

            i = processes.iterator();
            while (i.hasNext()) {
                MProcess process = (MProcess) i.next();
                if (process.getName().equals(nodeName)) {
		
                String strCoreId = processorList.get(processorIndex).toString();
                int sepInd = strCoreId.indexOf("_");
		String coreIdPart = strCoreId.substring(sepInd+1,strCoreId.length());
                
		if(strCoreId.contains("gpu"))
			return "9";
		else
			return coreIdPart;
                }
            }
        }
        System.err.println("Mapping core not found for CSDF node "+nodeName+" . Default core is assigned");
        return null;
    }


    /**
     * get next free core id (Moc in case the mapping is not provided)
     * @return next free core id
     */
    private Integer _assignFreeCore(){
        if(_curCore>=(_maxCores-1))
            _curCore=0;
        else
            _curCore++;
        return _curCore;
    }



    /**
     * Set maximum cores number
     * @param maxCores maximum cores number
     * TODO should be replaced by mapping specification
     */
    public void setMaxCores(int maxCores) {
        CPPSDFGVisitorPthread._maxCores = maxCores;
    }


    /**
     * Set mapping for he CSDF graph
     * @param mapping mapping for the CSDF graph
     */
    public void setMapping(Mapping mapping) {
        this._mapping = mapping;
    }

    ///////////////////////////////////////////////////////////////////
    ///           internal DNN library templates                   ///


 /**
     * Generate DNN functions class with our test CNN functions
     * @param dir directory to generate class file
     */
    public void generateDnnFuncClassTemplate(String dir, boolean printGPU){
        try {
            _printStream = FileWorker.openFile(dir,"dnnFunc" , "cpp");
            _printStream.println("// File automatically generated by ESPAM");

            _printStream.println("#include \"dnnFunc.h\"");
            _printStream.println("#include <iostream>");
            _printStream.println("#include <math.h>");
            _printStream.println("#include \"appFunc.h\"");
            _printStream.println("using namespace std;");

            if(printGPU) {
                _printStream.println("");
                _printStream.println("extern void kernelhost_2d(" +  _IODatatype + " *input, " + _paramDataType + " *weights, " +  _IODatatype + " *output, int input_h, int input_w,");
                _printStream.println("\t\tint output_h,int output_w, int k_h, int k_w, int stride);");
                _printStream.println("");
                _printStream.println("extern void kernelhost_3d(" + _IODatatype + " *input, " + _paramDataType + " *weights, " +_IODatatype + " *output, int channels, int input_h, int input_w,");
                _printStream.println("\t\tint output_h,int output_w, int k_h, int k_w, int stride);");
            }

            _printStream.println("");
            _printStream.println("dnnFunc::dnnFunc() {");
            _printStream.println("");
            _printStream.println("}");

            _printStream.println("");
            _printStream.println("dnnFunc::~dnnFunc() {");
            _printStream.println("");
            _printStream.println("}");

            _printExecuteDenseBlock();
            _printExecuteConv(printGPU);

            if(printGPU) {
                //generate dnnFunc::convolution_2d_inp_gpu function
                _printStream.println("void dnnFunc::convolution_2d_inp_gpu(int *input, int *weights, int *output, int input_h, int input_w,");
                _printStream.println("            int output_h, int output_w, int k_h, int k_w, int stride){");
                _printStream.println("");
                _printStream.println("      kernelhost_2d(input, weights, output, input_h, input_w, output_h, output_w, k_h, k_w, stride);");
                _printStream.println("");
                _printStream.println("}");
                _printStream.println("");

                //generate dnnFunc::convolution_3d_inp_gpu function
                _printStream.println("void dnnFunc::convolution_3d_inp_gpu(int *input, int *weights, int *output, int channels, int input_h, int input_w,");
                _printStream.println("          int output_h,int output_w, int k_h, int k_w, int stride){");
                _printStream.println("");
                _printStream.println("      kernelhost_3d(input, weights, output, channels, input_h, input_w, output_h, output_w, k_h, k_w, stride);");
                _printStream.println("");
                _printStream.println("}");
                _printStream.println("");
            }

            _printConv3DInpCPU();
            _printWeightAndSum();

            //generate dnnFunc::softmax function
            _printStream.println("void dnnFunc::softmax( "+_IODatatype +" *non_activated_stages, int len)");
            _printStream.println("{");
            _printStream.println("\tint softmax_summ = dnnFunc::get_softmax_summ(non_activated_stages, len);");
            _printStream.println("\tint id=0;");
            _printStream.println("\t" + _IODatatype + " *sm_elem;");
            _printStream.println("\tfor(int i=0; i<len; i++)");
            _printStream.println("\t{");
            _printStream.println("\t\tsm_elem = non_activated_stages + i;");
            _printStream.println("\t\t*(sm_elem)/=softmax_summ;");
            _printStream.println("\t}");
            _printStream.println("}");
            _printStream.println("");

            //generate dnnFunc::get_softmax_summ function
            _printStream.println("float dnnFunc::get_softmax_summ(" + _IODatatype + " *non_activated_stages, int len)");
            _printStream.println("{");
            _printStream.println("\tfloat softmax_summ = 0;");
            _printStream.println("\tfor( int i=0;i<len;i++)");
            _printStream.println("\t\tsoftmax_summ+=exp( *(non_activated_stages + i));");
            _printStream.println("\treturn softmax_summ;");
            _printStream.println("}");
            _printStream.println("");

            //generate dnnFunc::init_zeros function
            _printStream.println("void dnnFunc::init_zeros(" + _IODatatype + " *matrix, int h, int w){");
            _printStream.println("\tfor (int j = 0; j < h; j++){");
            _printStream.println("\t\tfor (int i = 0; i < w; i++){");
            _printStream.println("\t\t\tmatrix[i * w + j]=0;");
            _printStream.println("\t\t}");
            _printStream.println("\t}}");
            _printStream.println("");

            //generate dnnFunc::init_zeros function
            _printStream.println("void dnnFunc::init_zeros(" + _IODatatype + " *matrix, int d, int h, int w){");
            _printStream.println("\tfor (int k = 0; k < d; k++){");
            _printStream.println("\t\tfor (int j = 0; j < h; j++){");
            _printStream.println("\t\t\tfor (int i = 0; i < w; i++){");
            _printStream.println("\t\t\t\tmatrix[i * w + j]=0;");
            _printStream.println("\t\t\t\t}");
            _printStream.println("\t}\t\t}");
            _printStream.println("}");
            _printStream.println("");



            /**
            _writeMainClassCPPBeginning();
            _writeNoBaseCppConstructorAndDestructor(_mainClassName);
            _writeMainClassMain(sdfg);*/

            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + _mainClassName + " " + e.getMessage());
        }
    }

    /** Print execute Dense Block (GEMM/MATMUL) function*/
    private void _printExecuteDenseBlock(){
        _printStream.println("");
        _printStream.println("/**");
        _printStream.println(" * Execute dense function (MATMUL/GEMM)");
        _printStream.println(" * TODO extend by any NonLinear function, not only softmax");
        _printStream.println("*/");
        _printStream.println("void dnnFunc::execute_dense_block (std::string function," +_IODatatype + " *input, " +_IODatatype + " *weights, "
                    +_paramDataType + " *output," + _paramDataType + " *bias, std::map<std::string,int>* int_params_ptr ){");
        _prefixInc();
        _printStream.println(_prefix + "int input_len = int_params_ptr->at(\"input_len\");");
        _printStream.println(_prefix + "int output_len = int_params_ptr->at(\"output_len\");");

        _printStream.println("");
        _printStream.println(_prefix + "if (function.find(\"SOFTMAX\") != std::string::npos)");
        _prefixInc();
        _printStream.println(_prefix + "dnnFunc::softmax(input, input_len);");
        _prefixDec();
        _printStream.println(_prefix + "//MATMUL");
        _printStream.println(_prefix + "if(bias==nullptr)");
        _prefixInc();
        _printStream.println(_prefix + "dnnFunc::weight_and_sum(input, weights, output, input_len, output_len);");
        _prefixDec();
        _printStream.println(_prefix + "//GEMM");
        _printStream.println(_prefix + "else");
        _prefixInc();
        _printStream.println(_prefix + "dnnFunc::weight_and_sum(input, weights, output, bias, input_len, output_len);");
        _prefixDec();
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println("");
    }

    /**
     * Print execute convolution function
     * @param printGPU if the functions for GPU should be printed
     */
    private void _printExecuteConv(boolean printGPU){
            //generate dnnFunc::execute_conv function
            _printStream.println("/**");
            _printStream.println(" * Execute convolutional function");
            _printStream.println(" * TODO: optimize, support nD input tensors");
            _printStream.println(" */");
            _printStream.println("void dnnFunc::execute_conv (" + _IODatatype + " *input, " +
                    _paramDataType + " *weights, " + _IODatatype + " *output," + _paramDataType + " *bias, " +
                    " std::map<std::string,int>* int_params_ptr, int core_id)");
            _printStream.println("    {");
            _prefixInc();
        //    _printStream.println("cout<<\" core id is: \"<< core_id <<endl;");
            _printStream.println(_prefix + "int stride=int_params_ptr->at(\"stride\");");
            _printStream.println(_prefix + "int k_h = int_params_ptr->at(\"k_h\");");
            _printStream.println(_prefix + "int k_w = int_params_ptr->at(\"k_w\");");
            _printStream.println(_prefix + "int neurons = int_params_ptr->at(\"neurons\");");
            _printStream.println("");
            _printStream.println(_prefix + "int channels = 1;");
            _printStream.println(_prefix + "int input_h = int_params_ptr->at(\"input_dim_0\");");
            _printStream.println(_prefix + "int input_w = int_params_ptr->at(\"input_dim_1\");");
            _printStream.println("");
            _printStream.println(_prefix + "int output_d = 1;");
            _printStream.println(_prefix + "int output_h = int_params_ptr->at(\"output_dim_0\");");
            _printStream.println(_prefix + "int output_w =  int_params_ptr->at(\"output_dim_1\");");
            _printStream.println("");
            _printStream.println(_prefix+"if(int_params_ptr->at(\"input_dims\")==3){");
            _prefixInc();
            _printStream.println(_prefix + "channels = int_params_ptr->at(\"input_dim_0\"); ");
            _printStream.println(_prefix + "input_h = int_params_ptr->at(\"input_dim_1\");");
            _printStream.println(_prefix + "input_w = int_params_ptr->at(\"input_dim_2\");");
            _prefixDec();
            _printStream.println(_prefix + "}");

            _printStream.println("");
            _printStream.println(_prefix+"if(int_params_ptr->at(\"output_dims\")==3){");
            _prefixInc();
            _printStream.println(_prefix + "output_d = int_params_ptr->at(\"output_dim_0\"); ");
            _printStream.println(_prefix + "output_h = int_params_ptr->at(\"output_dim_1\");");
            _printStream.println(_prefix + "output_w = int_params_ptr->at(\"output_dim_2\");");
            _prefixDec();
            _printStream.println(_prefix + "}");
            _printStream.println("");

            if(printGPU)
                _printStream.println(_prefix + "if(core_id < 9)");

            else {_printStream.print(_prefix);}

            _printStream.print(_prefix + "dnnFunc::convolution_3d_inp_cpu(input, weights, output, bias, channels,input_h, input_w, output_d, output_h, output_w, k_h, k_w, stride);");
            _printStream.println("");

            if(printGPU)
                _printStream.println(_prefix + "else { dnnFunc::convolution_3d_inp_gpu(input,weights,output, channels,input_h, input_w, output_h, output_w, k_h, k_w, stride);}");

            _prefixDec();
            _printStream.println(_prefix + "}");
            _printStream.println("");
    }

    public void _printWeightAndSum(){

            _printStream.println(_prefix + "void dnnFunc::weight_and_sum(" +_IODatatype + " *input, " +
                    _paramDataType + " *weights, " + _IODatatype + " *result, " + _paramDataType +
                    " *bias, int input_len, int output_len){");
            _prefixInc();
            _printStream.println(_prefix + "for (int j=0; j<output_len; j++){");
            _prefixInc();
            _printStream.println(_prefix + "*(result+j) = *(bias+j);");
            _printStream.println(_prefix + "for(int i=0; i<input_len; i++)");
            _prefixInc();
            _printStream.println(_prefix + "*(result+j) +=*(input + i) * *(weights + i + j*input_len);");
            _prefixDec();
            _prefixDec();
            _printStream.println("}");
            _prefixDec();
            _printStream.println("}");
            _printStream.println("");

            _printStream.println(_prefix + "void dnnFunc::weight_and_sum(" +_IODatatype + " *input, " + _paramDataType + " *weights, " + _IODatatype + " *result, int input_len, int output_len){");
            _prefixInc();
            _printStream.println(_prefix + "for (int j=0; j<output_len; j++){");
            _prefixInc();
            _printStream.println(_prefix + "*(result+j) = 0;");
            _printStream.println(_prefix + "for(int i=0; i<input_len; i++)");
            _prefixInc();
            _printStream.println(_prefix + "*(result+j) +=*(input + i) * *(weights + i + j*input_len);");
            _prefixDec();
            _prefixDec();
            _printStream.println("}");
            _prefixDec();
            _printStream.println("}");
            _printStream.println("");

            _printStream.println(_prefix + "void dnnFunc::weight_and_sum(" +_IODatatype + " *input, " +_paramDataType + " *weights, " + _IODatatype + " *result, int input_len){");
            _prefixInc();
            _printStream.println(_prefix + "*result = 0;");
            _printStream.println("for(int i=0; i<input_len; i++)");
            _prefixInc();
            _printStream.println("*result +=*(input+i) * *(weights+i);");
            _prefixDec();
            _prefixDec();
            _printStream.println("}");
            _printStream.println("");

    }

    /** Print convolutional for 3D input (CPU)*/
    public void _printConv3DInpCPU(){
         _printStream.println("/** convolution for CPU function*/");
            _printStream.println("void dnnFunc::convolution_3d_inp_cpu(" + _IODatatype + " *input, " + _paramDataType + " *weights, "
                    +_IODatatype + " *output, " + _paramDataType + "* bias, int channels, int input_h, int input_w," +
            "int output_d, int output_h,int output_w, int k_h, int k_w, int stride){");

            _printStream.println("");
            _prefixInc();
            _printStream.println(_prefix + "//init output");
            _printStream.println(_prefix + "if(bias==nullptr){");
            prefixInc();
            _printStream.println(_prefix + "for (int i = 0; i < output_h * output_w * output_d; i++)");
            prefixInc();
            _printStream.println(_prefix + "output[i] = 0;");
            prefixDec();
            _printStream.println(_prefix + "}");
            prefixDec();
            _printStream.println(_prefix + "else {");
            prefixInc();
            _printStream.println(_prefix + _IODatatype +" bias_val;  ");
            _printStream.println(_prefix + "for (int d = 0; d < output_d; d++) {");
            prefixInc();
            _printStream.println(_prefix + "bias_val = bias[d];");
            _printStream.println(_prefix + "for (int i = 0; i < output_h * output_w; i++)");
            prefixInc();
            _printStream.println(_prefix + "output[i + d * output_h * output_w] = bias_val;");
            prefixDec();
            _printStream.println(_prefix + "}");
            prefixDec();
            _printStream.println(_prefix + "}");
            prefixDec();

            _printStream.println(_prefix + _IODatatype + " fold_cell = 0;");
            _printStream.println(_prefix + "int input_elem_ind = 0;");
            _printStream.println(_prefix + "int k_elem_ind = 0;");
            _printStream.println(_prefix + "int outp_elem_ind =0;");
            _printStream.println("");

        _printStream.println(_prefix + _IODatatype + " *sub_input;");
        _printStream.println(_prefix + _IODatatype + " *sub_output;");
        _printStream.println(_prefix + _IODatatype + " *sub_kernel;");
        _printStream.println("");
        _printStream.println(_prefix + "for (int ofm=0; ofm<output_d;ofm++){");
        _prefixInc();
        _printStream.println(_prefix + "sub_output = output + output_h * output_w * ofm;");
        _printStream.println(_prefix + "sub_kernel = weights + k_h * k_w * ofm;");
        _printStream.println("");
        _printStream.println(_prefix + "for (int ifm=0; ifm<channels;ifm++){");
        _prefixInc();
        _printStream.println(_prefix + "sub_input = input + input_w * input_h * ifm;");
        _prefixInc();
        _printStream.println(_prefix + "for (int j = 0; j < output_h; j+=stride){");
        _prefixInc();
        _printStream.println(_prefix + "for (int i = 0; i < output_w; i+=stride){");
            _prefixInc();
        _printStream.println(_prefix + "fold_cell = 0;");
        _printStream.println(_prefix + " //summ k,l");
        _printStream.println(_prefix + "for (int l = 0; l < k_h; l++){");
        _prefixInc();
        _printStream.println(_prefix + "for (int k = 0; k < k_w; k++){");
        _prefixInc();
        _printStream.println(_prefix + "k_elem_ind = l + k * k_h;");
        _printStream.println(_prefix + "input_elem_ind = (j+l) + (i+k)* input_w;");
        _printStream.println(_prefix + "fold_cell += *(input + input_elem_ind) * *(sub_kernel + k_elem_ind);");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println(_prefix + "outp_elem_ind = i * output_w + j;");
        _printStream.println(_prefix + "sub_output[outp_elem_ind] += fold_cell;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _prefixDec();
        _printStream.println(_prefix + "");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _prefixDec();
        _printStream.println(_prefix + "}");

        _prefixDec();
        _prefixDec();
    }

    /**
     * Generate cuda kernel class for convolutions
     * @param dir directory to generate class file
     */

    public void generateKernelClassTemplate(String dir){
        try {
            _printStream = FileWorker.openFile(dir, "kernel", "cu");
            _printStream.println("// File automatically generated by ESPAM");
            _printStream.println("");
            _printStream.println("#include <stdio.h>");
            _printStream.println("#include <cuda.h>");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("using namespace std;");
            _printStream.println("");
            _printStream.println("__global__ void kernel_2d(" +_IODatatype + " *input, " +_paramDataType + " *weights, " +_IODatatype + " *output, int input_h, int input_w,");
            _printStream.println("\t\tint output_h, int output_w, int k_h, int k_w, int stride) {");
            _printStream.println("");
            _printStream.println(" \t\t" + _IODatatype + " fold_cell = 0;");
            _printStream.println("\t\tint input_elem_ind = 0;");
            _printStream.println("\t\tint k_elem_ind = 0;");
            _printStream.println("\t\tint outp_elem_ind =0;");
            _printStream.println("");
            _printStream.println("     // fill in output matrix");
            _printStream.println("\t\t   for (int j = 0; j < output_h; j+=stride)");
            _printStream.println("\t\t   {");
            _printStream.println("\t\t\t   for (int i = 0; i < output_w; i+=stride)");
            _printStream.println("\t\t\t   {");
            _printStream.println("\t\t\t\t   //summ k,l");
            _printStream.println("\t\t\t\t   for (int l = 0; l < k_h; l++)");
            _printStream.println("\t\t\t\t   {");
            _printStream.println("");
            _printStream.println("\t\t\t\t\t   for (int k = 0; k < k_w; k++){");
            _printStream.println("");
            _printStream.println("\t\t\t           k_elem_ind = l + k * k_h;");
            _printStream.println("");
            _printStream.println("\t\t\t           input_elem_ind = (j+l) + (i+k)* input_w;");
            _printStream.println("\t\t\t           fold_cell += *(input + input_elem_ind) * *(weights + k_elem_ind);");
            _printStream.println("\t\t\t\t\t   }");
            _printStream.println("\t\t\t\t   }");
            _printStream.println("\t\t\t\t   outp_elem_ind = i * output_w + j;");
            _printStream.println("\t\t\t       output[outp_elem_ind] += fold_cell;");
            _printStream.println("\t\t\t       fold_cell = 0;");
            _printStream.println("\t\t\t   }");
            _printStream.println("\t\t   }");
            _printStream.println("}");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("__global__ void kernel_3d(" +_IODatatype + " *input, " +_paramDataType + " *weights, " +_IODatatype + " *output, int channels, int input_h, int input_w,");
            _printStream.println("\t\tint output_h, int output_w, int k_h, int k_w, int stride) {");
            _printStream.println("");
            _printStream.println("");
            _printStream.println(" \t\t" + _IODatatype + " fold_cell = 0;");
            _printStream.println("\t\tint input_elem_ind = 0;");
            _printStream.println("\t\tint k_elem_ind = 0;");
            _printStream.println("\t\tint outp_elem_ind =0;");
            _printStream.println("");
            _printStream.println("\t" + _IODatatype + " *sub_input;");
            _printStream.println("");
            _printStream.println("\tfor (int d=0; d<channels;d++){");
            _printStream.println("\t\tsub_input = input + input_w * input_h * d;");
            _printStream.println("       // cout<<sub_input<<sub_input<<endl;");
            _printStream.println("\t\t   for (int j = 0; j < output_h; j+=stride)");
            _printStream.println("\t\t   {");
            _printStream.println("\t\t\t   for (int i = 0; i < output_w; i+=stride)");
            _printStream.println("\t\t\t   {");
            _printStream.println("\t\t\t\t   //summ k,l");
            _printStream.println("\t\t\t\t   for (int l = 0; l < k_h; l++)");
            _printStream.println("\t\t\t\t   {");
            _printStream.println("");
            _printStream.println("\t\t\t\t\t   for (int k = 0; k < k_w; k++){");
            _printStream.println("");
            _printStream.println("\t\t\t           k_elem_ind = l + k * k_h;");
            _printStream.println("");
            _printStream.println("\t\t\t           input_elem_ind = (j+l) + (i+k)* input_w;");
            _printStream.println("\t\t\t           fold_cell += *(input + input_elem_ind) * *(weights + k_elem_ind);");
            _printStream.println("\t\t\t\t\t   }");
            _printStream.println("\t\t\t\t   }");
            _printStream.println("\t\t\t\t   outp_elem_ind = i * output_w + j;");
            _printStream.println("\t\t\t       output[outp_elem_ind] += fold_cell;");
            _printStream.println("\t\t\t       fold_cell = 0;");
            _printStream.println("\t\t\t   }");
            _printStream.println("\t\t   }");
            _printStream.println("}");
            _printStream.println("}");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("void kernelhost_2d(" +_IODatatype + " *input, " +_paramDataType + " *weights, " +_IODatatype + " *output, int input_h, int input_w,");
            _printStream.println("\t\tint output_h, int output_w, int k_h, int k_w, int stride){");
            _printStream.println("");
            _printStream.println("\t\t//clean output");
            _printStream.println("\t\tfor (int j = 0; j < output_h; j++){");
            _printStream.println("\t\t\tfor (int i = 0; i < output_w; i++){");
            _printStream.println("\t\t\t\toutput[i * output_w + j]=0;");
            _printStream.println("\t\t\t\t}");
            _printStream.println("\t          }");
            _printStream.println("");
            _printStream.println("        //allocate memory");
            _printStream.println("\t\tint N_input = input_h * input_w;");
            _printStream.println("\t\tint N_weights = k_h * k_w;");
            _printStream.println("\t\tint N_output = output_h * output_w;");
            _printStream.println("");
            _printStream.println("        int *d_input, *d_weights, *d_output;");
            _printStream.println("\t\tcudaMalloc(&d_input, N_input*sizeof(int));");
            _printStream.println("\t\tcudaMalloc(&d_weights, N_weights*sizeof(int));");
            _printStream.println("\t\tcudaMalloc(&d_output, N_output*sizeof(int));");
            _printStream.println("");
            _printStream.println("\t\tcudaMemcpy(d_input, input, N_input*sizeof(int), cudaMemcpyHostToDevice);");
            _printStream.println("\t\tcudaMemcpy(d_weights, weights, N_weights*sizeof(int), cudaMemcpyHostToDevice);");
            _printStream.println("\t\tcudaMemcpy(d_output, output, N_output*sizeof(int), cudaMemcpyHostToDevice);");
            _printStream.println("");
            _printStream.println("\t\tkernel_2d<<<256,256>>>(d_input, d_weights, d_output, input_h, input_w, output_h, output_w, k_h, k_w, stride);");
            _printStream.println("");
            _printStream.println("\t\tcudaMemcpy(output, d_output, N_output*sizeof(int), cudaMemcpyDeviceToHost);");
            _printStream.println("");
            _printStream.println("\t\tcudaFree(d_input);");
            _printStream.println("\t\tcudaFree(d_weights);");
            _printStream.println("\t\tcudaFree(d_output);");
            _printStream.println("}");
            _printStream.println("");
            _printStream.println("");
            _printStream.println("void kernelhost_3d(" +_IODatatype + " *input, " +_paramDataType + " *weights, " +_IODatatype + " *output, int channels, int input_h, int input_w,");
            _printStream.println("\t\tint output_h, int output_w, int k_h, int k_w, int stride){");

            _printStream.println("\t\t//clean output");
            _printStream.println("\t\tfor (int j = 0; j < output_h; j++){");
            _printStream.println("\t\t\tfor (int i = 0; i < output_w; i++){");
            _printStream.println("\t\t\t\toutput[i * output_w + j]=0;");
            _printStream.println("\t\t\t\t}");
            _printStream.println("\t          }");
            _printStream.println("");
            _printStream.println("        //allocate memory");
            _printStream.println("\t\tint N_input = input_h * input_w;");
            _printStream.println("\t\tint N_weights = k_h * k_w;");
            _printStream.println("\t\tint N_output = output_h * output_w;");
            _printStream.println("");
            _printStream.println("        int *d_input, *d_weights, *d_output;");
            _printStream.println("\t\tcudaMalloc(&d_input, N_input*sizeof(int));");
            _printStream.println("\t\tcudaMalloc(&d_weights, N_weights*sizeof(int));");
            _printStream.println("\t\tcudaMalloc(&d_output, N_output*sizeof(int));");
            _printStream.println("");
            _printStream.println("\t\tcudaMemcpy(d_input, input, N_input*sizeof(int), cudaMemcpyHostToDevice);");
            _printStream.println("\t\tcudaMemcpy(d_weights, weights, N_weights*sizeof(int), cudaMemcpyHostToDevice);");
            _printStream.println("\t\tcudaMemcpy(d_output, output, N_output*sizeof(int), cudaMemcpyHostToDevice);");
            _printStream.println("");
            _printStream.println("\t\tkernel_3d<<<256,256>>>(d_input, d_weights, d_output, channels, input_h, input_w, output_h, output_w, k_h, k_w, stride);");
            _printStream.println("");
            _printStream.println("\t\tcudaMemcpy(output, d_output, N_output*sizeof(int), cudaMemcpyDeviceToHost);");
            _printStream.println("");
            _printStream.println("\t\tcudaFree(d_input);");
            _printStream.println("\t\tcudaFree(d_weights);");
            _printStream.println("\t\tcudaFree(d_output);");
            _printStream.println("}");
            _printStream.println("");
            _printStream.close();
        }
        catch (Exception e){
            System.err.println(".cpp file creation error for " + "run" + " " + e.getMessage());
        }
    }
    /** Generate function that loads weights from external numpy files*/
    private void _generateDataLoadFunc(){
       _printStream.println(_prefix + "void dataLoader::data_load_from_numpy(std::string srcpathprefix, int partition_id, int partition_size, int start, int shift, " + _IODatatype + "* dst) {");
       prefixInc();
       _printStream.println(_prefix + "/** form the source file path*/");
       _generateDataPartitionPath();
       _printStream.println("");
       _printStream.println(_prefix + "/** load  source file into a new array */");
       _printStream.println(_prefix + "cnpy::NpyArray arr = cnpy::npy_load(srcfile);");
       _printStream.println(_prefix + _IODatatype +"* loaded_data = arr.data<" + _IODatatype +">();");
       _printStream.println("");
       _printStream.println(_prefix + "/** copy loaded array to destination array */");
       _printStream.println(_prefix + "for(int i = start;i<partition_size;i++)");
       prefixInc();
       _printStream.println(_prefix + "dst[i+shift] = *(loaded_data + i);");
       prefixDec();
       prefixDec();
       _printStream.print(_prefix);
       if(_silent)
           _printStream.print("//");
       _printStream.print("std::cout<<srcfile<<\" loaded \"<<std::endl;");
       _printStream.println(" ");
       _printStream.println(_prefix + "}");
    }

    /** Generate data partiotion path for weights load function*/
    private void _generateDataPartitionPath(){
       _printStream.println(_prefix + "std::string srcfile;");
       _printStream.println(_prefix + "srcfile.append(srcpathprefix);");
       _printStream.println(_prefix + "if (partition_id>=0) { ");
       _prefixInc();
       _printStream.println(_prefix + "std::stringstream ss;");
       _printStream.println(_prefix + "ss << partition_id;");
       _printStream.println(_prefix + "std::string stri = ss.str();");
       _printStream.println(_prefix + "srcfile.append(stri);");
       _prefixDec();
       _printStream.println(_prefix + "} ");
       _printStream.println(_prefix + "srcfile.append(\".npy\");");
    }

    /**
     * Set folder, from where the weights should be loaded
     * @param loadWeightsFolder folder, from where the weights should be loaded
     */
    public void setLoadWeightsFolder(String loadWeightsFolder) {
        _loadWeightsFolder = loadWeightsFolder;
    }

    /** use neuraghe functions*/
    public void setGenerateFuncNA(boolean generateDNNFuncNA) {
        _generateDNNFuncNA = generateDNNFuncNA;
    }

    ///////////////////////////////////////////////////////////////////
    ///                private variables                           ///

    /** primitive postfix*/
    private static String _externalRWPostfix = "SWF_CPU";

    /** application main class name*/
    private static String _mainClassName = "appMain";

    /** CSDF graph node base class*/
    private static String _baseClassName = "csdfNode";

    /** CSDF graph node functions class*/
    private static String _funcClassName = "appFunc";

    /** CSDF graph node base class*/
    private static String _loadWeightsClassName = "dataLoader";

    /** DNN input/output type */
    public String _IODatatype = "int";

    /** DNN parameters type*/
    public String _paramDataType = "int";

    /** CSDFG nodes schedule : get from the repetition vector of CSDFG??*/
    Vector<String> _schedule;

    /** current mapping */
    Mapping _mapping = null;

    ///// Mapping moc, in case mapping is not provided ///
    /** Number of cores */
    private static int _maxCores = 2;

    /** Current core Id */
    private static int _curCore = 0;

    /**
     * TODO should be replaced by an external parameter
     * partition size
     */
    private static int _partition_size = 100;

    /** If the debug couts should be printed*/
    private static boolean _silent = true;

    /** if weights should be initialized with dummy values*/
    private static boolean _initWeightsDummy = false;

    /** folder, from where the weights should be loaded*/
    /** if folder is set to null, no weights are loaded*/
    private String _loadWeightsFolder = null;

    /** If the NA library is used*/
    private static boolean _generateDNNFuncNA = false;
}
