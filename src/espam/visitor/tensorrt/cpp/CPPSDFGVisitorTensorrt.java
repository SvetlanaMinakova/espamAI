package espam.visitor.tensorrt.cpp;
import espam.datamodel.graph.cnn.BoundaryMode;
import espam.datamodel.graph.cnn.Layer;
import espam.datamodel.graph.cnn.Network;
import espam.datamodel.graph.cnn.Neuron;
import espam.datamodel.graph.cnn.neurons.arithmetic.Arithmetic;
import espam.datamodel.graph.cnn.neurons.cnn.CNNNeuron;
import espam.datamodel.graph.cnn.neurons.cnn.Convolution;
import espam.datamodel.graph.cnn.neurons.cnn.Pooling;
import espam.datamodel.graph.cnn.neurons.neurontypes.ArithmeticOpType;
import espam.datamodel.graph.cnn.neurons.neurontypes.DataType;
import espam.datamodel.graph.cnn.neurons.neurontypes.NonLinearType;
import espam.datamodel.graph.cnn.neurons.neurontypes.PoolingType;
import espam.datamodel.graph.cnn.neurons.normalization.LRN;
import espam.datamodel.graph.cnn.neurons.simple.Data;
import espam.datamodel.graph.cnn.neurons.simple.DenseBlock;
import espam.datamodel.graph.cnn.neurons.simple.NonLinear;
import espam.datamodel.graph.cnn.neurons.transformation.Concat;
import espam.datamodel.graph.cnn.operators.Operator;
import espam.datamodel.graph.csdf.datasctructures.Tensor;
import espam.utils.fileworker.FileWorker;
import espam.visitor.CNNGraphVisitor;

import java.util.HashMap;
import java.util.Iterator;
import java.util.TreeMap;
import java.util.Vector;

public class CPPSDFGVisitorTensorrt extends CNNGraphVisitor {

    ///////////////////////////////////////////////////////////////////
    ////                         public methods                     ///

    /**
     * Call CPP SDFG Visitor of the operational node (node, performs some useful job)
     *
     * @param dnn DNN
     * @param dir directory for .cpp templates
     */
    public void callDNNVisitor(Network dnn, String dir) {
        try {
            String className = dnn.getName();
            _printStream = FileWorker.openFile(dir, className, "cpp");
            _writeCommonCppBeginning(className);
            _writePartition(dnn);
            _initParams(dnn);
            _generateDummyWeights(dnn);
            _writeCppConstructor(className);
            _writeCppDestructor(className);
            //writeMain(y);
        } catch (Exception e) {
            System.err.println(".cpp file creation error for node" + dnn.getName() + " " + e.getMessage());
        }
    }


///////////////////        ONE       ENGINE    //////////////////////////////////

    /**
     * Write common beginning for all generated nodes, contains:
     * - definition of header
     * - definition of standard libraries
     * - definition of namespace
     *
     * @param className name of the .cpp class
     */
    protected void _writeCommonCppBeginning(String className) {
        // _printStream.println("// File automatically generated by ESPAM");
        // _printStream.println("");
        //tensorrt classes
        _printStream.println("#include \"NvInfer.h\"");
        _printStream.println("#include \"common.h\"");
        _printStream.println("#include \"cuda_runtime_api.h\"");

        _printStream.println("#include <map>");
        _printStream.println("#include <vector>");
        _printStream.println("#include <thread>");

        _printStream.println("#include \"" + className + ".h\"");
        //base class name
        _printStream.println("#include \"gpu_partition.h\"");

        _printStream.println("");
        _printStream.println(_prefix + "using namespace std;");
        _printStream.println(_prefix + "using namespace nvinfer1;");
        _printStream.println("");
    }


    //NETWORK ENGINE WITH API
    protected void _writePartition(Network partition){
    _printStream.println(_prefix +"//NETWORK ENGINE WITH API");
    _printStream.println(_prefix +"ICudaEngine* " + partition.getName() + "::createEngine(std::map<std::string, Weights>& weightMap," +
            " unsigned int maxBatchSize, IBuilder* builder, nvinfer1::DataType dt) {");
    prefixInc();
    _printStream.println(_prefix + "INetworkDefinition* network = builder->createNetwork();");
    _printStream.println("");


    //CREATE TOPOLOGY HERE
    partition.sortLayersInTraverseOrder();
    Layer inputLayer = partition.getInputLayer();
    if(!(inputLayer.getNeuron() instanceof Data))
        simulateDataLayer(inputLayer.getNeuron(), DataType.INPUT);

    Iterator i = partition.getLayers().iterator();
    Layer layer;
    while (i.hasNext()) {
        layer = (Layer) i.next();
        _printStream.println(_prefix + "//" + layer.getName());
        visitComponent(layer.getNeuron());
        if(layer.getNeuron().getNonlin()!=null){
            _printStream.println();
            _visitNonLin(layer);
        }

        _printStream.println("");
    }

    Layer outputLayer = partition.getOutputLayer();
        if(!(outputLayer.getNeuron() instanceof Data))
            simulateDataLayer(outputLayer.getNeuron(), DataType.OUTPUT);


    _printStream.println(_prefix + "// Build engine");

    // Build engine
    _printStream.println(_prefix + "builder->setMaxBatchSize(maxBatchSize);");
    _printStream.println(_prefix + "builder->setMaxWorkspaceSize(this->WORKSPACE_SIZE << 20);");
    _printStream.println(_prefix + "ICudaEngine* engine = builder->buildCudaEngine(*network);");

    _printStream.println(_prefix + "// Don't need the network any more");
    _printStream.println(_prefix + "network->destroy();");
   // std::cout<<"network 1 destroyed!"<<std::endl;

    _printStream.println(_prefix + "return engine;");

    _printStream.println("");

    _prefixDec();
    _printStream.println("}");
    _printStream.println("");

    }

    ///////////////////        NEURON VISITORS   //////////////////////////////////


    @Override
    public void visitComponent(Arithmetic x) {
        Layer parent = x.getParent();
        Vector<String> inputNames = _getInputLayerNamesList(parent);
        Integer inputsNum = inputNames.size();
        String layerName = parent.getName();
        if(inputsNum!=2) {
            System.out.println(_prefix + "ERROR: Aritmetic operator " + layerName +" processing error: 2 inputs expected, " +
            inputsNum + " provided!");
            return;
        }

        String elementWiseOp = _getElemenWiseOp(x);
        String inputLayer0 = inputNames.firstElement();
        String inputLayer1 = inputNames.elementAt(1);

        _printStream.println(_prefix + "auto " + layerName +" = network->addElementWise(*" + inputLayer0 +
                ", *" + inputLayer1 + ", ElementWiseOperation::" + elementWiseOp + " );");

        _writeLayerAssert(layerName);
    }

    /**
     * Translate Arithmetic (element wise) operator name from ESPAM namespace to TENSORRT namespace
     * @param x Aritmetic neuron
     * @return Arithmetic operator in TENSORRT namespace
     */
    private String _getElemenWiseOp(Arithmetic x){
        String elementWiseDefault= "kSUM";

        if(x.getName().equals(ArithmeticOpType.ADD.toString()))
            return "kSUM";

        if(x.getName().equals(ArithmeticOpType.MUL.toString()))
            return "kPROD";

        return elementWiseDefault;

    }

    @Override
    public void visitComponent(Concat x) {
        Layer parent = x.getParent();
        Vector<String> inputNames = _getInputLayerNamesList(parent);
        Integer inputsNum = inputNames.size();
        String layerName = parent.getName();
        if(inputsNum==0) {
            System.out.println(_prefix + "ERROR: zero inputs found for layer " + layerName);
            return;
        }

        _printStream.print(_prefix + "std::array<nvinfer1::ITensor*, " + inputsNum + "> "+  layerName + "_inputs{{");
        for(int i=0; i<inputNames.size()-1; i++)
             _printStream.print(inputNames.elementAt(i) + ", ");

        _printStream.println(inputNames.lastElement() + "}};");

        _printStream.println(_prefix + "IConcatenationLayer* " + layerName +" = network->addConcatenation(" +
                        layerName + "_inputs.data(), " +inputsNum + ");");

        _writeLayerAssert(layerName);
    }

    @Override
    public void visitComponent(Data x) {
        Layer parent = x.getParent();
        String layerName = parent.getName();
        if(x.getName().equals(DataType.INPUT.toString())){
            _printStream.println(_prefix + "// Create input tensor");
            _printStream.println(_prefix + "ITensor* " + layerName +" = network->addInput(this->INPUT_BLOB_NAME, dt, Dims4{1, this->INPUT_C, this->INPUT_H, this->INPUT_W});");
            _printStream.println(_prefix + "assert(" + layerName + ");");
        }

        if(x.getName().equals(DataType.OUTPUT.toString())) {
            String inputLayer = _getInputLayerName(parent);
            //process prev layer as output layer
            _printStream.println(_prefix + inputLayer +"->setName(this->OUTPUT_BLOB_NAME);");
            _printStream.println(_prefix + "network->markOutput(*" + inputLayer + ");");

        }
    }

    /** Simulate I/O layers for hidden partitions*/
    public void simulateDataLayer(Neuron x, DataType dt) {
        Layer parent = x.getParent();
        String layerName = parent.getName();
        String inputLayerName = _getInputLayerName(parent);
        String inputLayerNameDef = inputLayerName.replace("->getOutput(0)", "");

        if(dt == DataType.INPUT){
            _printStream.println(_prefix + "// Create input tensor");
            _printStream.println(_prefix + "ITensor* " + inputLayerNameDef +" = network->addInput(this->INPUT_BLOB_NAME, dt, Dims4{1, this->INPUT_C, this->INPUT_H, this->INPUT_W});");
            _printStream.println(_prefix + "assert(" + inputLayerNameDef + ");");
        }

        if(dt == DataType.OUTPUT) {
            //process this layer as output layer
            _printStream.println(_prefix + layerName +"->getOutput(0)->setName(this->OUTPUT_BLOB_NAME);");
            _printStream.println(_prefix + "network->markOutput(*" + layerName + "->getOutput(0));");
        }
    }

    @Override
    public void visitComponent(DenseBlock x) {
        Layer parent = x.getParent();
        String inputLayer = _getInputLayerName(parent);
        String layerName = parent.getName();

        _printStream.println(_prefix + "IFullyConnectedLayer* " + layerName +" = network->addFullyConnected(*" + inputLayer +
                ", " + x.getNeuronsNum() +
                    ", weightMap[\"" + layerName + "_weights\"], weightMap[\"" + layerName + "_bias\"]);");

        _writeLayerAssert(layerName);
    }

    /**
     * Visit LRN Layer
     * @param  x A Visitor Object.
     */
    @Override
    public void visitComponent(LRN x) {
        Layer parent = x.getParent();
        String inputLayer = _getInputLayerName(parent);
        String layerName = parent.getName();

        _printStream.println(_prefix + "ILRNLayer* " + layerName + "= network->addLRN(*" + inputLayer + "," +
               x.getSize() +"," + x.getAlpha() +"," + x.getBeta() + ", " + x.getBias() +");");

       _writeLayerAssert(layerName);

    }

    /** visit CNN layer */
    @Override
    public void visitComponent(CNNNeuron x) {
        if(x instanceof Convolution){
            visitComponent((Convolution) x);
            return;
        }

       if(x instanceof Pooling){
            visitComponent((Pooling) x);
            return;
        }
    }

    /** visit convolutional layer*/
    @Override
    public void visitComponent(Convolution x) {

        Layer parent = x.getParent();
        String inputLayer = _getInputLayerName(parent);
        String layerName = parent.getName();

        _printStream.println(_prefix + "IConvolutionLayer* " + layerName +" = network->addConvolution(*" + inputLayer +
                ", " + parent.getNeuronsNum() + ", DimsHW{" + x.getKernelSize() + ", " + x.getKernelSize() + "}," +
                    " weightMap[\"" + layerName + "_weights\"], weightMap[\"" + layerName + "_bias\"]);");

        _writeLayerAssert(layerName);
        _printStream.println(_prefix + layerName + "->setStride(DimsHW{"+ x.getStride() + ", " + x.getStride() + "});");

        _processPads(x);
    }


    /**
     * Visit Pooling
     * @param  x A Visitor Object.
     */
    @Override
    public void visitComponent(Pooling x) {
        String func = "PoolingType::kMAX";
        if(x.getName().equals(PoolingType.AVGPOOL.toString()))
            func = "PoolingType::kAVERAGE";

        Layer parent = x.getParent();
        String inputLayer = _getInputLayerName(parent);
        String layerName = parent.getName();

        _printStream.println(_prefix + "IPoolingLayer* " + layerName +" = network->addPooling(*" + inputLayer +
                ", " + func + ", DimsHW{" + x.getKernelSize() + ", " + x.getKernelSize() + "});");

        _writeLayerAssert(layerName);
        _printStream.println(_prefix + layerName + "->setStride(DimsHW{"+ x.getStride() + ", " + x.getStride() + "});");

        _processPads(x);
    }

        /**
     * Pads are values, added to the beginning and ending along each axis
     * to avoid "inconvenient" data formats in Convolutional and Padding layers
     * * in format [x1_begin, x2_begin...x1_end, x2_end,...],
    * where xi_begin the number of pixels added at the beginning of axis `i` and xi_end,
    * the number of pixels added at the end of axis `i`.
    * Pads should contain values >=0
     */
    private void _processPads(CNNNeuron x) {
        Layer parent = x.getParent();
        String layerName = parent.getName();
        int pads[] = parent.getPads();
        if(!(parent.isNullorEmptyPads())) {
            _printStream.println(_prefix + layerName + "->setPadding(DimsHW{" + pads[1] + ", " + pads[0] + "});");
        }

        else {

            if ((x instanceof Convolution) && x.getBoundaryMode() == BoundaryMode.SAME) {
                pads = x.simulateSameAutoPads();
                _printStream.println(_prefix + layerName + "->setPadding(DimsHW{" + pads[1] + ", " + pads[0] + "});");
            }
        }
    }

    /** visit nonlinear layer*/
    @Override
    public void visitComponent(NonLinear x) {
        Layer parent = x.getParent();
        String inputLayer = _getInputLayerName(parent);
        String layerName = parent.getName();
        String neuronName = x.getName();

        if(x.getName().equals(NonLinearType.ReLU.toString()))
            _printStream.println(_prefix + "IActivationLayer* " + layerName + "= network->addActivation(*" + inputLayer + ", ActivationType::kRELU);");

        if(x.getName().equals(NonLinearType.LeakyReLu.toString()))
            _printStream.println(_prefix + "IActivationLayer* " + layerName + "= network->addActivation(*" + inputLayer + ", ActivationType::kRELU);");

        if(x.getName().equals(NonLinearType.BN.toString())) {
            _printStream.println(_prefix + "// Create a scale layer with default power/shift and specified scale parameter.");

            _printStream.println(_prefix + "auto " + layerName + " = network->addScale(*" + inputLayer + ", ScaleMode::kUNIFORM," +
                    " weightMap[\"" + layerName + "_shift\"], weightMap[\"" + layerName + "_scale\"], weightMap[\"" + layerName + "_power\"]);");
        }

        if(x.getName().equals(NonLinearType.MULconst.toString()) ||x.getName().equals(NonLinearType.DIVConst.toString())|| x.getName().equals(NonLinearType.SUBConst.toString())){
             _printStream.println(_prefix + "auto " + layerName + " = network->addScale(*" + inputLayer + ", ScaleMode::kUNIFORM," +
                    " weightMap[\"" + layerName + "_shift\"], weightMap[\"" + layerName + "_scale\"], weightMap[\"" + layerName + "_power\"]);");
        }

        if(x.getName().equals(NonLinearType.SOFTMAX.toString()))
             _printStream.println(_prefix + "ISoftMaxLayer* " + layerName + "= network->addSoftMax(*"+ inputLayer +");");

        _writeLayerAssert(layerName);
    }

    /** Visit incapsulated nonLinear */
    private void _visitNonLin(Layer layer){

        String nonlinType = layer.getNeuron().getNonlin();
        String inputLayer = layer.getName() + "->getOutput(0)";
        String layerName = layer.getName() + nonlinType;

        if(nonlinType.equals(NonLinearType.LeakyReLu.toString()))
            _printStream.println(_prefix + "IActivationLayer* " + layerName + "= network->addActivation(*" + inputLayer + ", ActivationType::kRELU);");

        if(nonlinType.equals(NonLinearType.ReLU.toString()))
            _printStream.println(_prefix + "IActivationLayer* " + layerName + "= network->addActivation(*" + inputLayer + ", ActivationType::kRELU);");

        if(nonlinType.equals(NonLinearType.SOFTMAX.toString()))
             _printStream.println(_prefix + "ISoftMaxLayer* " + layerName + "= network->addSoftMax(*"+ inputLayer+");");

        _writeLayerAssert(layerName);

    }

     /**TODO refactoring
     * Get name of the input buffer
     * @param layer layer
     * @return name of the input buffer
     */
    protected String _getInputLayerName(Layer layer){
        String inputLayer = "nullptr";
        if(layer.getInputConnections().size()>0){
            Layer src = layer.getInputConnections().firstElement().getSrc();
            inputLayer = src.getName();
            if(src.getNeuron().getNonlin()!=null)
                inputLayer+=src.getNeuron().getNonlin();

            if(!(src.getNeuron() instanceof Data))
                inputLayer = inputLayer +"->getOutput(0)";
        }
        else inputLayer = layer.getName()+"_input";

        return inputLayer;

    }

     /**ONLY FOR multiple inputs processors
     * Get name of the input buffer
     * @param layer layer
     * @return name of the input buffer
     */
    protected Vector<String> _getInputLayerNamesList(Layer layer){
        Vector<String> namesList = new Vector<>();
        String inputLayer = "nullptr";
             for (int i = 0; i < layer.getInputConnections().size(); i++) {
                 Layer src = layer.getInputConnections().elementAt(i).getSrc();
                 inputLayer = src.getName();
                 if (src.getNeuron().getNonlin() != null)
                     inputLayer += src.getNeuron().getNonlin();

                 if (!(src.getNeuron() instanceof Data))
                     inputLayer = inputLayer + "->getOutput(0)";
                 namesList.add(inputLayer);
             }

        //MOC or outer input
        if(namesList.size()==0){
             inputLayer = layer.getName() + "_input";
             namesList.add(inputLayer);
         }

        return namesList;
    }

    /** write common ending for Layer definition*/
    protected void _writeLayerAssert (String layerName){
        _printStream.println(_prefix + "assert("+ layerName +");");
        _printStream.println(_prefix + layerName + "->setName(\"" + layerName + "\");");

    }


///////////////////        STANDARD LINES   //////////////////////////////////
    /**
     * Write constuctor
     *
     * @param className name of the .cpp class
     */
    protected void _writeCppConstructor(String className) {
        _printStream.println(_prefix + "//constructor");
        _printStream.println(_prefix + className + "::" + className + "(){");
        _prefixInc();
        _printStream.println(_prefix + "init_params();");
        _printStream.println(_prefix + "generate_dummy_weights();");
        _printStream.println(_prefix + "init_partition();");
        _prefixDec();
        _printStream.println("}");
        _printStream.println("");
    }

    /**
     * Write constructor and destructor .cpp definitions
     *
     * @param className name of the .cpp class
     */
    protected void _writeCppDestructor(String className) {
        _printStream.println(_prefix + "//destructor");
        _printStream.println(_prefix + className + "::~" + className + "() {}");
        _printStream.println("");
    }



    /**
     * Write init parameters function
     *
     * @param dnn DNN
     */
    protected void _initParams(Network dnn) {

    String className = dnn.getName();
    Layer inputLayer = dnn.getInputLayer();
    Layer outputLayer = dnn.getOutputLayer();

    _printStream.println(_prefix + "//init params function");

    _printStream.println(_prefix + "void "+ className + "::init_params(){ ");
    _prefixInc();
    _printStream.println(_prefix +" this->batchSize = 1;");
    _printStream.println(_prefix +" this->WORKSPACE_SIZE = 4;");
    if(inputLayer.getNeuron() instanceof Data)
        _printStream.println(_prefix +" this->INPUT_BLOB_NAME = \"" + inputLayer.getName() + "\";");
    else
        _printStream.println(_prefix +" this->INPUT_BLOB_NAME = \"" + inputLayer.getName() + "_input\";");

    if(outputLayer.getNeuron() instanceof Data)
        _printStream.println(_prefix +" this->OUTPUT_BLOB_NAME = \"" + outputLayer.getName() + "\";");
    else
        _printStream.println(_prefix +" this->OUTPUT_BLOB_NAME = \"" + outputLayer.getName() + "_output\";");

    /** TODO: refactoring*/
    if (inputLayer.getNeuron() instanceof DenseBlock){
       Operator op = inputLayer.getNeuron().getOperator();
       if (op==null) {
           inputLayer.initOperator();
           op = inputLayer.getNeuron().getOperator();
       }
       int inputChannels = op.getIntParams().get("channels");
       _printStream.println(_prefix + " this->INPUT_C = " + inputChannels + ";");
        }
    else
        _printStream.println(_prefix + " this->INPUT_C = " + inputLayer.getInputChannels() + ";");

    _printStream.println(_prefix +" this->INPUT_H = " + inputLayer.getInpH()+ ";");
    _printStream.println(_prefix +" this->INPUT_W = " + inputLayer.getInpW() + ";");
    _printStream.println();

    _printStream.println(_prefix +" this->OUTPUT_C = " + outputLayer.getOutputChannels() + ";");
    _printStream.println(_prefix +" this->OUTPUT_H = " + outputLayer.getOutpH() + ";");
    _printStream.println(_prefix +" this->OUTPUT_W = " + outputLayer.getOutpW() + ";");
    _printStream.println(_prefix +" this->OUTPUT_SIZE = OUTPUT_C * OUTPUT_H * OUTPUT_W;");
    _prefixDec();
    _printStream.println("}");
    _printStream.println();

    }

    /**
     * Write init parameters function
     *
     *
     * @param dnn DNN
     */
    protected void _generateDummyWeights(Network dnn) {
        _printStream.println(_prefix + "//generate dummy weights");
        _printStream.println(_prefix + "void " + dnn.getName() + "::generate_dummy_weights() {");

        _prefixInc();
        _printStream.println(_prefix + "//Weight blob names ");
        _printStream.println(_prefix + "std::vector<std::string> names;");
        _printStream.println(_prefix + "//Weight blob sizes ");
        _printStream.println(_prefix + "std::vector<uint32_t> sizes;");
        _printStream.println();


        for(Layer l: dnn.getLayers()){
            if(l.getNeuron() instanceof Convolution || l.getNeuron() instanceof DenseBlock){
                Operator op = l.getNeuron().getOperator();
                if (op==null) {
                    l.initOperator();
                    op = l.getNeuron().getOperator();
                }

                TreeMap<String, Tensor> tensorParams = op.getTensorParams();
                Tensor weights, bias;

                _printStream.println("//" + l.getName());
                _printStream.println(_prefix + "names.push_back(\"" + l.getName() + "_weights\");");
                 weights = tensorParams.get("weights");
                _printStream.println(_prefix + "sizes.push_back(" + weights.getElementsNumber() + ");");
                _printStream.println(_prefix + "names.push_back(\"" + l.getName() + "_bias\");");
                 bias = tensorParams.get("bias");
                _printStream.println(_prefix + "sizes.push_back(" +bias.getElementsNumber() + ");");
                _printStream.println();
            }

            if(l.getNeuron().getName().equals(NonLinearType.BN.toString())){
                Operator op = l.getNeuron().getOperator();
                if (op==null) {
                    l.initOperator();
                    op = l.getNeuron().getOperator();
                }

                TreeMap<String, Tensor> tensorParams = op.getTensorParams();
                Tensor power, shift, scale; //bias, means, scale, variance

                /**TODO check if the names are correctly transformed from ONNX to TRT*/
                shift = tensorParams.get("bias");
                scale = tensorParams.get("scale");
                power = tensorParams.get("variance");


                /** TODO: should be obtained from tensorParams*/
                //Weights power{DataType::kFLOAT, nullptr, 0};
                //Weights shift{DataType::kFLOAT, nullptr, 0};
                //Weights scale{DataType::kFLOAT, &scale_param, 1};

                _printStream.println("//" + l.getName());
                _printStream.println(_prefix + "names.push_back(\"" + l.getName() + "_shift\");");
                //_printStream.println(_prefix + "sizes.push_back(" + shift.getElementsNumber() + ");");
                _printStream.println(_prefix + "sizes.push_back(0);");

                _printStream.println(_prefix + "names.push_back(\"" + l.getName() + "_scale\");");
                //_printStream.println(_prefix + "sizes.push_back(" +scale.getElementsNumber() + ");");
                _printStream.println(_prefix + "sizes.push_back(0);");

                _printStream.println(_prefix + "names.push_back(\"" + l.getName() + "_power\");");
                //_printStream.println(_prefix + "sizes.push_back(" +power.getElementsNumber() + ");");
                _printStream.println(_prefix + "sizes.push_back(0);");

                _printStream.println();
            }

            if(l.getNeuron().getName().equals(NonLinearType.MULconst.toString()) || l.getNeuron().getName().equals(NonLinearType.DIVConst.toString())){
                Operator op = l.getNeuron().getOperator();
                if (op==null) {
                    l.initOperator();
                    op = l.getNeuron().getOperator();
                }

                TreeMap<String, Tensor> tensorParams = op.getTensorParams();
                Tensor power, shift, scale; //bias, means, scale, variance

                /**TODO check if the names are correctly transformed from ONNX to TRT*/
                shift = tensorParams.get("bias");
                scale = tensorParams.get("scale");
                power = tensorParams.get("variance");


                /** TODO: should be obtained from tensorParams*/
                //Weights power{DataType::kFLOAT, nullptr, 0};
                //Weights shift{DataType::kFLOAT, nullptr, 0};
                //Weights scale{DataType::kFLOAT, &scale_param, 1};

                _printStream.println("//" + l.getName());
                _printStream.println(_prefix + "names.push_back(\"" + l.getName() + "_shift\");");
                //_printStream.println(_prefix + "sizes.push_back(" + shift.getElementsNumber() + ");");
                _printStream.println(_prefix + "sizes.push_back(0);");

                _printStream.println(_prefix + "names.push_back(\"" + l.getName() + "_scale\");");
                //_printStream.println(_prefix + "sizes.push_back(" +scale.getElementsNumber() + ");");
                _printStream.println(_prefix + "sizes.push_back(0);");

                _printStream.println(_prefix + "names.push_back(\"" + l.getName() + "_power\");");
                //_printStream.println(_prefix + "sizes.push_back(" +power.getElementsNumber() + ");");
                _printStream.println(_prefix + "sizes.push_back(0);");

                _printStream.println();
            }
        }
        _printStream.println();
        _printStream.println(_prefix + "generate_dummy(names, sizes);");

        _prefixDec();
        _printStream.println("}");
        _printStream.println();

    }

}