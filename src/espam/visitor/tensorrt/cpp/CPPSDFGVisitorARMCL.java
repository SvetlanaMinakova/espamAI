package espam.visitor.tensorrt.cpp;

import espam.datamodel.graph.cnn.BoundaryMode;
import espam.datamodel.graph.cnn.Layer;
import espam.datamodel.graph.cnn.Network;
import espam.datamodel.graph.cnn.Neuron;
import espam.datamodel.graph.cnn.neurons.cnn.CNNNeuron;
import espam.datamodel.graph.cnn.neurons.cnn.Convolution;
import espam.datamodel.graph.cnn.neurons.cnn.Pooling;
import espam.datamodel.graph.cnn.neurons.neurontypes.DataType;
import espam.datamodel.graph.cnn.neurons.neurontypes.NonLinearType;
import espam.datamodel.graph.cnn.neurons.neurontypes.PoolingType;
import espam.datamodel.graph.cnn.neurons.normalization.LRN;
import espam.datamodel.graph.cnn.neurons.simple.Data;
import espam.datamodel.graph.cnn.neurons.simple.DenseBlock;
import espam.datamodel.graph.cnn.neurons.simple.NonLinear;
import espam.datamodel.graph.cnn.neurons.transformation.Concat;
import espam.utils.fileworker.FileWorker;
import espam.visitor.CNNGraphVisitor;

import java.util.Iterator;

public class CPPSDFGVisitorARMCL extends CNNGraphVisitor{

    ///////////////////////////////////////////////////////////////////
    ////                         public methods                     ///

    /**
     * Call CPP SDFG Visitor of the operational node (node, performs some useful job)
     *
     * @param dnn DNN
     * @param dir directory for .cpp templates
     */
    public void callDNNVisitor(Network dnn, String dir) {
        try {
            _printStream = FileWorker.openFile(dir, dnn.getName(), "cpp");
            _writeCommonCppBeginning(dnn.getName());
            _writePartition(dnn);
            _writeDoRun(dnn.getName());

        } catch (Exception e) {
            System.err.println(".cpp file creation error for node" + dnn.getName() + " " + e.getMessage());
        }
    }

///////////////////        ONE       ENGINE    //////////////////////////////////

    /**
     * Write common beginning for all generated nodes, contains:
     * - definition of header
     * - definition of standard libraries
     * - definition of namespace
     *
     * @param className name of the .cpp class
     */
    protected void _writeCommonCppBeginning(String className) {
        // _printStream.println("// File automatically generated by ESPAM");
        // _printStream.println("");
        //tensorrt classes
//ARM CL classes
        _printStream.println("#include \"arm_compute/graph.h\"");
        _printStream.println("#include \"support/ToolchainSupport.h\"");
        _printStream.println("#include \"utils/CommonGraphOptions.h\"");
        _printStream.println("#include \"utils/GraphUtils.h\"");
        _printStream.println("#include \"utils/Utils.h\"");

        _printStream.println("#include <chrono>");
        _printStream.println("#include <thread>");

        _printStream.println("#include \"" + className + ".h\"");
        _printStream.println("#include \"fifo.h\"");
        _printStream.println("#include \"types.h\"");


        _printStream.println("");
        _printStream.println(_prefix + "using namespace arm_compute::utils;");
        _printStream.println(_prefix + "using namespace arm_compute::graph::frontend;");
        _printStream.println(_prefix + "using namespace arm_compute::graph_utils;");
        _printStream.println("");
    }

    //NETWORK ENGINE WITH API
    protected void _writePartition(Network partition) {
        String className = partition.getName();
        _writeCommonPartitionStart(className);

        //DEFINE INPUT DATA AND START GRAPH
        _printStream.println(_prefix + "// Create input tensor");
        _printStream.println(_prefix + "const TensorShape tensor_shape = permute_shape(TensorShape(" +
                            "this->INPUT_W, this->INPUT_H, this->INPUT_C, 1U), DataLayout::NCHW, common_params.data_layout);");
        _printStream.println(_prefix + "TensorDescriptor input_descriptor = " +
                    "TensorDescriptor(tensor_shape, common_params.data_type).set_layout(common_params.data_layout);");
        _printStream.println();
        _printStream.println(_prefix + "graph << common_params.target");
        _prefixInc();
        _prefixInc();
        _prefixInc();
        _printStream.println(_prefix + "<< common_params.fast_math_hint");

        //CREATE TOPOLOGY HERE
        partition.sortLayersInTraverseOrder();
        Layer inputLayer = partition.getInputLayer();
        if(!(inputLayer.getNeuron() instanceof Data))
            simulateDataLayer(DataType.INPUT);

        Iterator i = partition.getLayers().iterator();
        Layer layer;
        while (i.hasNext()) {
        layer = (Layer) i.next();
        //_printStream.println(_prefix + "//" + layer.getName());
        visitComponent(layer.getNeuron());
        if(layer.getNeuron().getNonlin()!=null){
            _visitNonLin(layer);
        }
       }

       Layer outputLayer = partition.getOutputLayer();
        if(!(outputLayer.getNeuron() instanceof Data))
            simulateDataLayer(DataType.OUTPUT);

        _prefixDec();
        _prefixDec();
        _prefixDec();

        _printStream.println("");
        _writeCommonPartitionEnd();
    }


///////////////////        NEURON VISITORS   //////////////////////////////////

    @Override
    public void visitComponent(DenseBlock x) {
        Layer parent = x.getParent();
        String layerName = parent.getName();
        _printStream.println(_prefix + "<< FullyConnectedLayer(");
        _prefixInc();
        _prefixInc();
        _printStream.println(_prefix + x.getNeuronsNum() + "U,");
        _printStream.println(_prefix +"get_weights_accessor(data_path, \" \", weights_layout),");
        _printStream.println(_prefix +"get_weights_accessor(data_path, \" \"))");
        _prefixDec();
        _prefixDec();
        _printStream.println(_prefix + ".set_name(\"" + layerName + "\")");
    }

    /**
     * Visit LRN Layer
     * @param  x A Visitor Object.
     */
    @Override
    public void visitComponent(LRN x) {
        Layer parent = x.getParent();
        String layerName = parent.getName();
        _printStream.println(_prefix + "<< NormalizationLayer(NormalizationLayerInfo(NormType::CROSS_MAP, "+
                x.getSize()+", " + x.getAlpha() + "f, " + x.getBeta() + "f)).set_name(\"" + layerName + "\")");

    }

    @Override
    public void visitComponent(Data x) {

        if(x.getName().equals(DataType.INPUT.toString())){
            _printStream.println(_prefix + "<< InputLayer(input_descriptor, get_input_accessor(common_params, nullptr))");
        }

        if(x.getName().equals(DataType.OUTPUT.toString())) {//TODO: 5??
            _printStream.println(_prefix + "<< OutputLayer(get_output_accessor(common_params, 5));");
        }
    }

    /** Simulate I/O layers for hidden partitions*/
    public void simulateDataLayer(DataType dt) {
        if(dt == DataType.INPUT){
            _printStream.println(_prefix + "<< InputLayer(input_descriptor, get_input_accessor(common_params, nullptr))");
        }

        if(dt == DataType.OUTPUT) {//TODO: 5??
            _printStream.println(_prefix + "<< OutputLayer(get_output_accessor(common_params, 5));");
        }
    }


        /** visit CNN layer */
    @Override
    public void visitComponent(CNNNeuron x) {
        if(x instanceof Convolution){
            visitComponent((Convolution) x);
            return;
        }

       if(x instanceof Pooling){
            visitComponent((Pooling) x);
            return;
        }
    }

    /** visit convolutional layer*/
    /** TODO: weights!*/
    @Override
    public void visitComponent(Convolution x) {
        Layer parent = x.getParent();
        String layerName = parent.getName();
        _printStream.println(_prefix + "<< ConvolutionLayer(");
        _prefixInc();
        _prefixInc();
        _printStream.println(_prefix + x.getKernelSize() + "U, "+ x.getKernelSize() + "U, " + parent.getNeuronsNum() + "U,");
        _printStream.println(_prefix +"get_weights_accessor(data_path, \" \", weights_layout),");
        _printStream.println(_prefix +"get_weights_accessor(data_path, \" \"),");
        _printStream.print(_prefix + "PadStrideInfo(" + x.getStride() + ", " + x.getStride() + ", ");
        _processPads(x);
        _printStream.println("))");//, 2)???
        _prefixDec();
        _prefixDec();
        _printStream.println(_prefix + ".set_name(\"" + layerName + "\")");
    }

    /**
     * Visit Pooling
     * @param  x A Visitor Object.
     */

    @Override
    public void visitComponent(Pooling x) {
        String func = "PoolingType::MAX";
        if(x.getName().equals(PoolingType.AVGPOOL))
            func = "PoolingType::AV";

        Layer parent = x.getParent();
        String layerName = parent.getName();
        _printStream.print(_prefix + "<< PoolingLayer(PoolingLayerInfo(" + func +
                ", " + x.getStride() + ", PadStrideInfo(" + x.getStride() + ", " + x.getStride() + ", ");
        _processPads(x);
        _printStream.println("))).set_name(\"" + layerName + "\")");
    }

        /**
     * Pads are values, added to the beginning and ending along each axis
     * to avoid "inconvenient" data formats in Convolutional and Padding layers
     * * in format [x1_begin, x2_begin...x1_end, x2_end,...],
    * where xi_begin the number of pixels added at the beginning of axis `i` and xi_end,
    * the number of pixels added at the end of axis `i`.
    * Pads should contain values >=0
     */
    private void _processPads(CNNNeuron x) {

        Layer parent = x.getParent();
        int pads[] = parent.getPads();

        //explicitly provided pads
        if(!(parent.isNullorEmptyPads())) {
            _printStream.print(pads[0] + ", " + pads[1]);
            return;
        }

        //same-border simulation
        if ((x instanceof Convolution) && x.getBoundaryMode() == BoundaryMode.SAME) {
                pads = x.simulateSameAutoPads();
                _printStream.print(pads[0] + ", " + pads[1]);
                return;
        }

        //default
        _printStream.print(" 0, 0");

    }

    @Override
    /**
     * TODO: its a MOC!!!
     */
    public void visitComponent(Concat x) {
        Layer parent = x.getParent();
        String layerName = parent.getName();
        _printStream.println(_prefix + "<< ActivationLayer(ActivationLayerInfo(ActivationLayerInfo::ActivationFunction::RELU)).set_name(\"" + layerName + "\")");
    }

    /** visit nonlinear layer*/
    @Override
    public void visitComponent(NonLinear x) {
        Layer parent = x.getParent();
        String layerName = parent.getName();

        if(x.getName().equals(NonLinearType.ReLU.toString()))
            _printStream.println(_prefix + "<< ActivationLayer(ActivationLayerInfo(ActivationLayerInfo::ActivationFunction::RELU)).set_name(\"" + layerName + "\")");

        if(x.getName().equals(NonLinearType.LeakyReLu.toString()))
            _printStream.println(_prefix + "<< ActivationLayer(ActivationLayerInfo(ActivationLayerInfo::ActivationFunction::LEAKY_RELU)).set_name(\"" + layerName + "\")");

        if(x.getName().equals(NonLinearType.SOFTMAX.toString()))
             _printStream.println(_prefix + "<< SoftmaxLayer().set_name(\"" + layerName + "\")");

        if(x.getName().equals(NonLinearType.BN.toString())) {
            _printStream.println(_prefix + "<< BatchNormalizationLayer(get_weights_accessor(data_path, \" \"),");
            _prefixInc();
            _prefixInc();
            _printStream.println(_prefix + "get_weights_accessor(data_path, \"\"),");
            _printStream.println(_prefix + "get_weights_accessor(data_path, \"\"),");
            _printStream.println(_prefix + "get_weights_accessor(data_path, \"\"),");
            _printStream.println(_prefix + "0.0010000000474974513f)");
            _prefixDec();
            _prefixDec();
            _printStream.println(_prefix + ".set_name(\"" + layerName + "\")");
        }

          if(x.getName().equals(NonLinearType.MULconst.toString()) || x.getName().equals(NonLinearType.DIVConst.toString())) {
            _printStream.println(_prefix + "<< BatchNormalizationLayer(get_weights_accessor(data_path, \" \"),");
            _prefixInc();
            _prefixInc();
            _printStream.println(_prefix + "get_weights_accessor(data_path, \"\"),");
            _printStream.println(_prefix + "get_weights_accessor(data_path, \"\"),");
            _printStream.println(_prefix + "get_weights_accessor(data_path, \"\"),");
            _printStream.println(_prefix + "0.0010000000474974513f)");
            _prefixDec();
            _prefixDec();
            _printStream.println(_prefix + ".set_name(\"" + layerName + "\")");
        }

    }

    /** Visit incapsulated nonLinear */
    private void _visitNonLin(Layer layer){
        String nonlinType = layer.getNeuron().getNonlin();
        String layerName = layer.getName() + nonlinType;

        if(nonlinType.equals(NonLinearType.ReLU.toString()))
            _printStream.println(_prefix + "<< ActivationLayer(ActivationLayerInfo(ActivationLayerInfo::ActivationFunction::RELU)).set_name(\"" + layerName + "\")");

        if(nonlinType.equals(NonLinearType.SOFTMAX.toString()))
             _printStream.println(_prefix + "<< SoftmaxLayer().set_name(\"" + layerName + "\")");

    }




///////////////////        STANDARD LINES   //////////////////////////////////

    // Common part before topology definition
    protected void _writeCommonPartitionStart(String className){
        _printStream.println(_prefix + "//NETWORK ENGINE WITH API");
        _printStream.println(_prefix + "bool " + className + "::do_setup(int argc, char **argv) {");
        _prefixInc();

        _printStream.println();
        _printStream.println(_prefix + "// Parse arguments");
        _printStream.println(_prefix + "cmd_parser.parse(argc, argv);");
        _printStream.println(_prefix + "cmd_parser.validate();");
        _printStream.println();

        _printStream.println(_prefix + "// Consume common parameters");
        _printStream.println(_prefix + "common_params = consume_common_graph_parameters(common_opts);");
        _printStream.println();

        _printStream.println(_prefix + "// Return when help menu is requested");
        _printStream.println(_prefix + "if(common_params.help){");
        _prefixInc();
        _printStream.println(_prefix + "cmd_parser.print_help(argv[0]);");
        _printStream.println(_prefix + "return false;");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println();

        _printStream.println(_prefix + "// Checks");
        _printStream.println(_prefix + "ARM_COMPUTE_EXIT_ON_MSG(arm_compute::is_data_type_quantized_asymmetric(common_params.data_type), " +
                        "\"QASYMM8 not supported for this graph\");");

        _printStream.println(_prefix + "// Get trainable parameters data path");
        _printStream.println(_prefix + "std::string data_path = common_params.data_path;");
        _printStream.println();

        _printStream.println(_prefix + "// Set weights trained layout");
        _printStream.println(_prefix + "const DataLayout weights_layout = DataLayout::NCHW;");
        _printStream.println();
    }

    // Common part after topology definition
    protected void _writeCommonPartitionEnd(){
        _printStream.println();

        _printStream.println(_prefix + "  // Finalize graph");
        _printStream.println(_prefix + "GraphConfig config;");
        _printStream.println();

        _printStream.println(_prefix + "config.num_threads = common_params.threads;");
        _printStream.println(_prefix + "config.use_tuner   = common_params.enable_tuner;");
        _printStream.println(_prefix + "config.tuner_mode  = common_params.tuner_mode;");
        _printStream.println(_prefix + "config.tuner_file  = common_params.tuner_file;");

         _printStream.println();
         _printStream.println(_prefix + "// Load the precompiled kernels from a file into the kernel library, " +
                         "in this way the next time they are needed");
         _printStream.println(_prefix + "// compilation won't be required.");
         _printStream.println(_prefix + "if(common_params.enable_cl_cache) {");
         _prefixInc();
          _printStream.println(_prefix + "restore_program_cache_from_file();");
         _prefixDec();
         _printStream.println(_prefix + "}");
         _printStream.println();

         _printStream.println(_prefix + "graph.finalize(common_params.target, config);");
         _printStream.println();

         _printStream.println(_prefix + "// Save the opencl kernels to a file");
         _printStream.println(_prefix + "if(common_opts.enable_cl_cache) { ");
         _prefixInc();
         _printStream.println(_prefix + " save_program_cache_to_file();");
         _prefixDec();
         _printStream.println(_prefix + "}");
         _printStream.println();

          _printStream.println(_prefix + "return true;");
          _prefixDec();

         _printStream.println(_prefix + "}");
         _printStream.println();
    }


    protected void _writeDoRun (String className){
        _printStream.println();
        _printStream.println("// Run graph");
        _printStream.println(_prefix + "void " + className + "::do_run() { ");
        _prefixInc();
        _printStream.println(_prefix + "graph.run();");
        _prefixDec();
        _printStream.println(_prefix + "}");
        _printStream.println();
    }


}
