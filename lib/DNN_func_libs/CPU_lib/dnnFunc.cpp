// File automatically generated by ESPAM
#include "dnnFunc.h"
#include <iostream>
#include <math.h>
#include <cstdint>
#include <limits>
#include "appFunc.h"
using namespace std;

dnnFunc::dnnFunc() {

}

dnnFunc::~dnnFunc() {

}

/**
 * Execute dense function (MATMUL/GEMM)
 * TODO extend by any NonLinear function, not only softmax
*/
void dnnFunc::execute_dense_block (std::string function, float *input, float *weights, float *output,float *bias, std::map<std::string,int>* int_params_ptr ){
  int input_len = int_params_ptr->at("input_len");
  int output_len = int_params_ptr->at("output_len");

  //MATMUL
  if(bias==nullptr)
    dnnFunc::execute_matmul(input, weights, output, input_len, output_len);
  //GEMM
  else
    dnnFunc::execute_gemm(input, weights, output, bias, input_len, output_len);
}

/**
 * Execute convolutional function
 * TODO: optimize, support nD input tensors
 */
void dnnFunc::execute_conv (float *input, float *weights, float *output,float *bias, const int* pads, std::map<std::string,int>* int_params_ptr, int core_id)
  {
    int input_len = int_params_ptr->at("input_len");
  int output_len = int_params_ptr->at("output_len");

  int stride=int_params_ptr->at("stride");
  int k_h = int_params_ptr->at("k_h");
  int k_w = int_params_ptr->at("k_w");
  //int neurons = int_params_ptr->at("neurons");

  int channels = 1;
  int input_h = int_params_ptr->at("input_dim_1");
  int input_w = int_params_ptr->at("input_dim_0");

  int output_d = 1;
  int output_h = int_params_ptr->at("output_dim_1");
  int output_w =  int_params_ptr->at("output_dim_0");

  if(int_params_ptr->at("input_dims")==3){
    channels = int_params_ptr->at("input_dim_2"); 
    input_h = int_params_ptr->at("input_dim_1");
    input_w = int_params_ptr->at("input_dim_0");
  }

  if(int_params_ptr->at("output_dims")==3){
    output_d = int_params_ptr->at("output_dim_2"); 
    output_h = int_params_ptr->at("output_dim_1");
    output_w = int_params_ptr->at("output_dim_0");
  }

if(input_h == output_h || input_w == output_w ||pads!=nullptr){
	  
if(pads==nullptr){
//std::cout<<" SAME : "<<std::endl;
int w_pads = ((input_w * (stride-1)+k_w))/stride - 1;
	  int w_pad = w_pads/2;
	  //std::cout<<"w_pad = "<<w_pad<<std::endl;
	  int h_pads = ((input_h * (stride-1)+k_h))/stride - 1;
	  int h_pad = h_pads/2;
	  //std::cout<<"h_pad = "<<h_pad<<std::endl;
	  int e_i_h = input_h + h_pads;
	  int e_i_w = input_w + w_pads;

	  float inp_envided[channels][e_i_h][e_i_w] = {0};
        
       // dnnFunc::envide_input(&inp_arr[0][0][0],&inp_envided[0][0][0],channels,inp_h,inp_w,h_pad,w_pad);
       // std::cout<<"c: "<<channels<<" h: "<<input_h<<" w: "<<input_w<<std::endl;
	dnnFunc::envide_input(input,&inp_envided[0][0][0],channels,input_h,input_w,h_pad,w_pad);
//        if(channels ==1){        
//std::cout<<" ENVIDED : "<<std::endl;
//appFunc::print_3D(channels, e_i_h, e_i_w, &inp_envided[0][0][0]);
//}

	dnnFunc::convolution_3d_inp_cpu(&inp_envided[0][0][0], weights, output, bias, channels,e_i_h, e_i_w, output_d, output_h, output_w, k_h, k_w, stride);

//if(channels ==1){        
//std::cout<<" OUTPUT : "<<std::endl;
//appFunc::print_3D(output_d, output_h, output_w, output);
//}
  }
else{
//std::cout<<" VALID ENVIDED : "<<std::endl;
          int w_pad = pads[0];
          int w_pads = w_pad*2;
	  //std::cout<<"w_pad = "<<w_pad<<std::endl;
	  int h_pad = pads[1];
          int h_pads = h_pad*2;
	  //std::cout<<"h_pad = "<<h_pad<<std::endl;

          int e_i_h = input_h + h_pads;
	  int e_i_w = input_w + w_pads;

	  float inp_envided[channels][e_i_h][e_i_w] = {0};
        
       // dnnFunc::envide_input(&inp_arr[0][0][0],&inp_envided[0][0][0],channels,inp_h,inp_w,h_pad,w_pad);
       // std::cout<<"c: "<<channels<<" h: "<<input_h<<" w: "<<input_w<<std::endl;
	dnnFunc::envide_input(input,&inp_envided[0][0][0],channels,input_h,input_w,h_pad,w_pad);
//        if(channels ==1){        
//std::cout<<" ENVIDED : "<<std::endl;
//appFunc::print_3D(channels, e_i_h, e_i_w, &inp_envided[0][0][0]);
//}

	dnnFunc::convolution_3d_inp_cpu(&inp_envided[0][0][0], weights, output, bias, channels,e_i_h, e_i_w, output_d, output_h, output_w, k_h, k_w, stride);

}

}
  else{
//std::cout<<" VALID : "<<std::endl;
  	dnnFunc::convolution_3d_inp_cpu(input, weights, output, bias, channels,input_h, input_w, output_d, output_h, output_w, k_h, k_w, stride);

//std::cout<<"c: "<<channels<<" h: "<<input_h<<" w: "<<input_w<<std::endl;


}

//std::cout<<"OUTPUT: "<<std::endl;
  //appFunc::show_val(output, output_len, output_len);
//std::cout<<"END "<<std::endl;
}

/**
 * Execute convolutional function
 * TODO: optimize, support nD input tensors
 */
void dnnFunc::execute_pool(std::string function, float *input, float *output,float *bias, const int* pads, std::map<std::string,int>* int_params_ptr)
    {
  int stride=int_params_ptr->at("stride");
  int k_h = int_params_ptr->at("k_h");
  int k_w = int_params_ptr->at("k_w");
  //int neurons = int_params_ptr->at("neurons");

  int channels = 1;
  int input_h = int_params_ptr->at("input_dim_1");
  int input_w = int_params_ptr->at("input_dim_0");

  int output_d = 1;
  int output_h = int_params_ptr->at("output_dim_1");
  int output_w =  int_params_ptr->at("output_dim_0");

  if(int_params_ptr->at("input_dims")==3){
    channels = int_params_ptr->at("input_dim_2");
    input_h = int_params_ptr->at("input_dim_1");
    input_w = int_params_ptr->at("input_dim_0");
  }

  if(int_params_ptr->at("output_dims")==3){
    output_d = int_params_ptr->at("output_dim_2");
    output_h = int_params_ptr->at("output_dim_1");
    output_w = int_params_ptr->at("output_dim_0");
  }

 // std::cout<<function<<" c: "<<channels << " h: "<<input_h<<" w: "<<input_w<<" --> c: "<<output_d<<" h: "<<output_h<<" w: "<<output_w<<std::endl;

  if(function.find("MAX") != std::string::npos)
    dnnFunc::execute_maxpool(input, output, bias, channels, input_h, input_w, output_d, output_h, output_w, k_h, k_w, stride);

  if(function.find("AV") != std::string::npos)
      dnnFunc::execute_maxpool(input, output, bias, channels,input_h, input_w, output_d, output_h, output_w, k_h, k_w, stride);
}

/** convolution for CPU function*/
void dnnFunc::convolution_3d_inp_cpu(float *input, float *weights, float *output, float* bias, int channels, int input_h,
 int input_w,int output_d, int output_h,int output_w, int k_h, int k_w, int stride){
         
        //std::cout<<"check"<<std::endl;
	  //init output
	  if(bias==nullptr){
	    for (int i = 0; i < output_d * output_h * output_w; i++)
	      output[i] = 0;
	    }
	  else {
	    float bias_val;
	    for (int d = 0; d < output_d; d++) {
	      bias_val = bias[d];
	      //std::cout<<" d: "<<d <<" ,bias_val "<<bias_val<<" ";
	      for (int i = 0; i < output_w * output_h; i++)
	        output[i + d * output_w * output_h] = bias_val;
	      }
	    }

  float fold_cell = 0;
  int input_elem_ind = 0;
  int k_elem_ind = 0;
  int k_elem_ind_reverse = 0;
  int k_size = k_h*k_w;
  int outp_elem_ind =0;
  int sub_kern_start = 0;

  float *sub_input;
  float *sub_output;
  float *sub_kernel;
  
for (int ofm=0; ofm<output_d;ofm++){
  
  sub_output = output + output_h * output_w * ofm;
  for (int ifm=0; ifm<channels;ifm++){

    sub_input = input + input_w * input_h * ifm;
    sub_kern_start =  k_h * k_w * ifm  + k_h * k_w * channels * ofm;
    //if(channels!=1){
    //std::cout<<"sub_k_start: "<<sub_kern_start<<std::endl;
//}
    sub_kernel = weights + sub_kern_start;

     for (int j = 0; j < output_h; j++){
     for (int i = 0; i < output_w; i++){

    	//std::cout<<std::endl<<std::endl<<"out["<<i<<"]["<<j<<"]"<<std::endl;

        fold_cell = 0;
         //summ k,l
        for (int l = 0; l < k_h; l++){
          for (int k = 0; k < k_w; k++){
            k_elem_ind = l + k * k_h;
            //std::cout<<"k_i "<<k_elem_ind<< " ";
            k_elem_ind_reverse = k_size - k_elem_ind - 1;
            //std::cout<<",k_ir "<<k_elem_ind_reverse<< " ";
            input_elem_ind = (j*stride+l) + (i*stride+k)* input_w;
           // cout<<"inp["<<(i+k)<<"]["<<(j+l)<<"] * k["<<k<<"]["<<l<<"] + ";
                 fold_cell += *(sub_input + input_elem_ind) * *(sub_kernel + k_elem_ind);
             //  fold_cell += *(sub_input + input_elem_ind) * *(sub_kernel + k_elem_ind_reverse);
          }
        }
        outp_elem_ind = i * output_w + j;
        sub_output[outp_elem_ind] += fold_cell;
        fold_cell = 0;
}
}
}
}

}
void dnnFunc::execute_gemm(float *input, float *weights, float *result, float *bias, int input_len, int output_len){
  for (int j=0; j<output_len; j++){
    *(result+j) = *(bias+j);
    for(int i=0; i<input_len; i++)
      *(result+j) +=*(input + i) * *(weights + i + j*input_len);
}
}

/** convolution for CPU function*/
void dnnFunc::convolution_3d_inp_cpu_same(float *input, float *weights, float *output, float* bias, int channels, int input_h,
 int input_w,int output_d, int output_h,int output_w, int k_h, int k_w, int stride){

	  //init output
	  if(bias==nullptr){
	    for (int i = 0; i < output_d * output_h * output_w; i++)
	      output[i] = 0;
	    }
	  else {
	    float bias_val;
	    for (int d = 0; d < output_d; d++) {
	      bias_val = bias[d];
	      //std::cout<<" d: "<<d <<" ,bias_val "<<bias_val<<" ";
	      for (int i = 0; i < output_w * output_h; i++)
	        output[i + d * output_w * output_h] = bias_val;
	      }
	    }

float fold_cell = 0;
int input_elem_ind = 0;
int k_elem_ind = 0;
int k_elem_ind_reverse = 0;
int k_size = k_h*k_w;
int outp_elem_ind =0;

int sub_input_start = 0;
int sub_input_end = input_w * input_h - 1;
float input_val=0;

float *sub_input;
float *sub_output;
float *sub_kernel;

for (int ofm=0; ofm<output_d;ofm++){

sub_output = output + output_h * output_w * ofm;
sub_kernel = weights + k_h * k_w * ofm;

float in_val;

for (int ifm=0; ifm<channels;ifm++){

  sub_input = input + input_w * input_h * ifm;

  int w_pads = ((input_w * (stride-1)+k_w))/stride - 1;
  int w_pad = w_pads/2;
  int h_pads = ((input_h * (stride-1)+k_h))/stride - 1;
  int h_pad = h_pads/2;


  //COMPUTE MID PART
       for (int j = h_pad; j < output_h - h_pad; j++){
       for (int i = w_pad; i < output_w - w_pad; i++){

      	//std::cout<<std::endl<<std::endl<<"out["<<i<<"]["<<j<<"]"<<std::endl;

          fold_cell = 0;
           //summ k,l
          for (int l = 0; l < k_h; l++){
            for (int k = 0; k < k_w; k++){
              k_elem_ind = l + k * k_h;
              //std::cout<<"k_i "<<k_elem_ind<< " ";
              k_elem_ind_reverse = k_size - k_elem_ind - 1;
              //std::cout<<",k_ir "<<k_elem_ind_reverse<< " ";
              input_elem_ind = ((j-h_pad)*stride+l) + ((i-w_pad)*stride+k)* input_w;
             // cout<<"inp["<<(i+k)<<"]["<<(j+l)<<"] * k["<<k<<"]["<<l<<"] + ";
              float in_val = *(sub_input + input_elem_ind);

              fold_cell += *(sub_input + input_elem_ind) * *(sub_kernel + k_elem_ind_reverse);
            }
          }
          outp_elem_ind = j + i * (output_w);
          //if(j>1)
          // outp_elem_ind += i*w_pad;

          //cout<<"outp_elem_ind "<<outp_elem_ind<<", o_el: ["<<j<<"]["<<i<<"], ";
          sub_output[outp_elem_ind] += fold_cell;
          fold_cell = 0;

  }
  }

  }//ifm
  }//ofm

}//func

/** convolution for CPU function*/
void dnnFunc::convolution_3d_inp_cpu_same_no_border(float *input, float *weights, float *output, float* bias, int channels, int input_h,
 int input_w,int output_d, int output_h,int output_w, int k_h, int k_w, int stride){

	  //init output
	  if(bias==nullptr){
	    for (int i = 0; i < output_d * output_h * output_w; i++)
	      output[i] = 0;
	    }
	  else {
	    float bias_val;
	    for (int d = 0; d < output_d; d++) {
	      bias_val = bias[d];
	      //std::cout<<" d: "<<d <<" ,bias_val "<<bias_val<<" ";
	      for (int i = 0; i < output_w * output_h; i++)
	        output[i + d * output_w * output_h] = bias_val;
	      }
	    }

  float fold_cell = 0;
  int input_elem_ind = 0;
  int k_elem_ind = 0;
  int k_elem_ind_reverse = 0;
  int k_size = k_h*k_w;
  int outp_elem_ind =0;


  int w_pads = ((input_w * (stride-1)+k_w))/stride - 1;
  int w_pad = w_pads/2;
  int h_pads = ((input_h * (stride-1)+k_h))/stride - 1;
  int h_pad = h_pads/2;

  float *sub_input;


  float *sub_output;
  float *sub_kernel;

for (int ofm=0; ofm<output_d;ofm++){

  sub_output = output + output_h * output_w * ofm;
  sub_kernel = weights + k_h * k_w * ofm;

  for (int ifm=0; ifm<channels;ifm++){

    sub_input = input + input_w * input_h * ifm;


    //COMPUTE MID PART
     for (int j = h_pad; j < output_h - h_pad; j++){
     for (int i = w_pad; i < output_w - w_pad; i++){

    	//std::cout<<std::endl<<std::endl<<"out["<<i<<"]["<<j<<"]"<<std::endl;

        fold_cell = 0;
         //summ k,l
        for (int l = 0; l < k_h; l++){
          for (int k = 0; k < k_w; k++){
            k_elem_ind = l + k * k_h;
            //std::cout<<"k_i "<<k_elem_ind<< " ";
            k_elem_ind_reverse = k_size - k_elem_ind - 1;
            //std::cout<<",k_ir "<<k_elem_ind_reverse<< " ";
            input_elem_ind = ((j-h_pad)*stride+l) + ((i-w_pad)*stride+k)* input_w;
           // cout<<"inp["<<(i+k)<<"]["<<(j+l)<<"] * k["<<k<<"]["<<l<<"] + ";
            float in_val = *(sub_input + input_elem_ind);

            fold_cell += *(sub_input + input_elem_ind) * *(sub_kernel + k_elem_ind_reverse);
          }
        }
        outp_elem_ind = j + i * (output_w);
        //if(j>1)
        // outp_elem_ind += i*w_pad;

        //cout<<"outp_elem_ind "<<outp_elem_ind<<", o_el: ["<<j<<"]["<<i<<"], ";
        sub_output[outp_elem_ind] += fold_cell;
        fold_cell = 0;

}
}
}//ifm
}//ofm

}

void dnnFunc::execute_matmul(float *input, float *weights, float *result, int input_len, int output_len){
  for (int j=0; j<output_len; j++){
    *(result+j) = 0;
    for(int i=0; i<input_len; i++)
      *(result+j) +=*(input + i) * *(weights + i + j*input_len);
}
}

void dnnFunc::weight_and_sum(float *input, float *weights, float *result, int input_len){
  *result = 0;
for(int i=0; i<input_len; i++)
*result +=*(input+i) * *(weights+i);
}

void dnnFunc::execute_softmax( float *non_activated_stages, float *output, int len)
{
	int softmax_summ = dnnFunc::get_softmax_summ(non_activated_stages, len);
	float *sm_elem;
	for(int i=0; i<len; i++)
	{
		sm_elem = non_activated_stages + i;
		output[i]=exp(*(sm_elem))/softmax_summ;
	}
}

float dnnFunc::get_softmax_summ(float *non_activated_stages, int len)
{
	float softmax_summ = 0;
	for( int i=0;i<len;i++)
		softmax_summ+=exp( *(non_activated_stages + i));
	return softmax_summ;
}

void dnnFunc::init_zeros(float *matrix, int h, int w){
	for (int j = 0; j < h; j++){
		for (int i = 0; i < w; i++){
			matrix[i * w + j]=0;
		}
	}}

void dnnFunc::init_zeros(float *matrix, int d, int h, int w){
	for (int k = 0; k < d; k++){
		for (int j = 0; j < h; j++){
			for (int i = 0; i < w; i++){
				matrix[i * w + j]=0;
				}
	}		}
}

void dnnFunc::softmax(float *input, int n, float *output)
{
    int i;
    float sum = 0;
    float largest = -numeric_limits<float>::max();
    for(i = 0; i < n; ++i){
        if(input[i] > largest) largest = input[i];
    }
    for(i = 0; i < n; ++i){
        float e = exp(input[i] - largest);
        sum += e;
        output[i] = e;
    }
    for(i = 0; i < n; ++i){
        output[i] /= sum;
    }
}

/**MARNIX*/

void dnnFunc::execute_addconst (float* input, float* weights, float* output, int input_len){
    for(int i = 0; i < input_len; i++)
        output[i] = input[i] + 1;
}

void dnnFunc::execute_relu(float* input, float* output, int input_len){
    float zero_val = 0;
	for(int i=0; i < input_len; i++)
        output[i] = std::max(zero_val,input[i]);
}

void dnnFunc::execute_maxpool (float *input, float *output, float* bias, int channels, int input_h,
		 int input_w,int output_d, int output_h,int output_w, int k_h, int k_w, int stride){

	  //init output
	  if(bias==nullptr){
	    for (int i = 0; i < output_d * output_h * output_w; i++)
	      output[i] = 0;
	    }
	  else {
	    float bias_val;
	    for (int d = 0; d < output_d; d++) {
	      bias_val = bias[d];
	      //std::cout<<" d: "<<d <<" ,bias_val "<<bias_val<<" ";
	      for (int i = 0; i < output_w * output_h; i++)
	        output[i + d * output_w * output_h] = bias_val;
	      }
	    }

	  float fold_cell = 0;
	  int input_elem_ind = 0;
	  float input_elem;
	  int outp_elem_ind =0;

	  float *sub_input;
	  float *sub_output;

	  for (int ofm=0; ofm<output_d;ofm++){

	  sub_output = output + output_h * output_w * ofm;
	  sub_input = input + input_w * input_h * ofm;

	    //loop over input
	     for (int j = 0; j < output_h; j++){
	     for (int i = 0; i < output_w; i++){
                input_elem_ind = (j*stride) + (i*stride)* input_w;
                fold_cell = -numeric_limits<float>::max();

	         // loop over k,l area
	        for (int l = 0; l < k_h; l++){
	          for (int k = 0; k < k_w; k++){
	            input_elem_ind = (j*stride+l) + (i*stride+k)* input_w;
	            input_elem = *(sub_input + input_elem_ind);
	            if(fold_cell<input_elem)
	            	fold_cell = input_elem;
	          }
	        }
	        outp_elem_ind = i * output_w + j;
	        sub_output[outp_elem_ind] += fold_cell;
	}
	}
	}
}

/** init 3D array with bias or zeros*/
void dnnFunc::init_output(float *arr, float* bias, int depth, int height, int width){
	  //init output
	  if(bias==nullptr){
	    for (int i = 0; i < depth * height * width; i++)
	      arr[i] = 0;
	    }
	  else {
	    float bias_val;
	    for (int d = 0; d < depth; d++) {
	      bias_val = bias[d];
	      //std::cout<<" d: "<<d <<" ,bias_val "<<bias_val<<" ";
	      for (int i = 0; i < height * width; i++)
	        arr[i + d * height * width] = bias_val;
	      }
	    }
}

void dnnFunc::execute_avgpool (float *input, float *output, float* bias, int channels, int input_h,
		 int input_w,int output_d, int output_h,int output_w, int k_h, int k_w, int stride){

	  //init output
	  if(bias==nullptr){
	    for (int i = 0; i < output_d * output_h * output_w; i++)
	      output[i] = 0;
	    }
	  else {
	    float bias_val;
	    for (int d = 0; d < output_d; d++) {
	      bias_val = bias[d];
	      //std::cout<<" d: "<<d <<" ,bias_val "<<bias_val<<" ";
	      for (int i = 0; i < output_w * output_h; i++)
	        output[i + d * output_w * output_h] = bias_val;
	      }
	    }

	  float fold_cell = 0;
	  int input_elem_ind = 0;
	  float input_elem;
	  int outp_elem_ind =0;

	  float avg_divider = float(k_w*k_h);

	  float *sub_input;
	  float *sub_output;

	  for (int ofm=0; ofm<output_d;ofm++){

	  sub_output = output + output_h * output_w * ofm;
	  sub_input = input + input_w * input_h * ofm;

	    //loop over input
	     for (int j = 0; j < output_h; j++){
	     for (int i = 0; i < output_w; i++){
                 fold_cell =  0;
	         // loop over k,l area
	        for (int l = 0; l < k_h; l++){
	          for (int k = 0; k < k_w; k++){
	            input_elem_ind = (j*stride+l) + (i*stride+k)* input_w;
	            input_elem = *(sub_input + input_elem_ind);
	            fold_cell += input_elem;
	          }
	        }
	        //cout<<"fold cell["<<i<<"]["<<j<<"]"<<fold_cell<<" ";
	        fold_cell/=avg_divider;
	        outp_elem_ind = i * output_w + j;
	        *(sub_output+outp_elem_ind) = fold_cell;
	}
	}
	}
}

void dnnFunc::execute_lrn (float* input, float* output, int input_len, int neurons, int nsize){
	//WHY?
	//if(int_params_ptr->at("input_dims") != 1)
		//return;

	//int input_len;
	//should not be there! shoubl be in the calling (interface) function
	//int neurons = int_params-ptr->at("neurons");
	//int nsize = int_params_ptr->at("_size");
	int dist = input_len / neurons;
	int avg;

	for(int i = 0; i < dist; i++){
		avg = 0;
		for(int j = 0; j < nsize; j++){
			avg += input[i + (j * dist)];
		}
		avg /= nsize;
		for(int j = 0; j < nsize; j++){
			output[i + (j * dist)] = avg;
		}
	}
}


void dnnFunc::execute_sigm (float* input, float* output, int input_len){
    for(int i = 0; i < input_len; i++)
        output[i] = 1 / (1 + (exp(-1 * input[i])));
}

void dnnFunc::execute_thn (float* input, float* output, int input_len){
    for(int i = 0; i < input_len; i++)
        output[i] = tanh(input[i]);
}

void dnnFunc::transpose(float *input, int inp_h, int inp_w){
	float tmp[inp_w][inp_h] = {0};

	for(int j=0; j<inp_h; j++){
		for(int i=0; i<inp_w; i++){
		tmp[i][j] = *(input+i+j*inp_w);
		}
	}

	for(int j=0; j<inp_h; j++){
		for(int i=0; i<inp_w; i++){
		*(input+j+i*inp_h) = tmp[i][j];
		}
	}

	std::cout<<std::endl;

}


void dnnFunc::envide_input(float *input, float* envided, int inp_d, int inp_h, int inp_w, int h_pad, int w_pad){
	   float *sub_input;
	   float *sub_input_line;
	   float *sub_envided;
	   float *sub_envided_line;

	   for(int d=0; d<inp_d; d++){
		   sub_input = input + inp_w * inp_h * d;

		   sub_envided = envided + (inp_h + h_pad*2) * (inp_w + w_pad * 2) * d;
		   for(int j=0; j<(inp_h+h_pad*2); j++){
			   for(int i=0; i<(inp_w+w_pad*2); i++){
				  *( sub_envided + i + j*(inp_w + w_pad*2)) = 0;
			   }
		   }

		   for(int j=0; j<inp_h; j++){
			   sub_input_line = sub_input + j*(inp_h);
			   sub_envided_line = sub_envided + (j+h_pad) *(inp_w + w_pad*2);
			   for(int i=0; i<inp_w; i++){
				* (sub_envided_line + i + w_pad) = *(sub_input_line + i);
				//  cout<< input[i * inp_w + j] <<" ";
			   }

			   //cout<<std::endl;
		   }

	   }
}
